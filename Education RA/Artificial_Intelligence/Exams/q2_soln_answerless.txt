# 6.034 Quiz 2, Spring 2005
## 1 Decision Trees (13 pts)
Data points:  
Negative: (-1, 0), (2, 1), (2, -2)  
Positive: (0, 0), (1, 0)  

### 1. Construct a decision tree for the data.
### 2. Draw decision boundaries on the graph.
### 3. Explain how you chose the top-level test in the tree.
### 4. What class does the tree predict for the point (1, -1.01)?

## 2 Nearest Neighbors (8 pts)
### 1. Draw 1-NN decision boundaries.
### 2. What class does 1-NN predict for (1, -1.01)? Explain.
### 3. What class does 3-NN predict for (1, -1.01)? Explain.

## 3 Perceptron (7 pts)
Data points:  
Negative: (-1, 0), (2, -2)  
Positive: (1, 0)  

### 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
### 2. Predict the class for (2.0, -1.01).
### 3. Describe the result if applied to the 5-point dataset from Problem 1.

## 4 Neural Net (9 pts)
### 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
### 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).

## 5 Naive Bayes (8 pts)
Training data (12 examples: 6 positive, 6 negative):  
|  Feature  | \(y=0\) | \(y=1\) |  
|-----------|---------|---------|  
| \(x_1=1\) | 6       | 6       |  
| \(x_2=1\) | 0       | 0       |  
| \(x_3=1\) | 2       | 4       |  

### 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
### 2. Identify the most influential feature.

## 6 Learning Algorithms (16 pts)
For each scenario, choose the best algorithm and justify:  
### 1. 1M training examples, 6D features, 100 test queries.
### 2. Classifier for kindergarten special education requiring justification.
### 3. Book preference prediction with 1M features and frequent updates.
### 4. Rainfall prediction from ocean currents/tides.

## 7 Error vs Complexity (15 pts)
For each algorithm, specify:  
### 1. Perceptron
### 2. Linear SVM
### 3. Decision Tree (fixed depth)
### 4. Neural Network (no regularization)
### 5. SVM (\(c < \infty\))

## 8 Regression (12 pts)
Draw regression outputs for:  
### 1. 2-NN (equally weighted)
### 2. Regression Trees (leaf size=1)
### 3. Linear Neural Network
### 4. Multi-layer Neural Network

## 9 SVM (12 pts)
Data points:  
Negative: (-1, 0), (2, -2)  
Positive: (1, 0)  

### 1. Evaluate if separators satisfy SVM conditions:
   (a) \(x_1 + x_2 = 0\)  
   (b) \(x_1 + 1.5x_2 = 0\)  
   (c) \(x_1 + 2x_2 = 0\)  
   (d) \(2x_1 + 3x_2 = 0\)  
### 2. Match kernels to decision boundaries:
   (a) Polynomial (degree=2)  
   (b) Polynomial (degree=3)  
   (c) RBF (\(\sigma=0.5\))  
   (d) RBF (\(\sigma=1.0\))  
