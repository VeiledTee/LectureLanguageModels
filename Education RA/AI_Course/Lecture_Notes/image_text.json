[
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 2.1\n\n---\n\nSlide 2.1.1\n\nSearch plays a key role in many parts of AI. These algorithms provide the conceptual backbone of almost every approach to the systematic exploration of alternatives.\n\nWe will start with some background, terminology and basic implementation strategies and then cover four classes of search algorithms, which differ along two dimensions: First, is the difference between **uninformed** (also known as blind) search and then **informed** (also known as heuristic) searches. Informed searches have access to task-specific information that can be used to make the search process more efficient. The other difference is between **any path** searches and **optimal** searches. Optimal searches are looking for the best possible path while any-path searches will just settle for finding some solution.\n\n---\n\n6.034 Artificial Intelligence\n\n\u2022 Big idea: Search allows exploring alternatives\n\n\u2022 Background\n\n\u2022 Uninformed vs Informed\n\n\u2022 Any Path vs Optimal Path\n\n\u2022 Implementation and Performance\n\n---\n\nSlide 2.1.2\n\nThe search methods we will be dealing with are defined on trees and graphs, so we need to fix on some terminology for these structures:\n\nA ***tree*** is made up of ***nodes*** and ***links*** (circles and lines) connected so that there are no loops (cycles). Nodes are sometimes referred to as vertices and links as edges (this is more common in talking about graphs).\n\nA ***tree*** has a **root node** (where the tree \"starts\"). Every node except the root has a single parent (aka direct ancestor). More generally, an ***ancestor*** node is a node that can be reached by repeatedly going to a parent node. Each node (except the terminal (aka leaf) nodes) has one or more children (aka direct descendants). More generally, a **descendant** node is a node that can be reached by repeatedly going to a child node.\n\n---\n\nTrees and Graphs\n\n  B is parent of C\n  B is child of A\n  A is ancestor of C\n  C is descendant of A\n\n---\n\nSlide 2.1.3\n\nA ***graph*** is also a set of nodes connected by links but where loops are allowed and a node can have multiple parents. We have two kinds of graphs to deal with: **directed graphs**, where the links have direction (akin to one-way streets).\n\n---\n\nTrees and Graphs\n\n  \\[Diagram showing a tree and a directed graph\\]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n## Trees and Graphs\n\n![Diagrams of Trees and Graphs]\n\n__Tree__\n- Node (vertex)\n- Link (edge)\n\nDirected Graph (one-way streets)\n\nUndirected Graph (two-way streets)\n\n## Slide 2.1.4\nAnd, **undirected graphs** where the links go both ways. You can think of an undirected graph as shorthand for a graph with directed links going each way between connected nodes.\n\n## Slide 2.1.5\nGraphs are everywhere; for example, think about road networks or airline routes or computer networks. In all of these cases we might be interested in finding a path through the graph that satisfies some property. It may be that any path will do or we may be interested in a path having the fewest \"hops\" or a least cost path assuming the hops are not all equivalent, etc.\n\n## Examples of Graphs\nAirline Routes\n\n![Diagram with cities San Fran, Boston, Austin, LA, Dallas, Chicago]\n\n## Slide 2.1.6\nHowever, graphs can also be much more abstract. Think of the graph defined as follows: the nodes denote descriptions of a state of the world, e.g., which blocks are on top of what in a blocks scene, and where the links represent actions that change from one state to the other.\n\nA path through such a graph (from a start node to a goal node) is a \"plan of action\" to achieve some desired goal state from some known starting state. It is this type of graph that is of more general interest in AI.\n\n## Slide 2.1.7\nOne general approach to problem solving in AI is to reduce the problem to be solved to one of searching a graph. To use this approach, we must specify what are the states, the **actions** and the **goal test**.\n\nA **state** is supposed to be **complete**, that is, to represent all (and preferably only) the relevant aspects of the problem to be solved. So, for example, when we are planning the cheapest round-the-world flight plan, we don't need to know the address of the airports; knowing the identity of the airport is enough. The address will be important, however, when planning how to get from the hotel to the airport. Note that, in general, to plan an air route we need to know the airport, not just the city, since some cities have multiple airports.\n\nWe are assuming that the actions are **deterministic**, that is, we know exactly the state after the action is performed. We also assume that the actions are discrete, so we don't have to represent what happens while the action is happening. For example, we assume that a flight gets us to the scheduled destination and that what happens during the flight does not matter (at least when planning the route).\n\n## Problem Solving Paradigm\n- **What are the states?** (All relevant aspects of the problem)\n  - Arrangement of parts (to plan an assembly)\n  - Positions of blocks (to plan package distribution)\n  - City (to plan a trip)\n  - Set of facts (e.g. to prove geometry theorem)\n- **What are the actions (operations)?** (Deterministic and discrete)\n  - Assemble new parts\n  - Move a block to a new position\n  - Fly to a new city\n  - Apply a theorem to derive new facts\n- **What's the goal state?** (Conditions for success)\n  - All parts in place\n  - All packages delivered\n  - Reached destination city\n  - Derived goal fact"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_003.jpeg",
    "text": "**6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.**\n\nNot that we've indicated that (in general) we need a test for the goal, not just one specific goal state. So, for example, we might be interested in any city in Germany rather than specifically Frankfurt. Or, when proving a theorem, all we care is about knowing one fact in our current data base of facts. Any final set of facts that contains the desired fact is a proof.\n\nIn principle, we could also have multiple starting states, for example, if we have some uncertainty about the starting state. But, for now, we are not addressing issues of uncertainty either in the starting state or in the result of the actions.\n\n---\n\n**Graph Search as Tree Search**\n\n- Trees are directed graphs without cycles and with nodes having <= 1 parent.\n\n\\[Figure showing a tree structure with nodes and directed edges.\\]\n\n- We can turn graph search problems into tree search problems by:\n  - replacing undirected links by directed links\n  - avoiding loops in path (or keeping track of visited nodes globally)\n\n---\n\n**Slide 2.1.8**\n\nNote that trees are a subclass of directed graphs (even when not shown with arrows on the links). Trees don't have cycles and every node has a single parent (or is the root). Cycles are bad for searching, since, obviously, you don't want to go round and round getting nowhere.\n\nWhen asked to search a graph, we can construct an equivalent problem of searching a tree by doing two things: turning undirected links into two directed links; and, more importantly, making sure we never consider a path with a loop or, even better, by never visiting the same node twice.\n\n---\n\n**Slide 2.1.9**\n\nYou can see an example of this converting from a graph to a tree here. If we assume that S is the start of our search and we are trying to find a path to G, then we can walk through the graph and make connections from every node to every connected node that would not create a cycle (and stop whenever we hit G). Note that the tree has a leaf node for every non-looping path in the graph starting at S.\n\nAlso note, however, that even though we avoided loops, some nodes (the colored ones) are duplicated in the tree, that is, they were reached along different non-looping paths. This means that a complete search of this tree might do extra work.\n\nThe issue of how much effort to place in avoiding loops and avoiding extra visits to nodes is an important one that we will revisit later when we discuss the various search algorithms.\n\n---\n\n**Terminology**\n\n- **State** \u2013 Used to refer to the vertices of the underlying graph that is being searched, that is, a piece to the problem domain, for example, a city, an arrangement of blocks or the arrangement of parts in a puzzle.\n\n- **Search Node** \u2013 Refers to the vertices of the search tree which is being generated by the search algorithm. Each node refers to a state of the world, many nodes may refer to the same state. Importantly, a node implicitly represents a path from the start state of the search to the state associated with the node. (Because search nodes are part of a search tree, they have a unique ancestor node (except for the root node)).\n\n---\n\n**Slide 2.1.10**\n\nOne important distinction that will help us keep things straight is that between a **state** and a **search node**.\n\nA state is an arrangement of the real world (or at least our model of it). We assume that there is an underlying \"real\" state graph that we are searching (although it might not be explicitly represented in the computer; it may be implicitly defined by the actions). We assume that you can arrive at the same real world state by multiple routes, that is, by different sequences of actions.\n\nA search node, on the other hand, is a data structure in the search algorithm, which constructs an explicit tree of nodes while searching. Each node refers to some state, but not uniquely. Note that a node also corresponds to a path from the start state to the state associated with the node. This follows from the fact that the search algorithm is generating a tree. So, if we return a node, we're returning a path."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 2.2\n\nSlide 2.2.1\n\nSo, let's look at the different classes of search algorithms that we will be exploring. The simplest class is that of the uninformed, any-path algorithms. In particular, we will look at depth-first and breadth-first search. Both of these algorithms basically look at all the nodes in the search tree in a specific order (independent of the goal) and stop when they find the first path to a goal state.\n\nClasses of Search\n------------------------------------------------------------------------\nClass                            |  Name            | Operation\n------------------------------------------------------------------------\nAny Path   Uninformed | Depth-First   | Systematic exploration of whole tree\nAny Path   Uninformed | Breadth-First | Systematic exploration of whole tree\n                                            |                        | until a goal node is found.\n------------------------------------------------------------------------\n\nSlide 2.2.2\n\nThe next class of methods are informed, any-path algorithms. The key idea here is to exploit a task specific measure of goodness to try to either reach the goal more quickly or find a more desirable goal state.\n\nClasses of Search\n------------------------------------------------------------------------\nClass                             | Name         | Operation\n------------------------------------------------------------------------\nAny Path  Uninformed  | Depth-First | Systematic exploration of whole tree\nAny Path  Uninformed  | Breadth-First | Systematic exploration of whole tree\nAny Path  Informed       | Best-First   | Uses heuristic measure of goodness\n                                                |                      | of a state, e.g. estimated distance to goal.\n------------------------------------------------------------------------\n\nSlide 2.2.3\n\nNext, we look at the class of uninformed, optimal algorithms. These methods guarantee finding the \"best\" path (as measured by the sum of weights on the graph edges) but do not use any information beyond what is in the graph definition.\n\nClasses of Search\n------------------------------------------------------------------------\nClass                            | Name            | Operation \n------------------------------------------------------------------------\nAny Path   Uninformed | Depth-First   | Systematic exploration of whole tree\nAny Path   Uninformed | Breadth-First | Systematic exploration of whole tree\nAny Path   Informed      | Best-First      | Uses heuristic measure of goodness\n                                            |                        | of a state, e.g. estimated distance to goal.\nOptimal      Uninformed | Uniform-Cost | Uses path \"length\" measure.\n                                            |                        | Finds \"shortest\" path.\n------------------------------------------------------------------------\n\nSlide 2.2.4\n\nFinally, we look at informed, optimal algorithms, which also guarantee finding the best path but which exploit heuristic (\"rule of thumb\") information to find the path faster than the uninformed methods.\n\nClasses of Search\n------------------------------------------------------------------------\nClass                            | Name             | Operation\n------------------------------------------------------------------------\nAny Path   Uninformed | Depth-First    | Systematic exploration of whole tree\nAny Path   Uninformed | Breadth-First  | Systematic exploration of whole tree\nAny Path   Informed      | Best-First      | Uses heuristic measure of goodness\n                                             |                        | of a state, e.g. estimated distance to goal.\nOptimal      Uninformed | Uniform-Cost  | Uses path \"length\" measure.\n                                             |                        | Finds \"shortest\" path.\nOptimal      Informed      | A*                   | Uses path \"length\" measure and heuristic.\n                                             |                        | Finds \"shortest\" path.\n------------------------------------------------------------------------"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_005.jpeg",
    "text": "Slide 2.2.5\n\nThe search strategies we will look at are all instances of a common search algorithm, which is shown here. The basic idea is to keep a list (Q) of nodes (that is, partial paths), then to pick one such node from Q, see if it reaches the goal and otherwise extend that path to its neighbors and add them back to Q. Except for details, that\u2019s all there is to it.\n\nNote, by the way, that we are keeping track of the states we have reached (visited) and not entering them in Q more than once. This will certainly keep us from ever looping, no matter how the underlying graph is connected, since we can only ever reach a state once. We will explore the impact of this decision later.\n\nSimple Search Algorithm\n\nA search node is a path from the start node S to the search node, e.g. {ABAS}\n\nThe state of a search node is the most notable fact of the path, e.g:X\n\u2022 Let A be search node, eg, {BABAS} then state(A) = S\n\u2022 Let S be the start state\n\n1. Initialize Q with search node (S) as only entry; set Visited = {S}\n\n2. If Q is empty, fail. Else, pick some search node N from Q\n\n3. If state(N) is a goal, return N (we\u2019ve reached the goal)\n\n4. (Otherwise) Remove N from Q\n\n5. Find all the descendants of state(N) not in Visited and create all the one-step extensions of N to each descendant.\n\n6. Add the extended paths to Q; add children of state(N) to Visited\n\n7. Go to step 2\n\nCritical decisions:\nStep 2: picking N from Q\nStep 6: adding new paths to Q\n\nSlide 2.2.6\n\nThe key questions, of course, are which entry to pick off of Q and how precisely to add the new paths back onto Q. Different choices for these operations provide the various search strategies.\n\nSlide 2.2.7\n\nAt this point, we are ready to actually look at a specific search. For example, depth-first search\nalways looks at the deepest node in the search tree first. We can get that behavior by:\n\npicking the first element of Q as the node to test and extend.\n\nadding the new (extended) paths to the FRONT of Q, so that the next path to be examined will be one of the extensions of the current path to one of the descendants of that node's state.\n\nOne good thing about depth-first search is that Q never gets very big. We will look at this in more detail later, but it\u2019s fairly easy to see that the size of the Q depends on the depth of the search tree and not on its breadth.\n\nImplementing the Search Strategies\n\nDepth-first:\n\n\u2022 Pick first element of Q\n\n\u2022 Add path extensions to front of Q\n\nSlide 2.2.8\n\nBreadth-first is the other major type of uninformed (or blind) search. The basic approach is to once again pick the first element of Q to examine BUT now we place the extended paths at the back of Q. This means that the next path pulled off of Q will typically not be a descendant of the current one, but rather one at the same level in tree.\n\nNote that in breadth-first search, Q gets very big because we postpone looking at longer paths (that go to the next level) until we have finished looking at all the paths at one level.\n\nWe\u2019ll look at how to implement other search strategies in just a bit. But, first, lets look at some of the more subtle issues in the implementation.\n\nImplementing the Search Strategies\n\nBreadth-first:\n\n\u2022 Pick first element of Q\n\n\u2022 Add path extensions to end of Q\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_006.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 2.2.9**\n\nOne subtle point is where in the algorithm one tests for success (that is, the goal test). There are two plausible points: one is when a path is extended and it reaches a goal, the other is when a path is pulled off of Q. We have chosen the latter (testing in step 3 of the algorithm) because it will generalize more readily to optimal searches. However, testing on extension is correct and will save some work for any-path searches.\n\n---\n\n**Testing for the Goal**\n\n- This algorithm stops (in step 3) when state(0) = G or, in general, when state(n) satisfies the goal test.\n\n- We could have performed this test in step 6 as each extended path is added to Q. This would catch termination earlier and be perfectly correct for the searches we have covered so far.\n\n- However, performing the test in step 6 will be incorrect for the optimal\n  searches. We have chosen to leave the test in step 3 to maintain uniformity\n  with these future searches.\n\n---\n\n**Slide 2.2.10**\n\nAt this point, we need to agree on more terminology that will play a key role in the rest of our discussion of search. \n\nLet's start with the notion of Visited as opposed to Expanded. We say a state is visited when a path that reaches that state (that is, a node that refers to that state) gets added to Q. So, if the state is anywhere in any node on Q, it has been visited. Note that this is true even if no path to that state has been taken off Q.\n\n---\n\n**Slide 2.2.11**\n\nA state M is Expanded when a path to that state is pulled off of Q. At that point, the descendants of M are visited and the paths to those descendants added to the Q.\n\n---\n\n**Terminology**\n\n- **Visited** - a state M is first visited when a path to M first gets added to Q. In general, a state is said to have been visited if it has ever shown up in a search node in Q. The intuition is that we have briefly \"visited\" them to place them on Q, but we have not yet examined them carefully.\n\n- **Expanded** - a state M is expanded when it is the state of a search node that is pulled off of Q. At that point, the descendants of M are visited and the path the node M is intended to the eligible descendants. In principle, a state may be expanded multiple times. We sometimes refer to the search node to end Q (instead of M itself) as being expanded. However, once a node is expanded we are done with it; we will not need to expand it again. In fact, we discard it from Q.\n\n- This distinction plays a key role in our discussion of the various search algorithms; study it carefully.\n\n---\n\n**Slide 2.2.12**\n\nPlease try to get this distinction straight; it will save you no end of grief."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_007.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 2.2.13**\n\nIn our description of the simple search algorithm, we made use of a Visited list. This is a list of all the states corresponding to nodes that were added to Q. As we mentioned earlier, avoiding nodes on the visited list will certainly keep us from looping, even if the graph has loops in it. Note that this mechanism is stronger than just avoiding loops locally in every path; this is a global mechanism across all paths. In fact, it is more general than a loop check on each path, since by definition a loop will involve visiting a state more than once.\n\nBut, in addition to avoiding loops, the Visited list will mean that our search will never expand a state more than once. The basic idea is that we do not need to search for a path from any state to the goal more than once. If we did not find a path the first time we tried it, one is not going to materialize the second time. And, it saves work, possibly an enormous amount, not to look again. More on this later.\n\n- *Visited States*\n  - Keeping track of visited states generally improves time efficiency when searching graphs, without affecting correctness. Note, however, that substantial additional space may be required to keep track of visited states.\n  - If all we wanted to do is find a path from the start to the goal, there is no advantage to adding a search node whose state is already the state of another reachable node.\n  - Any states reachable from the node the second time would have been reachable from that node the first time.\n  - Note that, when using Visited, each state will only ever have at most one path to it (search node) in Q.\n  - We'll have to revisit this issue when we look at optimal searching.\n\n---\n\n**Slide 2.2.14**\n\nImplementation Issues: The Visited list\n\n- Although we speak of a Visited list, this is never the preferred implementation.\n- If the graph states are known ahead of time as is often the case in puzzles or games, then rather than adding nodes to a Visited list, it is much more efficient to add a boolean variable to each node's state information. \n- If the states are generated as you go, so they are unknown ahead of time, then a hash table, keyed on some property of the states (like position of the space in a slide puzzle) can be used to keep track of previously visited states.\n- Not that if the entire universal space of Q is visited states are generated\n  incrementally, then the number of states -- which can be very high in some\n  problems.\n\n---\n\n**Slide 2.2.15**\n\nAnother key concept to keep straight is that of a heuristic value for a state. The word *heuristic* generally refers to a \"rule of thumb\", something that's helpful but not guaranteed to work.\n\n- Terminology:\n  - *Heuristic* - The word generally refers to a \"rule of thumb,\" something that may be helpful in some cases but not always. Generally held to be in contrast to \"guaranteed\" or \"optimal.\"\n\n---\n\n**Slide 2.2.16**\n\nA heuristic function has similar connotations. It refers to a function (defined on a state - not on a path) that may be helpful in guiding search but which is not guaranteed to produce the desired outcome. Heuristic searches generally make no guarantees on shortest paths or best anything (even when they are called best-first). Nevertheless, using heuristic functions may still provide help by speeding up, at least on average, the process of finding a goal.\n\n- Terminology\n  - *Heuristic* - The word refers to a \"rule of thumb,\" something that may be helpful in some cases but not always. Generally to be in contrast to \"guaranteed\" or \"optimal.\"\n  - *Heuristic function* - In search terms, a function that computes a value for a state (but does not depend on any path to that state) that may be helpful in guiding the search. There are two related forms of heuristic guidance that one sees."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_008.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.2.17\n\nIf we can get some estimate of the \"distance\u201d to a goal from the current node and we introduce a preference for nodes closer to the goal, then there is a good chance that the search will terminate more quickly. This intuition is clear when thinking about \"airline\" (as-the-crow-flies) distance to guide a search in Euclidean space, but it generalizes to more abstract situations (as we will see). \n\nTerminology\n- Heuristic\u2013The word generally refers to a \"rule of thumb,\u201d something that may be helpful in some cases but not always. Generally held to be in contrast to \"guaranteed\u201d or \"optimal.\u201d\n- Heuristic function\u2013In search situations, a function that computes a value for a state (but does not depend on any future path that maybe helpful in guiding the search.\n- Estimated distance to goal\u2013this type of heuristic function depends on the start and the goal. The classic example is straight-line distance used as an estimate for actual distance in a road network. This type of information can help increase the efficiency of a search.\n\nSlide 2.2.18\n\nImplementing the Search Strategies\n\nDepth-first:\n- Pick first element of Q\n- Add path extensions to front of Q\n\nBreadth-first:\n- Pick first element of Q\n- Add path extensions to end of Q\n\nBest-first:\n- Pick \u201cbest\u201d (measured by heuristic value of state) element of Q\n- Add path extensions anywhere in Q (may move closer to front if keep h\\(\\theta\\) ordered in some way so as to make it easier to find the best schedule\n\nBest-first (also known as \"greedy\") search is a heuristic (informed) search that uses the value of a heuristic function defined on the states to guide the search. This will not guarantee finding a \"best\" path, for example, the shortest path to a goal. The heuristic is used in the hope that it will steer us to a quick completion of the search or to a relatively good goal state.\n\nBest-first search can be implemented as follows: pick the \"best\" path (as measured by heuristic value of the node's state) from all of Q and add the extensions somewhere on Q. So, at any stage, we are always examining the pending node with the best heuristic value.\n\nNote that, in the worst case, this search will examine all the same paths that depth or breadth first would examine, but the order of examination may be different and therefore the resulting path will generally be different. Best-first has a kind of breadth-first flavor and we expect that Q will tend to grow more than in depth-first search.\n\nSlide 2.2.19\n\nNote that best-first search requires finding the best node in Q. This is a classic problem in computer science and there are many different approaches that are appropriate in different circumstances. One simple method is simply to scan the Q completely, keeping track of the best element found. Surprisingly, this simple strategy turns out to be the right thing to do in some circumstances. A more sophisticated strategy, such as keeping a data structure called a \"priority queue,\u201d is more often the correct approach. We will pursue this issue further when we talk about optimal searches.\n\nImplementation issues: Finding the best node\n- There are many possible approaches to finding the best node in Q.\n- Scanning Q to find lowest value\n- Sorting Q and picking the first element\n- Keeping the Q sorted by doing \"sorted\" insertions\n- Keeping Q as a priority queue\n- Which of these is best will depend among other things on how many children nodes have on average. We will look at this\n\nSlide 2.2.20\n\nWorst Case Running Time\nMax Time = Max #Visited\n\n- The number of states in search space may be exponential in some \"depth\u201d parameter, e.g., number of actions in a plan, number of moves in a game.\n\nLet's think a bit about the worst case running time of the searches that we have been discussing. The actual running time, of course, will depend on details of the computer and of the software implementation. But, we can roughly compare the various algorithms by thinking of the number of nodes added to Q. The running time should be roughly proportional to this number.\n\nIn AI we usually think of a \"typical\u201d search space as being a tree with uniform branching factor b and depth d. The depth parameter may represent the number of steps in a plan of action or the number of moves in a game. The branching factor reflects the number of different choices that we have at each step. It is easy to see that the number of states in such a tree grows exponentially with the depth.\n\nImage: A binary tree diagram with branching factor (b) = 2 and depth (d) = 3."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 2.2.21**\n\nIn a tree-structured search space, the nodes added to the search Q will simply correspond to the visited states. In the worst case, when the states are arranged in the worst possible way, all the search methods may end up having to visit or expand all of the states (up to some depth). In practice, we should be able to avoid this worst case but in many cases one comes pretty close to this worst case.\n\n**Worst Case Running Time**\n- The number of states in the search space may be exponential in some \"depth\" parameter, e.g., number of actions in a plan, number of moves in a game.\n- All the searchers without memory of visited list, may have to visit each state at least once, in the worst case.\n- So, all searchers will have worst case running time that is at least proportional to the total number of states and therefore to the \"depth\" parameter.\n\n**Slide 2.2.22**\n\nIn addition to thinking about running time, we should also think about the memory space required for searches. The dominant factor in the space requirements for these searches is the maximum size of the search Q. The size of the search Q in a tree-structured search space is simply the number of visited states minus the number of expanded states.\n\nFor a depth-first search, we can see that Q holds the unexpanded \"siblings\" of the nodes along the path that we are currently considering. In a tree, the path length cannot be greater than d and the number of unexpanded siblings cannot be greater than b-1, so this tells us that the length of Q is always less than b*d, that is, the space requirements are linear in d.\n\n**Worst Case Space**\n- Max Q Size = Max (#Visited - #Expanded):\n  - For breadth-first: b^d (states in tree)\n  - For depth-first: (b - 1)*d + 1\n\n**Slide 2.2.23**\n\nThe situation for breadth-first search is much different than that for depth-first search. Here the worst case happens after we\u2019ve visited all the nodes at depth d-1. At that point, all the nodes at depth d have been visited and none expanded. So, the Q has size b^d, that is, a size exponential in d.\n\nNote that, in the worst case, best-first behaves as breadth-first and has the same space requirements.\n\n**Worst Case Space**\n- Max Q Size = Max (#Visited - #Expanded):\n  - For depth-first: (b-1)\u2022d + bd\n  - For breadth-first: b^d\n\n**Slide 2.2.24**\n\n**Cost and Performance of Any-Path Methods**\n\nSearching a tree with branching factor b and depth d\n(without using a Visited list)\n\n| Search Method   | Time | Space    | Guaranteed to Find Path? |\n|-----------------|------|----------|---------------------------|\n| Depth-First     | b^d  | bd       | No                        |\n| Breadth-First   | b^d  | b^d      | Yes*                      |\n| Best-First      | b^d  | b^d      | Yes*                      |\n\n* If there are no infinitely long paths in the search space\n** Best-First needs more time to locate the best node in Q\n\n- Worst case time is proportional to number of nodes added to Q\n- Worst case space is proportional to maximal length of Q\n\nThis table summarizes the key cost and performance properties of the different any-path search methods. We are assuming that our state space is a tree and so we cannot revisit states and a Visited list is useless.\n\nRecall that this analysis is done for searching a tree with uniform branching factor b and depth d. Therefore, the size of this search space grows exponentially with the depth. So, it should not be surprising that methods that guarantee finding a path will require exponential time in this situation. These estimates are not intended to be tight and precise; instead they are intended to convey a feeling for the tradeoffs.\n\nNote that we could have phrased these results in terms of V, the number of vertices (nodes) in the tree, and then everything would have worst case behavior that is linear in V. We phrase it the way we do because in many applications, the number of nodes depends in an exponential way on some depth parameter. For example, the length of an action plan, and thinking of the cost as linear in the number of nodes is misleading. However, in the algorithms literature, many of these algorithms are described as requiring time linear in the number of nodes."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nThere are two points of interest in this table. One is the fact that depth-first search requires much less space than the other searches. This is important, since space tends to be the limiting factor in large problems (more on this later). The other is that the time cost of best-first search is higher than that of the others. This is due to the cost of finding the best node in Q, not just the first one. We will also look at this in more detail later.\n\nSlide 2.2.25\nRemember that we are assuming in this slide that we are searching a tree, so states cannot be visited more than once - so the Visited list is completely superfluous when searching trees. However, if we were to use a Visited list (even implemented as a constant-time access hash table), the only thing that seems to change in this table is that the worst-case space requirements for all the searches go up (and way up for depth-first search). That does not seem to be very useful! Why would we ever use a Visited list?\n\nCost and Performance of Any Path Methods\nSearching a tree unbounded by breadth k and depth d\n(handle infinity)\n\n| Search Method | Nodes Searched |\n| ------------- | --------------- |\n| Breadth-first | b ^ (d+1)       |\n| Depth-first   | b^m             |\n| Depth-limited | bl             |\n| Best-first    | bm              |\n\n| Worst-Case Time | Worst-Case Space |\n| --------------- | ---------------- |\n| b^(d+1) * t     | 1                |\n| b^m * (if there are no infinitely long paths in the search space) (2) Mail Find-Line node to locate the best node in Q) |\n| b/t             |\n| \u2014              | (2)              |\n\nWorst case time is proportional to number of nodes added to Q\nWorst case space is proportional to maximal length of Q (and Visited list)\n\nSlide 2.2.26\nAs we mentioned earlier, the key observation is that with a Visited list, our worst-case time performance is limited by the number of states in the search space (since you visit each state at most once) rather than the number of paths through the nodes in the space, which may be exponentially larger than the number of states, as this classic example shows. Note that none of the paths in the tree have a loop in them, that is, no path visits a state more than once. The Visited list is a way of spending space to limit this time penalty. However, it may not be appropriate for very large search spaces where the space requirements would be prohibitive.\n\nSlide 2.2.27\nSo far, we have been treating time and space in parallel for our algorithms. It is tempting to focus on time as the dominant cost of searching and, for real-time applications, it is. However, for large offline applications, space may be the limiting factor.\n\nIf you do a back of the envelope calculation on the amount of space required to store a tree with branching factor 8 and depth 10, you get a very large number. Many real applications may want to explore bigger spaces.\n\nSpace (the final frontier)\n\n- In large search problems, memory is often the limiting factor.\n- Imagine searching a tree with branching factor 8 and depth 10. Assume a node requires just 8 bytes of storage. Then, breadth-first search might require up to\n  (2^b)^k 2^2^23 * 8 bytes = 8,000 Mbytes = 80 Gbytes\n\n[Figure: States vs Paths]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.2.28\nOne strategy for enabling such open-ended searches, which may run for a very long time, is Progressive Deepening Search (aka Iterative Deepening Search). The basic idea is to simulate searches with a breadth-like component by a succession of depth-limited depth-first searches. Since depth-first has negligible storage requirements, this is a clean tradeoff of time for space. \n\nInterestingly, PDS is more than just a performance tradeoff. It actually represents a merger of two algorithms that combines the best of both. Let\u2019s look at that a little more carefully.\n\nSpace \n(the final frontier)\n\u2022 In large search problems, memory is often the limiting factor. \n\u2022 Imagine searching a tree with branching factor 10, depth 10. Assume a node requires 10 bytes of storage. Then, breadth-first search might require up to\n    o (b^d) = (10^10) = 10^10 Bytes = 10,000 MBytes = 10 GBytes = 10 Bytes\n\n\u2022 One strategy is to trade time for memory. For example, we can emulate breadth-first search by repeated applications of depth-first search, each up to a preset depth limit. This is called progressive deepening search (PDS):\n  1. Call\n  2. DFS to max depth d_1. If path found, return it.\n  3. Otherwise, increment d_1 and Go to 2.\n\nSlide 2.2.29\nDepth-first search has one strong point - its limited space requirements, which are linear in the depth of the search tree. Aside from that there\u2019s not much that can be said for it. In particular, it is susceptible to \u201cgoing off the deep-end\u201d, that is, chasing very deep (possibly infinite) deep paths. Because of this it does not guarantee, as breadth-first, does to find the shallowest goal states - those requiring the fewest actions to reach.\n\nSlide 2.2.30\nBreadth-first search on the other hand, does guarantee finding the shallowest goal, but at the expense of space requirements that are exponential in the depth of the search tree.\n\nSlide 2.2.31\nProgressive-deepening search, on the other hand, has both limited space requirements of DFS and the strong optimality guarantee of BFS. Great! No?\n\nProgressive Deepening Search\nBest of Both Worlds\n\u2022 Depth-First Search (DFS) has small space requirements (linear in depth), but has major problems:\n  - DFS can run forever in search spaces with infinite length paths\n  - DFS does not guarantee of finding shallowest goal\n\n\u2022 Breadth-First Search (BFS) guarantees finding shallowest goal, even in the presence of infinite paths, but has one great problem:\n  - BFS requires a great deal of space (exponential in depth)\n\nProgressive Deepening Search\nBest of Both Worlds\n\u2022 Depth-First Search (DFS) has small space requirements (linear in depth), but has major problems:\n  - DFS can run forever in search spaces with infinite length paths\n  - DFS does not guarantee of finding shallowest goal\n\n\u2022 Breadth-First Search (BFS) guarantees finding shallowest goal, even in the presence of infinite paths, but has one great problem:\n  - BFS requires a great deal of space (exponential in depth)\n\n\u2022 Progressive Deepening Search (PDS) has the advantages of DFS and BFS.\n  - PDS has small space requirements (linear in depth)\n  - PDS guarantees finding shallowest goal"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_012.jpeg",
    "text": "Slide 2.2.32\nProgressive Deepening Search\n- Isn't Progressive Deepening (PDS) too expensive?\n\nAt first sight, most people find PDS horrifying. Isn\u2019t progressive deepening really wasteful? It looks at the same nodes over and over again...\n\nSlide 2.2.33\nIn small graphs, yes it is wasteful. But, if we really are faced with an exponentially growing space (in the depth), then it turns out that the work at the deepest level dominates the total cost.\n\nSlide 2.2.34\nIt's easy to see this for binary trees, where the number of nodes at level d is about equal to the number of nodes in the rest of the tree. The worst-case time for BFS at level d is proportional to the number of nodes at level d, while the worst case time for PDS at that level is proportional to the number of nodes in the whole tree which is almost exactly twice those at the deepest level. So, in the worst case, PDS (for binary trees) does no more than twice as much work as BFS, while using much less space.\n\nThis is a worst case analysis, it turns out that if we try to look at the expected case, the situation is even better.\n\nSlide 2.2.35\nOne can derive an estimate of the ratio of the work done by progressive deepening to that done by a single depth-first search: (b^(d+1)-1)/(b-1). This estimate is for the average work (averaging over all possible searches in the tree). As you can see from the table, this ratio approaches one as the branching factor increases (and the resulting exponential explosion gets worse).\n\nProgressive Deepening Search\n- Isn't Progressive Deepening (PDS) too expensive?\n- In exponential trees, time is dominated by deepest search.\n\nFor example, if branching factor is 2, then the number of nodes at depth d is 2\u207f while total number of nodes in all previous levels is 2\u207f-1, so the difference between looking d whole tree versus only the deepest level is worst a factor of 2 in performance.\n\nFigure\n\nb | ratio\n2 | 3\n3 | 5\n5 | 11.66\n25 | 100\n100 | 100"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n-------------------------------------------\n\nProgressive Deepening Search\n\n- Compare the ratio of average time spent on PDS with average time spent on a single DFS with the full depth there.\n  (Average for DFS(N^d) time for DFS) = [d/N]^[d-1])\n- Progressive deepening is an effective strategy for difficult searches.\n\nb  r=100\nd\n2     3\n3     2\n5     15\n25    ~100\n100   102\n\nSlide 2.2.36:\nFor many difficult searches, progressive deepening is in fact the only way to go. There are also progressive deepening versions of the optimal searches that we will see later, but that's beyond our scope.\n\n-------------------------------------------\n\n6.034 Notes: Section 2.3\n\nSlide 2.3.1:\nWe will now step through the any-path search methods looking at their implementation in terms of the simple algorithm. We start with depth-first search using a Visited list.\n\nThe table in the center shows the contents of Q and of the Visited list at each time through the loop of the search algorithm. The nodes in Q are indicated by reversed paths, blue is used to indicate newly added nodes (paths). On the right is the graph we are searching and we will label the state of the node that is being extended at each step.\n\n-------------------------------------------\n\nDepth-First\n\nPick first element of Q. Add path extensions to front of Q.\n\nQ   Visited\n\n0\n1\n2\n3\n4\n5\n\nAdded paths in blue\nWe show the paths in reversed order; the node's state is the first entry.\n\n-------------------------------------------\n\nSlide 2.3.2:\nThe first step is to initialize Q with a single node corresponding to the start state (S in this case) and the Visited list with the start state.\n\n-------------------------------------------\n\nDepth-First\n\nPick first element of Q. Add path extensions to front of Q.\n\nQ   Visited\n\n1 (S) S\n2\n3\n4\n5\n\nAdded paths in blue\nWe show the paths in reversed order; the node's state is the first entry.\n\n-------------------------------------------"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.3  \nWe pick the first element of Q, which is that initial node, remove it from Q, extend its path to its descendant states (if they have not been Visited) and add the resulting nodes to the front of Q. We also add the states corresponding to these new nodes to the Visited list. So, we get the situation on line 2.\n\nNote that the descendant nodes could have been added to Q in the other order. This would be absolutely valid. We will typically add nodes to Q in such a way that we end up visiting states in alphabetical order, when no other order is specified by the algorithm. This is purely an arbitrary decision.\n\nWe then pick the first node on Q, whose state is A, and repeat the process, extending to paths that end at C and D and placing them at the front of Q.\n\nSlide 2.3.4  \nWe pick the first node, whose state is C, and note that there are no descendants of C and so no new nodes to add.\n\nSlide 2.3.5  \nWe pick the first node of Q, whose state is D, and consider extending to states C and G, but C is on the Visited list so we do not add that extension. We do add the path to G to the front of Q.\n\nSlide 2.3.6  \nWe pick the first node of Q, whose state is G, the intended goal state, so we stop and return the path.\n\n--- Tables ---\n\nPick first element of Q. Add path extensions to front of Q\n\n1. Q: 0, Visited: \\[ (S) \\], S\n   2. Q: \\[ A\\(S\\), B\\(S\\) \\], S, Visited: A, B, S\n   3. Q: \\[ C\\(A, S\\), D\\(A, S\\), B\\(S\\) \\], S, Visited: C, D, B, A, S\n   4. Q: \\[ A\\(S\\), B\\(S\\) \\], S, Visited: C, D, B, A, S\n   5. Q: \\[ G\\(D, B, A, S\\) \\], S, Visited: C, D, B, A, S\n\nAdded paths in blue \nWe show the paths in reversed order; the node\u2019s state is the first entry. \n\n--- Graph ---\n\n  - An unlabelled graph showing nodes and edges with states indicated as follows:\n    - Initial state: 0\n    - Extensions: A, B\n    - From A: C, D\n    - From D: G \n  - We pick G as the final state in every case."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.7\nThe final path returned goes from S to A, then to D and then to G.\n\n[Diagram illustrating Depth-First search]\n\nPick first element off Q. Add path extensions to front of Q.\n| Step | Q               | Visited |\n|------|-----------------|---------|\n| 0    | [(S)]          |         |\n| 1    | [(A (S)) (D (S)) B (S))] | S       |\n| 2    | [A (S) (S)]              | A, B, S |\n| 3    | [C (A A (S)) G (A (S))  E (A (S))] | C, D, B, A, S |\n| 4    | [F (A B B B A (S)) (S)]            | C, D, B, A, S |\n| 5    | [D (A A B B S)]                    | C, D, B, A, S |\n\nAdded paths in blue\nWe show the paths in reversed order; the node\u2019s state is the first entry.\n\nSlide 2.3.8\nTracing out the content of Q can get a little monotonous, although it allows one to trace the performance of the algorithms in detail. Another way to visualize simple searches is to draw out the search tree, as shown here, showing the result of the first expansion in the example we have been looking at.\n\n[Diagram illustrating Depth-First search tree with nodes labeled 1, 2, 3]\n\nSlide 2.3.9\nIn this view, we introduce a left to right bias in deciding which nodes to expand - this is purely arbitrary. It corresponds exactly to the arbitrary decision of which nodes to add to Q first. Giving this bias, we decide to expand the node whose state is A, which ends up visiting C and D.\n\n(Left side: Diagram illustrating expanded node)\n\nSlide 2.3.10\nWe now expand the node corresponding to C, which has no descendants, so we cannot continue to go deeper. At this point, one talks about having to back up or backtrack to the parent node and expanding any unexpanded descendant nodes of the parent. If there were none at that level, we would continue to keep backing up to its parent and so on until an unexpanded node is found. We declare failure if we cannot find any remaining unexpanded nodes. In this case, we find an unexpanded descendant of A, namely D.\n\n(Right side: Diagram illustrating backtracking process)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_016.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.11\n\nSo, we expand D. Note that states C and G are both reachable from D. However, we have already visited C, so we do not add a node corresponding to that path. We add only the new node corresponding to the path to G.\n\n[Image on right]\nDepth-First\nAnother (easier?) way to see it\n\n[Graph diagram with arrows and nodes labeled A, B, C, D, G, S]\n\nNumbers indicate order pulled off of Q (expanded)\nDark blue fill=Visited & Expanded\nLight gray fill=Visited\n\nNB: C is not\nvisited again\n\nSlide 2.3.12\n\nWe now expand G and stop.\n\nThis view of depth-first search is the more common one (rather than tracing Q). In fact, it is in this view that one can visualize why it is called depth-first search. The red arrow shows the sequence of expansions during the search and you can see that it is always going as deep in the search tree as possible. Also, we can understand another widely used name for depth-first search, namely backtracking search. However, you should convince yourself that this view is just a different way to visualize the behavior of the Q-based algorithm.\n\n[Image]\nDepth-First\nAnother (easier?) way to see it\n\n[Graph diagram with arrows and nodes labeled A, B, C, D, G, S]\n\nNumbers indicate order pulled off of Q (expanded)\nDark blue fill=Visited & Expanded\nLight gray fill=Visited\n\nSlide 2.3.13\n\nWe can repeat the depth-first process without the Visited list and, as expected, one sees the second path to C added to Q, which was blocked by the use of the Visited list. I\u2019ll leave it as an exercise to go through the steps in detail.\n\nNote that in the absence of a Visited list, we still require that we do not form any paths with loops, so if we have visited a state along a particular path, we do not re-visit that state again in any extensions of the path.\n\n[Image]\nDepth-First (without Visited list)\n\nPick first element off Q; Add path extensions to front of Q\n\n[Queue with nodes]\n1 (S)\n2 (A) (S B)\n3 (C A S) (B A S) (B)  \n4 (B A S) (D A) (B)\n5 (B A) (D A) (B G)\n6 (D A S B) (B G)\n\n[Graph diagram with arrows and nodes labeled A, B, C, D, G, S]\nAdded paths in blue\nWe show the paths in reverse order; the node\u2019s state is the first entry.\n\nDo not extend a path to a state if the resulting path would have a loop.\n\nSlide 2.3.14\n\nLet's look now at breadth-first search. The difference from depth-first search is that new paths are added to the back of Q. We start as with depth-first with the initial node corresponding to S.\n\nBreadth-First\nPick first element off Q; Add path extensions to end of Q\n\n[Table]\nQ Visited\n1 (S)\n2   S\n\n[Added paths in blue]\nWe show the paths in reverse order; the node\u2019s state is the first entry.\n\n[Graph diagram with arrows and nodes labeled A, B, C, D, G, S]\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_017.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.15\nWe pick it and add paths to A and B, as before.\n\n[Table]\nBreadth-First\nPick first element of Q. Add path extensions to end of Q.\n---\n| Q        | Visited |\n|----------|---------|\n| (S)      | S       |\n| (A) (S) (B) (S) | A,B,S  |\n|          |         |\n|          |         |\n|          |         |\n|          |         |\n|          |         |\n---\n\nAdded paths in blue\nWe show the paths in reversed order; the node's state is the first entry.\n\n[Diagram showing nodes connected with paths]\n\nSlide 2.3.16\nWe pick the first node, whose state is A, and extend the path to C and D and add them to Q (at the back), and here we see the difference from depth-first.\n\n[Table]\n---\n| Q                  | Visited |\n|--------------------|---------|\n| (S)                | S       |\n| (A) (S) (B) (S)    | A,B,S   |\n| (C,A,S) (D,A,S) (B) (S) | C,D,A,S |\n|                     |         |\n|                     |         |\n|                     |         |\n---\n\nAdded paths in blue\nWe show the paths in reversed order; the node's state is the first entry.\n\nSlide 2.3.17\nNow, the first node in Q is the path to B so we pick that and consider its extensions to D and G. Since D is already Visited, we ignore that and add the path to G to the end of Q.\n\n[Table]\nBreadth-First\n---\n| Q                                | Visited       |\n|----------------------------------|---------------|\n| (S)                              | S             |\n| (A) (S) (B) (S)                  | A,B,S         |\n| (C,A,S) (D,A,S) (B) (S)          | C,D,A,S       |\n| (G,B,S) (C,A,S) (D,A,S) (B) (S)  | G,C,D,A,S     |\n|                                  |               |\n|                                  |               |\n|                                  |               |\n---\n\nAdded paths in blue\nWe show the paths in reversed order; the node's state is the first entry.\n\nSlide 2.3.18\nAt this point, having generated a path to G, we would be justified in stopping. But, as we mentioned earlier, we proceed until the path to the goal becomes the first path in Q.\n\n[Table]\nBreadth-First\n---\n| Q                                        | Visited            |\n|------------------------------------------|--------------------|\n| (S)                                      | S                  |\n| (A) (S) (B) (S)                          | A,B,S              |\n| (C,A,S) (D,A,S) (B) (S)                  | C,D,A,S            |\n| (G,B,S) (C,A,S) (D,A,S) (B) (S)          | G,C,D,A,S          |\n| (C,A,S) (D,A,S) (G,B,S) (C,A,S) (D,A,S) (B) (S) | G,C,D,A,S       |\n|                                          |                    |\n|                                          |                    |\n---\n\nAdded paths in blue\nWe show the paths in reversed order; the node's state is the first entry.\n* We could have stopped here, when the first path to the goal was generated.\n\n[Diagram showing nodes connected with paths]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_018.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.19\nWe now pull out the node corresponding to C from Q but it does not generate any extensions since C has no descendants.\n\n[Diagram]\n\nBreadth-First\nPick first element of Q. Add path extensions to end of Q\n\nQ               | Visited\n----------------|-------\n(S)             | S\n(A) (S) (B)     | A,B,S\n(D) (S) (C) (A) | C,D,B,A,S\n(A) (C) (S) (D) | G,C,D,A,S\n(D) (S) (B) (S) | G,C,D,A,S\n\nAdded paths in blue\nWe show the paths in reversed order, the node's state is the first entry.\n*We could have stopped here, when the first path to the goal was generated.\n\nSlide 2.3.20\nSo we pull out the path to D. Its potential extensions are to previously visited states and so we get nothing added to Q.\n\n[Diagram]\n\nBreadth-First\nPick first element of Q. Add path extensions to end of Q\n\nQ               | Visited\n----------------|-------\n(S)             | S\n(A) (S) (B)     | A,B,S\n(D) (S) (C) (A) | C,D,B,A,S\n(A) (C) (S) (D) | G,C,D,A,S\n(D) (S) (B) (S) | G,C,D,A,S\n\nSlide 2.3.21\nFinally, we get the path to G and we stop.\n\n[Diagram]\n\nBreadth-First\nPick first element of Q. Add path extensions to end of Q\n\nQ               | Visited\n----------------|-------\n(S)             | S\n(A) (S) (B)     | A,B,S\n(D) (S) (C) (A) | C,D,B,A,S\n(A) (C) (S) (D) | G,C,D,A,S\n(D) (S) (B) (S) | G,C,D,A,S\n(B) (S)          | G,C,D,A,S\n\nAdded paths in blue\nWe show the paths in reversed order, the node's state is the first entry.\n*We could have stopped here, when the first path to the goal was generated.\n\nSlide 2.3.22\nNote that we found a path with fewer states than we did with depth-first search, from S to B to G. In general, breadth-first search guarantees finding a path to the goal with the minimum number of states."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_019.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.23\nHere we see the behavior of breadth-first search in the search-tree view. In this view, you can see why it is called breadth-first -- it is exploring all the nodes at a single depth level of the search tree before proceeding to the next depth level.\n\nBreadth-First\nAnother (clearer?) way to see it\n\nNumbers indicate order pulled off of Q (expanded)\nDark blue fill = Visited & Expanded\nLight gray fill = Visited\n\n1\n2\n3\n\n4\n4\n6\n5\nNB: D is not visited again\n\nSlide 2.3.24\nWe can repeat the breadth-first process without the Visited list and, as expected, one sees multiple paths to C, D and G are added to Q, which were blocked by the Visited list earlier. I'll leave it as an exercise to go through the steps in detail.\n\nBreadth-First (without Visited list)\nPick first element of Q. Add path extensions to end of Q\nQ        Action\n1        (S)      \n2        (A S) (B S)\n3        (C A S)\n4        (C A S) (D B S)  (G B S)\n5        (B C A S) (D C A S) (G C A S) (G D B S)\n6        (B D S) (C D S)  (G D S) (A B S)\n7        (C G D S) (A G D S) (G A B S) \n8        (C G D B S)\n\nWe show the paths in reversed order, the node's state is the first entry.\nWe could have stopped here, when the first path to the goal was generated.\n\nSlide 2.3.25\nFinally, let's look at Best-First Search. The key difference from depth-first and breadth-first is that we look at the whole Q to find the best node (by heuristic value).\n\nWe start as before, but now we're showing the heuristic value of each path (which is the value of its state) in the Q, so we can easily see which one to extract next.\n\nSlide 2.3.26\nWe pick the first node and extend to A and B.\n\nBest-First\nPick \u201cbest\u201d (by heuristic value) element of Q. Add path extensions anywhere in Q\n\nQ        Visited\n1        (10 S)  \n2\n\n\nAdded paths in blue; heuristic value of node\u2019s state in front.\nWe show the paths in reversed order; the node\u2019s state is the first entry.\n\nHeuristic Values\nA=2\nB=3\nC=1\nD=4\nS=10\nG=0\n\nQ        Visited\n1        (10 S)\n2        S\n\n3        (2 A S) (3 B S)\n4\n5\n\nAdded paths in blue; heuristic value of node\u2019s state in front.\nWe show the paths in reversed order; the node\u2019s state is the first entry."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_020.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.27\nWe pick the node corresponding to A, since it has the best value (= 2) and extend to C and D.\n\n(Images of graphs with nodes and edges)\n\nBest-First\nPick \u201cbest\u201d (by heuristic value) element of Q. Add path extensions anywhere in Q.\n\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Q  \u2502Visited    \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021  \u2502(t (0) S)  \u2502\n\u25022  \u2502(2 A (S) B)\u2502\n\u25023  \u2502(4 C (A S))\u2502\n\u25024  \u25025          \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAdded paths in blue; heuristic value of node\u2019s state is in front.\nWe show the paths in reversed order; the node's state is the first entry.\n\nHeuristic Values:\nA=2 B=3 C=1 D=4 G=0 S=10\n\n-----------------------------------\nSlide 2.3.28\nThe node corresponding to C has the lowest value so we pick that one. That goes nowhere.\n\n(Images of graphs with nodes and edges)\n\nBest-First\nPick \u201cbest\u201d (by heuristic value) element of Q. Add path extensions anywhere in Q.\n\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Q  \u2502Visited       \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021  \u2502(t (0) S)     \u2502\n\u25022  \u2502(2 A (S) B)   \u2502\n\u25023  \u2502(4 C (A S) B) \u2502\n\u25024  \u2502(5 B (D) A S) \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAdded paths in blue; heuristic value of node\u2019s state is in front.\nWe show the paths in reversed order; the node's state is the first entry.\n\n-----------------------------------\nSlide 2.3.29\nThen, we pick the node corresponding to B which has lower value than the path to D and extend to G (not C because of previous Visit).\n\n(Images of graphs with nodes and edges)\n\nBest-First\nPick \u201cbest\u201d (by heuristic value) element of Q. Add path extensions anywhere in Q.\n\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Q  \u2502Visited       \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021  \u2502(t (0) S)     \u2502\n\u25022  \u2502(2 A (S) B)   \u2502\n\u25023  \u2502(4 C (A S) B) \u2502\n\u25024  \u2502(5 B (D A S)) \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAdded paths in blue; heuristic value of node\u2019s state is in front.\nWe show the paths in reversed order; the node's state is the first entry.\n\n-----------------------------------\nSlide 2.3.30\nWe pick the node corresponding to G and rejoice.\n\n(Images of graphs with nodes and edges)\n\nBest-First\nPick \u201cbest\u201d (by heuristic value) element of Q. Add path extensions anywhere in Q.\n\n\u250c\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Q  \u2502Visited        \u2502\n\u251c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u25021  \u2502(t (0) S)      \u2502\n\u25022  \u2502(2 A (S) B)    \u2502\n\u25023  \u2502(4 C (A S) B)  \u2502\n\u25024  \u2502(5 B (D A S))  \u2502\n\u25025  \u2502(G (B D A S))  \u2502\n\u2514\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\nAdded paths in blue; heuristic value of node\u2019s state is in front.\nWe show the paths in reversed order; the node's state is the first entry.\n\nHeuristic Values:\nA=2 B=3 C=1 D=4 G=0 S=10"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search1\\page_021.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.3.31\nWe found the path to the goal from S to B to G.\n\n[Image of graph with points A, B, C, D, G, S]\n\n                  Best-First\n\nPick \"best\" (by heuristic value) element of Q. Add path extensions anywhere in Q\n\nQ                             Visited\n1  [(10 S)]                     S\n\n2  [2 (A S) 9 (B S)]           A,B,S\n\n3  [(C A S) 9 (B S)] 4 (D A S) C,D,A,B,S\n\n4  [3 (B S) 4 (D A S)]          C,D,B,A\n\n5  [0 (B S G)] 4 (D A S)       G,C,D,A,B,S\n\n                      Heuristic Values\n\n                      A=2    C=1    S=10\n                      B=3    D=4\n                             G=0\n\nAdded paths in blue; heuristic value of node's state is in front.\nWe show the paths in reversed order; the node's state is the first entry. \n\n\u00a9 1998\u201304 ISC."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 2.4\n\nSlide 2.4.1\nSo far, we have looked at three any-path algorithms, depth-first and breadth-first, which are\nuninformed, and best-first, which is heuristically guided.\n\nSlide 2.4.2\nNow, we will look at the first algorithm that searches for optimal paths, as defined by a \"path\nlength\" measure. This uniform cost algorithm is uninformed about the goal, that is, it does not use\nany heuristic guidance.\n\nClasses of Search\n\nClass          | Name          | Operation\n-----------------------------------------------------------\nAny Path      | Depth-First   | Systematic exploration of whole tree\nUninformed   | Breadth-First | until a goal node is found.\n-----------------------------------------------------------\nAny Path      | Best-First    | Uses heuristic measure of goodness\nInformed      |                  | of a node, e.g. estimated distance to goal.\n\nClasses of Search\n\nClass             | Name            | Operation\n------------------------------------------------------------\nAny Path       | Depth-First     | Systematic exploration of whole tree\nUninformed    | Breadth-First  | until a goal node is found.\n------------------------------------------------------------\nAny Path       | Best-First      | Uses heuristic measure of goodness\nInformed        |                   | of a node, e.g. estimated distance to goal.\n------------------------------------------------------------\nOptimal         | Uniform-Cost   | Uses path \"length\" measure.\nUninformed    |                   | Finds \"shortest\" path."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.4.3\n\nThis is the simple algorithm we have been using to illustrate the various searches. As before, we will see that the key issues are picking paths from Q and adding extended paths back in.\n\nSimple Search Algorithm\n\nA search node is a path from some state X to the start st8, e.g. [X,B,A,S]\nThe state of a search node is the most recent label of the path, e.g. X\nLet Q be a list of search nodes, e.g. [[A,B,A,S],[B,A,S],[S]]\nLet S be the start/st8.\n\n1. Initialize Q with search node [S] as only entry; set Visited = {S}  //Visited = {S}\n2. IF Q is empty, fail. Else, pick one search node P from Q\n3. If st8(P) is a goal, return P (we've reached a goal)\n4. (Otherwise) Remove P from Q\n5. Find all the children of st8(P) not in Visited and create all the one-step extensions of P to each descendant.\n6. Add all the extended paths to Q; add children of st8(P) to Visited\n7. Go to step 2.\n\nCritical decisions:\nStep 2: picking P from Q\nStep 6: adding extensions of P to Q\n\nSlide 2.4.4\n\nWe will continue to use the algorithm but (as we will see) the use of the Visited list conflicts with optimal searching, so we will leave it out for now and replace it with something else later.\n\nSlide 2.4.5\n\nWhy can\u2019t we use a Visited list in connection with optimal searching? In the earlier searches, the use of the Visited list guaranteed that we would not do extra work by re-visiting or re-expanding states. It did not cause any failures then (except possibly of intuition).\n\nWhy not a Visited list?\n\n\u2022 For the any-path algorithms, the Visited list would not cause us to fail to find a path when one existed, since the path to a state did not matter.\n\nSlide 2.4.6\n\nBut, using the Visited list can cause an optimal search to overlook the best path. A simple example will illustrate this.\n\nWhy not a Visited list?\n\n\u2022 For the any-path algorithms, the Visited list would not cause us to fail to find a path when one existed, since the path to a state did not matter.\n\n\u2022 However, the Visited list in connection with optimal searches can cause us to miss the best path."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.4.7\nClearly, the shortest path (as determined by sum of link costs) to G is (S A D G) and an optimal search had better find it.\n\n![Diagram: Node graph with nodes S, A, D, G connected by links]\n\nFor the any-path algorithms, the Visited list would not cause us to fail to find a path when one existed, since the path to a state did not matter.\n\nHowever, the Visited list in connection with UC can cause us to miss the best path.\n\nThe shortest path from S to G is (S A D G)\n\nSlide 2.4.8\nWhy not a Visited list?\n\nFor the any-path algorithms, the Visited list would not cause us to fail to find a path when one existed, since the path to a state did not matter.\n\nHowever, the Visited list in connection with UC can cause us to miss the best path.\n\nThe shortest path from S to G is (S A D G)\n\nHowever, on expanding S, A and D are Visited, which means that the extension from A to D would never be generated and we would miss the best path, So, we can't use a Visited list; nevertheless, we still have the problem of multiple paths to a state leading to wasted work. We will deal with that issue later, since it can get a bit complicated. So, first, we will focus on the basic operation of optimal searches.\n\nSlide 2.4.9\nThe first, and most basic, algorithm for optimal searching is called uniform-cost search. Uniform-cost is almost identical in implementation to best-first search.That is, we always pick the best node on Q to expand. The only, but crucial, difference is that instead of assigning the node value based on the heuristic value of the node's state, we will assign the node value as the \"path length\" or \"path cost\", a measure obtained by adding the \"length\" or \"cost\" of the links making up the path.\n\nImplementing Optimal Search Strategies\n\nUniform Cost:\n- Pick best (measured by path length) element of Q\n- Add path extensions anywhere in Q\n\nSlide 2.4.10\nUniform Cost\n- Like best-first except that it uses the \"total length (cost)\" of a path instead of a heuristic value for the state.\n- Each link has a \"length\" or \"cost\" (which is always greater than 0)\n- We want \"shortest\" or \"least cost\" path\n\n[Diagram: Node graph with nodes S, A, B, C, D, E, G connected by links]\n\nTo reiterate, uniform-cost search uses the total length (or cost) of a path to decide which one to expand. Since we generally want the least-cost path, we will pick the node with the smallest path cost/length. By the way, we will often use the word \"length\" when talking about these types of searches, which makes intuitive sense when we talk about the pictures of graphs. However, we mean any cost measure (like length) that is positive and greater than 0 for the link between any two states."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.4.11\nThe path length is the SUM of the length associated with the links in the path. For example, the path from S to A to C has total length 4, since it includes two links, each with edge 2.\n\nUniform Cost\n- Like best-first except that it uses the \"total length (cost)\" of a path instead of a heuristic value for the state.\n- Each link has a \u201clength\u201d or \u201ccost\u201d (which is always greater than 0)\n- We want \u201cshortest\u201d or \u201cleast cost\u201d path\n\n                    2\n        S --------------- A\\\n        |                /\\ 1\n        |            2  /\n   5    |               /\n        | 4          C\n        |        \\ 2\n       B ---------------/- D\n                1     /\n\nTotal path cost:\n (S A C)  4\n\nSlide 2.4.12\nThe path from S to B to D to G has length 8 since it includes links of length 5 (S-B), 1 (B-D) and 2 (D-G).\n\nSlide 2.4.13\nSimilarly for S-A-D-C.\n\n                    3\n        S --------------- A\\\n        |                /\\ 1\n        | 4          C\n        |        \\ 2\n       B ---------------/- D\n                1     /\n\nTotal path cost:\n (S B D G)  8\n\n                    3\n        S --------------- A\\\n        |                /\\ 1\n        | 4          C\n        |        \\ 2\n       B ---------------/- D\n                1     /\n\nUniform Cost\n- Like best-first except that it uses the \"total length (cost)\" of a path instead of a heuristic value for the state.\n- Each link has a \u201clength\u201d or \u201ccost\u201d (which is always greater than 0)\n- We want \u201cshortest\u201d or \u201cleast cost\u201d path\n\nTotal path cost:\n (S A C)  4\n (S B D G) 8\n (S A D C) 9\n\nSlide 2.4.14\nGiven this, let\u2019s simulate the behavior of uniform-cost search on this simple directed graph. As usual we start with a single node containing just the start state S. This path has zero length. Of course, we choose this path for expansion.\n\nQ\n/----------------------\\\n| (0 S)                |\n\\----------------------/\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry.\n\n                    2\n        S --------------- A\\\n        |                /\\ 1\n        |            2  /\n   5    |               /\n        | 4          C\n        |        \\ 2  \\ \n       B ---------------/- D\n                1     /\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.4.15\nThis generates two new entries on Q; the path to A has length 2 and the one to B has length 5. So, we pick the path to A to expand.\n\nUniform Cost\nPick best (by path length) element of Q; Add path extensions anywhere in Q\n\n-------------------------------------------------\n|   0    |  (1 S)                                     |\n-------------------------------------------------\n|   2    |  (2 A S), (6 B S)                       |\n-------------------------------------------------\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry.\n\nSlide 2.4.16\nThis generates two new entries on the queue. The new path to C is the shortest path on Q, so we pick it to expand.\n\nUniform Cost\nPick best (by path length) element of Q; Add path extensions anywhere in Q\n\n-------------------------------------------------\n|   0    |  (1 S)                                     |\n-------------------------------------------------\n|   2    |  (2 A S), (6 B S)                       |\n-------------------------------------------------\n|           |   (4 C A S), (6 D A S), (6 B S)    |\n-------------------------------------------------\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry.\n\nSlide 2.4.17\nSince C has no descendants, we add no new paths to Q and we pick the best of the remaining paths, which is now the path to B.\n\nUniform Cost\nPick best (by path length) element of Q; Add path extensions anywhere in Q\n\n-------------------------------------------------------\n|   0    |  (1 S)                                             |\n-------------------------------------------------------\n|   2    |  (2 A S), (6 B S)                               |\n-------------------------------------------------------\n|   4    |  (4 C A S)                                         |\n-------------------------------------------------------\n|           |   (6 B S), (6 D A S)                          |\n-------------------------------------------------------\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry.\n\nSlide 2.4.18\nThe path to B is extended to D and G, and the path to D from B is tied with the path to D from A.\nWe are using order in Q to settle ties and so we pick the path from B to expand. Note that at this point G has been visited but not expanded.\n\nUniform Cost\nPick best (by path length) element of Q; Add path extensions anywhere in Q\n\n-------------------------------------------------------\n|   0    |  (1 S)                                             |\n-------------------------------------------------------\n|   2    |  (2 A S), (6 B S)                               |\n-------------------------------------------------------\n|   4    |  (4 C A S), (6 D A S), (8 S)               |\n-------------------------------------------------------\n|           |  (6 B S), (8 D B S), (10 G S)       |\n-------------------------------------------------------\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_006.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.4.19\nExpanding D adds paths to C and G. Now the earlier path to D from A is the best pending path and we choose it to expand.\n\nSlide 2.4.20\nThis adds a new path to G and a new path to C. The new path to G is the best on the Q (at least tied for best) so we pull it off Q.\n\nSlide 2.4.21\nAnd we have found our shortest path (S A D G) whose length is 8.\n\nSlide 2.4.22\nNote that once again we are not stopping on first visiting (placing on Q) the goal. We stop when the goal gets expanded (pulled off Q).\n\nImage: Diagram with nodes and paths labeled with numbers. Nodes shown include S, A, B, C, D, G. Paths have weights like 1, 3, 7, 6.\n\nText boxes:\nUniform Cost\n1. Pick best (by path length) element off Q. Add path extensions anywhere in Q.\n2. Paths listed in reverse order, node's state is the first entry.\n\nWhy not stop on first visiting a goal?\n- When doing Uniform Cost, it is not correct to stop the search when the first path to a goal is generated, that is, when a node whose state is a goal is added to Q.\n\nTable Example:\n- Added paths in blue, underlined paths are chosen for extension. \n- We show the paths in reverse order, the node's state is the first entry."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_007.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 2.4.23**\nIn uniform-cost search, it is imperative that we only stop when G is expanded and not just when it is visited. Until a path is first expanded, we do not know for a fact that we have found the shortest path to the state.\n\n---\n\n**Why not stop on first visiting a goal?**\n- When doing Uniform Cost, it is not correct to stop the search when the first path to a goal is generated, that is, when a node whose state is a goal is added to Q.\n- We must wait until such a path is pulled off the Q and tested in step 3. It is only at this point that we are sure it is the shortest path to a goal since there are no other shorter paths that remain unexpanded.\n\n**(Slide)**\n\n---\n\n**Slide 2.4.24**\nIn the any-path searches we chose to do the same thing, but that choice was motivated at the time simply by consistency with what we HAVE to do now. In the earlier searches, we could have chosen to stop when visiting a goal state and everything would still work fine (actually better).\n\n\n---\n\n**Slide 2.4.25**\nNote that the first path that visited G was not the eventually chosen optimal path to G. This explains our unwillingness to stop on first visiting G in the example we just did.\n\n\n---\n\n**Why not stop on first visiting a goal?**\n- When doing Uniform Cost, it is not correct to stop the search when the first path to a goal is generated, that is, when a node whose state is a goal is added to Q.\n- We must wait until such a path is pulled off the Q and tested in step 3. It is only at this point that we are sure it is the shortest path to a goal since there are no other shorter paths that remain unexpanded.\n- This contrasts with any-path searches where the choice of where to test for a goal was a matter of convenience and efficiency, not correctness.\n- In the previous example, a path to G was generated at step 5, but it was a different, shorter, path at step 7 that we accepted.\n\n**(Slide)**\n\n---\n\n**Slide 2.4.26**\n\nUniform Cost\n\nAnother (easier?) way to see it\n\n[(Diagram with nodes A, B, C, D, E, F, G)]\n\nUC enumerates paths in order of total path cost!\n\n- The diagram illustrates a graph with nodes and edges labeled with costs.\n- Numerical labels next to nodes indicate path costs (0, 1, 3, 6, etc.)\n\nIt is very important to drive home the fact that what uniform-cost search is doing (if we focus on the sequence of expanded paths) is enumerating the paths in the search tree in order of their path cost. The green numbers next to the tree on the left are the total path cost of the path to that state. Since, in a tree, there is a unique path from the root to any node, we can simply label each node by the length of that path."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_008.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.4.27\nSo, for example, the trivial path from S to S is the shortest path.\n\nUniform Cost\nAnother (easier?) way to see it\n\n2                 3\nS\u25cf--\u25cfA          \u25cfC\n  2  |          |  4\n   \\ |          | /\n    \\|          |/\n     \u25cfB---------\u25cf\n      10\n\nUC enumerates paths in order of total path cost\nOrder pulled off of \u03b1 (expanded)\n\nSlide 2.4.28\nThen the path from S to A, with length 2, is the next shortest path.\n\nUniform Cost\nAnother (easier?) way to see it\n\n2                 3\nS\u25cf--\u25cfA          \u25cfC\n  2  |          |  4\n   \\ |          | /\n    \\|          |/\n     \u25cfB---------\u25cf\n      10\n\nUC enumerates paths in order of total path cost\nOrder pulled off of \u03b1 (expanded)\n\nSlide 2.4.29\nThen the path from S to A to C, with length 4, is the next shortest path.\n\nUniform Cost\nAnother (easier?) way to see it\n\n2                 3\nS\u25cf--\u25cfA          \u25cfC\n  2  |          |  4\n   \\ |          | /\n    \\|          |/\n     \u25cfB---------\u25cf\n      10\n\nUC enumerates paths in order of total path cost\nOrder pulled off of \u03b1 (expanded)\n\nSlide 2.4.30\nThen comes the path from S to B, with length 5.\n\nUniform Cost\nAnother (easier?) way to see it\n\n2                 3\nS\u25cf--\u25cfA          \u25cfC\n  2  |          |  4\n   \\ |          | /\n    \\|          |/\n     \u25cfB---------\u25cf\n      10\n\nUC enumerates paths in order of total path cost\nOrder pulled off of \u03b1 (expanded)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.4.31  \nFollowed by the path from S to A to D, with length 6.\n\n[image of graph with nodes and paths marked with numbers in purple and blue]\n\nUniform Cost\nAnother (easier?) way to see it\n\n[arrows with costs shown, some bolded]\n\nUC enumerates paths in order of total path cost!\n\nSlide 2.4.32\nAnd the path from S to B to D, also with length 6.\n\n[image of graph with nodes and paths marked with numbers in purple and blue]\n\nUniform Cost\nAnother (easier?) way to see it\n\n[arrows with costs shown, some bolded]\n\nUC enumerates paths in order of total path cost!\n\nSlide 2.4.33\nAnd, finally the path from S to A to D to G with length 8. The other path (S B D G) also has length 8.\n\n[image of graph with nodes and paths marked with numbers in purple and blue]\n\nUniform Cost\nAnother (easier?) way to see it\n\n[arrows with costs shown, some bolded]\n\nUC enumerates paths in order of total path cost!\n\nSlide 2.4.34\nThis gives us the path we found. Note that the sequence of expansion corresponds precisely to path-length order, so it is not surprising we find the shortest path.\n\n[image of graph with nodes and paths marked with numbers in purple and blue]\n\nUniform Cost\nAnother (easier?) way to see it\n\n[arrows with costs shown, some bolded]\n\nUC enumerates paths in order of total path cost!"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n# 6.034 Notes: Section 2.5\n\n---\n\n## Slide 2.5.1\n\nNow, we will turn our attention to what is probably the most popular search algorithm in AI, the A* algorithm. A* is an informed, optimal search algorithm. We will spend quite a bit of time going over A*; we will start by contrasting it with uniform-cost search.\n\n### Classes of Search\n\n| Class               | Name         | Operation                                                                                                                    |\n|---------------------|--------------|-------------------------------------------------------------------------------------------------------------------------------|\n| Any Path            | Uninformed   | Depth-First/ Breadth-First | Systematic exploration of whole tree until a goal node is found           |\n| Any Path            | Informed     | Best-First  | Uses heuristic measure of goodness of a node, e.g. estimated distance to goal    |\n| Optimal             | Uninformed   | Uniform-Cost | Uses path \"length\" measure, finds \"shortest path.\"                                           |\n| Optimal             | Informed     | A*             | Uses path \"length\" measure and heuristic. Finds \"shortest path.\"                                    |\n\n---\n\n## Slide 2.5.2\n\n**Goal Direction**\n\n- UC [Uniform-Cost] is really trying to identify the shortest path to every state in the graph in order. It has no particular bias to finding a path to a goal early in the search.\n\nUniform-cost search as described so far is concerned only with expanding short paths; it pays no particular attention to the goal (since it has no way of knowing where it is). UC is really an algorithm for finding the shortest paths to all states in a graph rather than being focused in reaching a particular goal.\n\n---\n\n## Slide 2.5.3\n\nWe can bias UC to find the shortest path to the goal that we are interested in by using a heuristic estimate of remaining distance to the goal. This, of course, cannot be the exact path distance (if we knew that we would not need much of a search); instead, it is a stand-in for the actual distance that can give us some guidance.\n\n**Goal Direction**\n\n- UC is really trying to identify the shortest path to every state in the graph in order. It has no particular bias to finding a path to a goal early in the search.\n- We can introduce such a bias by means of heuristic function (h(n)), which is an estimate (h) of the distance from a state to the goal."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 2.5.4\n\nGoal Direction\n- UC is really trying to identify the shortest path to every state in the graph in order. It has no particular bias to finding a path to a goal early in the search.\n- We can introduce such a bias by means of heuristic function h(n), which is an estimate of the distance from a state to the goal.\n- Instead of enumerating paths in order of just length g, enumerate paths in terms of f = estimated total path length g + h.\n\n---\n\nWhat we can do is to enumerate the paths by order of the SUM of the actual path length and the estimate of the remaining distance. Think of this as our best estimate of the TOTAL distance to the goal. This makes more sense if we want to generate a path to the goal preferentially to short paths away from the goal.\n\n---\n\nSlide 2.5.5\n\nWe call an estimate that always underestimates the remaining distance from any node an admissible (heuristic) estimate.\n\n---\n\nSlide 2.5.6\n\nIn order to preserve the guarantee that we will find the shortest path by expanding the partial paths based on the estimated total path length to the goal (like in UC without an expanded list), we must ensure that our heuristic estimate is admissible. Note that straight-line distance is always an underestimate of path-length in Euclidean space. Of course, by our constraint on distances, the constant function 0 is always admissible (but useless).\n\nGoal Direction\n- UC is really trying to identify the shortest path to every state in the graph in order. It has no particular bias to finding a path to a goal early in the search.\n- We can introduce such a bias by means of heuristic function h(n), which is an estimate of the distance from a state to the goal.\n- Instead of enumerating paths in order of just length g, enumerate paths in terms of f = estimated total path length g + h.\n- An estimate that always underestimates the real path length to the goal is called admissible. For example, an estimate of 0 is admissible (but useless). Straight-line distance is admissible estimate for path length in Euclidean space.\n- Use of an admissible estimate guarantees that UC will find the shortest path.\n\n---\n\nSlide 2.5.7\n\nUC using an admissible heuristic is known as A* (A star). It is a very popular search method in AI.\n\nGoal Direction\n- UC is really trying to identify the shortest path to every state in the graph in order. It has no particular bias to finding a path to a goal early in the search.\n- We can introduce such a bias by means of heuristic function h(n), which is an estimate of the distance from a state to the goal.\n- Instead of enumerating paths in order of just length g, enumerate paths in terms of f = estimated total path length g + h.\n- An estimate that always underestimates the real path length to the goal is called admissible. For example, an estimate of 0 is admissible (but useless). Straight-line distance is admissible estimate for path length in Euclidean space.\n- Use of an admissible estimate guarantees that UC will find the shortest path.\n- UC with an admissible estimate is known as A* (pronounced \u2018A star\u2019) search."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_012.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nStraight-Line Estimate\n\nSlide 2.5.8\n\nLet's look at a quick example of the straight-line distance underestimate for path length in a graph. Consider the following simple graph, which we are assuming is embedded in Euclidean space, that is, think of the states as city locations and the length of the links are proportional to the driving distance between the cities along the best roads.\n\n---\n\nSlide 2.5.9\n\nStraight-Line Estimate\n\nThen, we can use the straight-line (airline) distances (shown in red) as an underestimate of the actual driving distance between any city and the goal. The best possible driving distance between two cities cannot be better than the straight-line distance. But, it can be much worse.\n\n---\n\nStraight-Line Estimate\n\nSlide 2.5.10\n\nHere we see that the straight-line estimate between B and G is very bad. The actual driving distance is much longer than the straight-line underestimate. Imagine that B and G are on different sides of the Grand Canyon, for example.\n\n---\n\nSlide 2.5.11\n\nIt may help to understand why an underestimate of remaining distance may help reach the goal faster to visualize the behavior of UC in a simple example.\n\nImagine that the states in a graph represent points in a plane and the connectivity is to nearest neighbors. In this case, UC will expand nodes in order of distance from the start point. That is, as time goes by, the expanded points will be located within expanding circular contours centered on the start point. Note, however, that points heading away from the goal will be treated just the same as points that are heading towards the goal.\n\n---\n\nWhy use estimate of goal distance?\n\nOrder in which UC looks at states. A and B are same distance from start, S will be looked at before any longer path. No \"bias\" towards goal.\n\nAssume states are points in the Euclidean plane."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 2.5.11**\n\nWhy use estimate of goal distance?\n\nOrder in which U we look at states. A and B are same distance from start, so I D will be looked at before any longer paths. No \"bias\" towards goal.\n\nA\n\nE\n\nB\n\nC\n\ngoal\n\nstart\n\nD\n\nOrder of examination using dist. from start + estimate of dist. to goal. Note \u201cbias\u201d toward the goal, points newly from goal look worse.\n\nAssume states are points in the Euclidean plane.\n\n---\n\n**Slide 2.5.12**\n\nIf we add in an estimate of the straight-line distance to the goal, the points expanded will be bounded contours that keep constant the sum of the distance from the start and the distance to the goal, as suggested in the figure. What the underestimate has done is to \"bias\" the search towards the goal.\n\n---\n\n**Slide 2.5.13**\n\nLet's walk through an example of A*, that is, uniform-cost search using a heuristic which is an underestimate of remaining cost to the goal. In this example we are focusing on the use of the underestimate. The heuristic we will be using is similar to the earlier one but slightly modified to be admissible.\n\nWe start at S as usual.\n\n---\n\nA*\n\nPick best (by path length+heuristic) element of Q. Add path extensions anywhere in Q.\n\nQ\n\n0    (S)\n1\n2\n\n1 \n\n2 \n\nA=2\nB=3\nC=1\nD=f=1\nG=0\n\nHeuristic Values\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node's state in the first entry.\n\n---\n\n**Slide 2.5.14**\n\nAnd expand to A and B. Note that we are using the path length + underestimate and so the S-A path has a value of 4 (length 2, estimate 2). The S-B path has a value of 8 (5 + 3). We pick the path to A.\n\nQ\n\n0    (S)\n1    (S)\n2    (A A S) (B S B 9)\n\nA=2\nB=3\nC=1\nD=1\nG=0\n\nHeuristic Values\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node's state in the firest entry.\n\n---\n\n**Slide 2.5.15**\n\nExpand to C and D and pick the path with shorter estimate, to C.\n\nQ\n\n0    (S)\n1    (A AS) (B S B 9)\n2    (C A S T D & A A S)\n\nA=4\nC=1\nD=1\nG=0\n\nHeuristic Values\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in the reversed order; the node's state in the firest entry."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n[Image of a graph with paths labeled from A to B to C to D and heuristic values in a table below it]\n\nA* \nPick best (by path length+heuristic) element of Q. Add path extensions anywhere in Q.\nQ\n1  (0)  S\n2  (4+A) S (A)\n3  (5+B) S (B) \n4  (6+C) S (A) (C) \n5  (6+D) S (A) (B) (D) \nHeuristic Values\nA=2  C=1  S=0\nB=3  D=1  G=0\n\nAdded paths in blue; underlined paths are chosen for extension.\nWe show the paths in reversed order; the node\u2019s state is in the first entry.\n\nSlide 2.5.16\nC has no descendants, so we pick the shorter path (to D).\n\n[Another graph image with highlighted paths and the same table format for Q]\n\nSlide 2.5.17\nThen a path to the goal has the best value. However, there is another path that is tied, the S-B path. It is possible that this path could be extended to the goal with a total length of 8 and we may prefer that path (since it has fewer states). We have assumed here that we will ignore that possibility, in some other circumstances that may not be appropriate.\n\nA* \nPick best (by path length+heuristic) element of Q. Add path extensions anywhere in Q.\nQ\n1  (0)  S\n2  (4+A) S (A)\n3  (5+B) S (B)\n4  (8) S (A) (C) (A) (G)\n5  (6+D) S (A) (B) (S)\nHeuristic Values\nA=2  C=1  S=0\nB=3  D=1  G=0\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is in the first entry.\n\nSlide 2.5.18\nSo, we stop with a path to the goal of length 8.\n\nSlide 2.5.19\nIt is important to realize that not all heuristics are admissible. In fact, the rather arbitrary heuristic values we used in our best-first example are not admissible given the path lengths we later assigned. In particular, the value for D is bigger than its distance to the goal and so this set of distances is not everywhere an underestimate of distance to the goal from every node. Note that the (arbitrary) value assigned for S is also an overestimate but this value would have no ill effect since at the time S is expanded there are no alternatives.\n\nNot all heuristics are admissible\nGiven the link lengths in the figure, is the table of heuristic values that we used in our earlier best-first example an admissible heuristic?\nNo! \nA is ok\nB is ok\nC is ok\nD is too big, needs to be < = 2\nB=3  D=4  G=0\nS is too big, can always use 0 for start\n\n[Graph with paths and heuristic values table displayed]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search2\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n(Picture showing two 8-puzzle states and an arrow indicating a shift of the \"empty\u201d tile to the right.)\n\nSlide 2.5.20\nAdmissible Heuristics\n8 Puzzle: Move tiles to reach goal. Think of move as moving \"empty\u201d tile.\n\n0       0\n\n8       8\n\nAlternative underestimates of \"distance\u201d (number of moves) to goal:\n\n1. Number of misplaced tiles (7 in example above)\n\nAlthough it is easy and intuitive to illustrate the concept of a heuristic by using the notion of straight-line distance to the goal in Euclidean space, it is important to remember that this is by no means the only example.\n\nTake solving the so-called 8-puzzle, in which the goal is to arrange the pieces as in the goal state on the right. We can think of a move in this game as sliding the \"empty\u201d space to one of its nearest vertical or horizontal neighbors. We can help steer a search to find a short sequence of moves by using a heuristic estimate of the moves remaining to the goal.\n\nOne admissible estimate is simply the number of misplaced tiles. No move can get more than one misplaced tile into place, so this measure is a guaranteed underestimate and hence admissible.\n\nSlide 2.5.21\n\nWe can do better if we note that, in fact, each move can at best decrease by one the \"Manhattan\u201d (aka Taxicab, aka rectilinear) distance of a tile from its goal.\n\nSo, the sum of these distances for each misplaced tile is also an underestimate. Note that it is always a better (larger) underestimate than the number of misplaced tiles. In this example, there are 7 misplaced tiles (all except tile 2), but the Manhattan distance estimate is 17 (4 for tile 1, 0 for tile 2, 2 for tile 3, 3 for tile 4, 1 for tile 5, 3 for tile 6, 1 for tile 7 and 3 for tile 8).\n\n8 Puzzle: Move tiles to reach goal. Think of move as moving \"empty\u201d tile.\n\n(Picture showing two 8-puzzle states and an arrow indicating a shift of the \"empty\u201d tile to the right.)\n\n0       0\n\n8       8\n\nAlternative underestimates of \u201cdistance\u201d (number of moves) to goal:\n\n1. Number of misplaced tiles (7 in example above)\n2. Sum of Manhattan distance of tile to its goal location (17 in example above). Manhattan distance between (x-gx) and (y-gy) is |x-gx|+|y-gy|. Each move can only decrease the distance of each tile.\nThe second of these is much better at predicting actual number of moves."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 2.6\n\nSlide 2.6.1\nIn our discussion of uniform-cost search and A* so far, we have ignored the issue of revisiting states. We indicated that we could not use a Visited list and still preserve optimality, but can we use something else that will keep the worst-case cost of a search proportional to the number of states in a graph rather than to the number of non-looping paths? The answer is yes. We will start looking at uniform-cost search, where the extension is straightforward and then tackle A*, where it is not.\n\nImage: \"States vs Paths\"\n\nSlide 2.6.2\nDynamic Programming Optimality Principle and the Expanded list\n- Given that path length is additive, the shortest path from S to G via a state X is made up of the shortest path from S to X and the shortest path from X to G. This is the \u201cdynamic programming optimality principle\u201d.\n\nDiagram: S \u2192 o \u2192 c \u2192 G\n\nSlide 2.6.3\nWhat will come to our rescue is the so-called \"Dynamic Programming Optimality Principle\", which is fairly intuitive in this context. Namely, the shortest path from the start to the goal that goes through some state X is made up of the shortest path to X followed by the shortest path from X to G. This is easy to prove by contradiction, but we won't do it here.\n\nSlide 2.6.4\nDynamic Programming Optimality Principle and the Expanded list\n- Given that path length is additive, the shortest path from S to G via a state X is made up of the shortest path from S to X and the shortest path from X to G.\n- This is the \u201cdynamic programming optimality principle\u201d.\n- This means that we only need to keep the single best path from S to any state Xi; if we find a new path to a state already in Q, discard the longer one.\n\nSlide 2.6.3\nGiven this, we know that there is no reason to compute any path except the shortest path to any state, since that is the only path that can ever be part of the answer. So, if we ever find a second path to a previously visited state, we can discard the longer one. So, when adding nodes to Q, check whether another node with the same state is already in Q and keep only the one with shorter path length."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 2.6.4\n\n**Dynamic Programming Optimality Principle\nand the Expanded List**\n- Given that path length is additive, the shortest path from S to G via a state X is made up of the shortest path from S to X and the shortest path from X to G.\n  - This is the \u201cdynamic programming optimality principle\u201d.\n- This means that we only need to keep the single best path from S to any state X; if we find a new path to a state already in Q, discard the longer one.\n- Note that the first time UC pulls a search node off of Q whose state is X, this path is the shortest path from S to X. The follows from the fact that UC expands nodes in order of actual path lengths. \n\n---\n\nWe have observed that uniform-cost search pulls nodes off Q (expands them) in order of their actual path length. So, the first time we expand a node whose state is X, that node represents the shortest path to that state. Any subsequent path we find to that state is a waste of effort, since it cannot have a shorter path.\n\n---\n\nSlide 2.6.5\n\nSo, let\u2019s remember the states that we have expanded already, in a \u201clist\u201d (or, better, a hash table) that we will call the Expanded list. If we try to expand a node whose state is already on the Expanded list, we can simply discard that path. We will refer to algorithms that do this, that is, no expanded\nstate is re-visited, as using a strict Expanded list.\n\nNote that when using a strict Expanded list, any visited state will either be in Q or in the Expanded list. So, when we consider a potential new node we can check whether (a) its state is in Q, in which case we accept it or discard it depending on the length of the new path versus the previous best, or (b) it is in Expanded, in which case we always discard it. If the node\u2019s state has never been visited, we add the node to Q.\n\n---\n\n**Dynamic Programming Optimality Principle and the Expanded List**\n\n- Given that path length is additive, the shortest path from S to G via a state X is made up of the shortest path from S to X and the shortest path from X to G.\n  -   This is the \u201cdynamic programming optimality principle\u201d.\n-   This means that we only need to keep the single best path from S to any state X; if we find a new path to a state already in Q, discard the longer one.\n-   Note that the first time UC pulls a search node off of Q whose state is X, this path is the shortest path from S to X. The follows from the fact that UC expands nodes in order of actual path lengths.\n-   So, once we expand a path to state X, we don\u2019t need to consider (extend) any other path to X. We can keep a list of these states, call Expanded. If the state of the search we have pulled off of Q is in the Expanded list, we discard the note. When we use the Expanded list this way, we call it \u201cstrict\u201d.\n\n---\n\nSlide 2.6.6\n\nThe correctness of uniform-cost search does not depend on using an expanded list or even on discarding longer paths to the same state (the Q will just be longer than necessary). We can use UC with or without these optimizations and it is still correct. Exploiting the optimality principle by discarding longer paths to states in Q and not re-visiting expanded states can, however, make UC much more efficient for densely connected graphs."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.6.7\nSo, now, we need to modify our simple algorithm to implement uniform-cost search to take advantage of the Optimality Principle. We start with our familiar algorithm...\n\nSimple Optimal Search Algorithm\nUniform Cost\nA search node is a path from some state X to the start state, e.g., [X, B, A, S]\nThe state of a search node is the non-terminal of the path, e.g., X\nLet S be a list of search nodes, e.g., [[X, B, A, S], [C, B, A, S], ...]\nS\u2019 be the start state\n\n1. Initialize Q with search node (S') as only entry\n2. If Q is empty, fail. Else, pick least cost search node N from Q\n3. If state(N) is a goal, return N (we\u2019ve reached the goal)\n4. (Otherwise) Remove N from Q\n5. Find all the children of state(N) and create all the one-step extensions of N to each descendent\n6. Add all the extended paths to Q\n7. Go to step 2\n\u00a9 \n\nSlide 2.6.8\n... and modify it. First we initialize the Expanded list in step 1. Since this is uniform-cost search, the algorithm picks the best element of Q, based on path length, in step 2. Then, in step 5, we check whether the state of the new node is on the Expanded list and if so, we discard it. Otherwise, we add the state of the new node to the Expanded List. In step 6, we avoid visiting nodes that are Expanded since that would be a waste of time. In step 7, we check whether there is a node in Q corresponding to each newly visited state, if so, we keep only the shorter path to that state.\n\nSimple Optimal Search Algorithm\nUniform Cost + Strict Expanded List\nA search node is a path from some state X to the start state, e.g., [X, B, A, S]\nThe state of a search node is the root node of the path, e.g., X\nLet Q be a list of search nodes, e.g., [[X, B, A, S], [C, B, A, S] ...]\nLet E be the start state\n\n1. Initialize Q with search node (S') as only entry; set Expanded = {}.\n2. If Q is empty, fail. Else, pick least cost search node N from Q\n3. If (state(n) is a goal, return N (we\u2019ve reached the goal)\n4. (Otherwise) Remove N from Q.\n5. If state(n) in Expanded, go to step 2, otherwise add state(n) to Expanded.\n6. Find all the children of state(n) (not in Expanded) and create all the one-step extensions of N to each descend.\n7. Add all the extended paths to Q; if descendant state already in Q, keep only shorter path to the state in Q.\n8. Go to step 2.\n\u00a9\n\nSlide 2.6.9\nLet\u2019s step through the operation of this algorithm on our usual example. We start with a node for S, having a 0-length path, as usual.\n\nUniform Cost (with strict expanded list)\nPick best (by path length) element of Q. Add path extensions anywhere in Q\n\nQ | Expanded\n1 | S S\n  | S A\n2 | S A B\n  | X (1)  \nS (0)\n A  1\n B  2\n\n2\n0\n\nAdded paths in blue; underlined paths are chosen for extension.\nWe show the paths in reversed order, the node\u2019s state is the first entry."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.  \n\n**Slide 2.6.10**  \nUniform Cost (with strict expanded list)  \nPick best (by path length) element of Q. Add path extensions anywhere in Q\n\n|  | Q | Expanded |\n|---|---------------|-----------|\n| 0 | (S)              |             |\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry.\n\nDiagram: \n- S connected to A, B, and C.\n\nWe expand the S node, add its descendants to Q and add the state S to the Expanded list.\n\n**Slide 2.6.11**  \nUniform Cost (with strict expanded list)  \nPick best (by path length) element of Q. Add path extensions anywhere in Q\n\n|  | Q                                             | Expanded |\n|---|-------------------------------------|-----------|\n| 1  | (S)                                          | S         |\n| 2  | (A S) (B S) (C S)                |            |\n| 3  | (A C A S) (B D A S) (E S C S) |  S, A     |\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry.\n\nDiagram:\n- S connected to A, B, and C.\n- A connected to C and D.\n\nWe then pick the node at A to expand since it has the shortest length among the nodes in Q. We get the two extensions of the A node, which gives us paths to C and D. Neither of the two new nodes\u2019 states is already present in Q or in Expanded so we add them both to Q. We also add A to the Expanded list.\n\n**Slide 2.6.12**  \nUniform Cost (with strict expanded list)  \nPick best (by path length) element of Q. Add path extensions anywhere in Q\n\n|  | Q                                             | Expanded |\n|---|-------------------------------------|-----------|\n| 1  | (S)                                          | S          |\n| 2 | (A S) (B S) (C S)                 | S, A      |\n| 3 | (A C A S) (B D A S) (E S C S)  |             |\n\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node\u2019s state is the first entry.\n\nDiagram:\n- S connected to A, B, and C.\n- C indicates completed node list.\n\nWe pick the node at C to expand, but C has no descendants. So, we add C to Expanded but there are no new nodes to add to Q.\n\n**Slide 2.6.13**  \nUniform Cost (with strict expanded list)  \nPick best (by path length) element of Q. Add path extensions anywhere in Q\n\n|  | Q                                                      | Expanded |\n|---|------------------------------------------------|-----------|\n| 1  | (S)                                                   | S          |\n| 2 | (A S) (B S) (C S)                          | S, A     |\n| 3 | (A C A S) (B D A S) (E S C S)  |             |\n\nDiagram:\n- S connected to A, B, and C.\n\nWe select the node with the shortest path in Q, which is the node at B with path length 5 and generate the new descendant nodes, one to D and one to G. Note that at this point we have generated two paths to D - (S A D) and (S B D) both with length 6. We\u2019re free to keep either one but we do not need both. We will choose to discard the new node and keep the one already in Q.\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n### Slide 2.6.14\n**Uniform Cost (with strict expanded list)**\nPick best (by path length) element of Q. Add path extensions anywhere in Q.\n```\nExpanded   G  Path\n    {}          0   (S)    \n   S           2   (SA)  \n (A)         3   (SB)   \n (S A)     2   (SAC)   \n (S B)     5   (S A D)   \n (S A C) 5   (S A G)  \n (S A D)  4   (S C A) \n (S C)     6   (S G)  \n```\n- The node corresponding to the (S A D) path is now the shortest path, so we expand it and generate two descendants, one going to C and one going to G. The new C node can be discarded since C is on the Expanded list.\n- The new G node shares its state with a node already on Q, but it corresponds to a shorter path - so we discard the older node in favor of the new one. So, at this point, Q only has one remaining node.\n  \n### Slide 2.6.15\nThis node corresponds to the optimal path that is returned. It is easy to show that the use of an Expanded list, as well as keeping only the shortest path to any state in Q, preserve the optimality guarantee of uniform-cost search and can lead to substantial performance improvements. Will this hold true for A* as well?\n\n### Uniform Cost (with strict expanded list)\nAdded paths in blue; underlined paths are chosen for extension. We show the paths in reversed order; the node's state is the first entry.\n```\nExpanded     G  Path\n    {S A D}       4  (S C A)   S \n    {S A D}       5  (S A C B)   \n    {S A D}       6  (S A C e)  \n    {S A D}       6  (S A C)    \n    {S A D G} 5  (S B G)  \n    {S A D G} 6  (SB)\n    {S A D G} 7  (SB)\n    {S A D G} 7  (SB A)\n```\n\n### Slide 2.6.16\n**A* (without expanded list)**\n- Let g(N) be the path cost of n, where n is a search tree node, i.e., a partial path.\n- Let h(N) be h(STATE(n)), the heuristic estimate of the remaining path length to the goal from STATE(n).\n- Let f(n) = g(n) + h(state(n)) be the total estimated path cost of a node, i.e., the estimate of a path to a goal that starts with the path given by N.\n- A* picks the node with lowest f value to expand.\n\n### Slide 2.6.17\nA*, without using an Expanded list or discarding nodes in Q but using an admissible heuristic -- that is, one that underestimates the distance to the goal -- is guaranteed to find optimal paths.\n\n-- END --"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_006.jpeg",
    "text": "**6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.**\n\n**Slide 2.6.18**  \nIf we use the search algorithm we used for uniform-cost search with a strict Expanded list for A*,\nadding in an admissible heuristic to the path length, then we can no longer guarantee that it will \nalways find the optimal path. We need a stronger condition on the heuristics used than being an \nunderestimate.\n\n**A* and the strict Expanded List**\n- The strict Expanded list (also known as a Closed list) is commonly used in \n  implementations of A*, but to guarantee finding optimal paths, this \n  implementation requires a stronger condition for a heuristic than simply \n  being an underestimate.\n\n**Slide 2.6.19**\nHere's an example that illustrates this point. The exceedingly optimistic heuristic estimate at B\n\"lures\" the A* algorithm down the wrong path.\n\n*(Figure with nodes and heuristic values)*\n\n**A and the strict Expanded List**\n- The strict Expanded list (also known as Closed list) is commonly used in \n  implementations of A*, but to guarantee finding optimal paths, this \n  implementation requires a stronger condition for a heuristic than simply \n  being an underestimate. \n- Here\u2019s a counterexample: The heuristic values listed below are all \n  underestimates but using an Expanded list will not find the optimal path. \n  The misleading estimate at B throws the algorithm off, C is expanded before \n  the optimal path to it is found.\n\n*(Path example with A->B->C->G)*\n\n**Slide 2.6.20**\nYou can see the operation of A* in detail here, confirming that it finds the incorrect path. The\ncorrect partial path via A is blocked when the path to C via B is expanded. In step 4, when A is \nfinally expanded, the new path to C is not put on Q, because C has already been expanded.\n\n_(Table with path sequences, heuristic values, and costs)_\n\n- Expanded:\n  (Start) A = {(10, C A D G B)}\n  A = C B (14, G A D E)\n  C (14, A G D E B)\n  B C (14, C G E A D)\n  (10, G C B)\n\n_(Table Notes)_\n- Added paths in blue; underlined paths are chosen for expansion.\n- New paths shown in reversed order; the node's state is given the first entry.\n\n**Slide 2.6.21**\n\nThe stronger conditions on a heuristic that enables us to implement A* just the same way we \nimplemented uniform-cost search with a strict Expanded list are known as the consistency \nconditions. They are also called monotonicity conditions by others. The first condition is simple, \nnamely that goal states have a heuristic estimate of zero, which we have already been assuming. The \nnext condition is the critical one. It indicates that the difference in the heuristic estimate between \none state and its descendant must be less than or equal to the actual path cost on the edge connecting \nthem.\n\n - Consistency:\n   - To enable implementing A* using the strict Expanded list, it needs to satisfy \n     the following consistency (also known as monotonicity) conditions.\n     - h(n\u2093) = 0, if n\u2093 is a goal \n     - h(n\u1d62) \u2264 h(n\u2c7c) + c(n\u1d62,n\u2c7c), for n\u2c7c a child of n\u1d62\n\n**Consistency:**"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_007.jpeg",
    "text": "Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.6.22\nConsistency\n- To enable implementing A* using the strict Expanded list, H needs to satisfy the following consistency (also known as monotonicity) conditions.\n   \u2022 h(a) = 0, if n(a) is a goal\n   \u2022 h(n) <= h(s) + c(s,a), for a child n of s.\nThat is, the heuristic cost in moving from one entry to the next cannot decrease by more than the arc cost between the states. This is a kind of triange inequality. This condition is a highly desirable property of a heuristic function and often simply assumed (more on this later).\n\nThe best way of visualizing the consistency condition is as a \u201ctriangle inequality,\" that is, one side of the triangle is less than or equal the sum of the other two sides, as seen on the diagram here.\n\nSlide 2.6.23\nHere is a simple example of a (gross) violation of consistency. If you believe goal is 100 units from n, then moving 10 units to nj should not bring you to a distance of 10 units from the goal. These heuristic estimates are not consistent.\n\nConsistent Violation\nExample:\nh(s)=100, h(a)=10\n     100\n     /     \\     \n    n - h(a)=10 goal\n A simple example of a violation of consistency\u2014\n   \u2022 h(n) > h(s) + c(s, a)\n   \u2022 In example, (100 > 10 + 0)\n   \u2022 If you believe goal is 100 units from n, then moving 10 units to nj should not bring you a distance of 10 units from the goal.\n\nSlide 2.6.24\nA* (without expanded list)\n   \u2022 Let g(n) be the path cost of n, where n is a search tree node, i.e., a partial path.\n   \u2022 Let h(n) be the heuristic estimate of the remaining path's length to the goal from state(n).\n   \u2022 Let f(n) = g(n) + h(n) be the total estimated path cost of a node, i.e., the estimation of a path to a goal that starts with the path given by n.\n   \u2022 A* picks the node with the lowest value to expand.\nA* (without expanded list) and with admissible heuristic is guaranteed to find optimal paths\u2014those with the smallest path cost.\nThis is true even if heuristic is NOT consistent.\n\nI want to stress that consistency of the heuristic is only necessary for optimality when we want to discard paths from consideration, for example, because a state has already been expanded. Otherwise, plain A* without using an expanded only requires only that the heuristic be admissible to guarantee optimality.\n\nSlide 2.6.25\nThis illustrates that A* without an Expanded list has no trouble coping with the example we saw earlier that showed the pitfalls of using a strict Expanded list. This heuristic is not consistent but it is an underestimate and that is all that is needed for A* without an Expanded list to guarantee optimality.\n\nA* (without expanded list)\nNote that heuristic is admissible but not consistent:\nOpened paths:\n      (1 Q S)\n      (3 E Q S)\n      (4 C B (1 0 1 A) S)\n      (4 C B S (1 0 1 A) S)\n      (4 C B S A)\n      (6 G C B S (4 C B S))\nAd:\n      A=100  C-B.\n      E=90  G=80\nAdded paths in blue, underlined paths are chosen for extension."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_008.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nA* (with strict expanded list)\n\u2022 Just like Uniform Cost search.\n\u2022 When a node N is expanded, if state(Q) is in expanded list, discard N, else add state(Q) to expanded list.\n\u2022 If some node in N has the same state as some descendant of N, keep only node with smaller f, which will correspond to smaller g.\n\u2022 For A* with strict expanded list to be guaranteed to find the optimal path, the heuristic must be consistent.\n\nSlide 2.6.26\n\nThe extension of A* to use a strict expanded list is just like the extension to uniform-cost search. In fact, it is the identical algorithm except that it uses f values instead of g values. But, we stress that for this algorithm to guarantee finding optimal paths, the heuristic must be consistent.\n\nSlide 2.6.27\n\nIf we modify the heuristic in the example we have been considering so that it is consistent, as we have done here by increasing the value of h(B), then A* (even when using a strict Expanded list) will work.\n\nSlide 2.6.28\n\nDealing with inconsistent heuristic\n\u2022 What can we do if we have an inconsistent heuristic but we still want optimal paths?\n\nPeople sometimes simply assume that the consistency condition holds and implement A* with a strict Expanded list (also called a Closed list) in the simple way we have shown before. But, this is not the only (or best) option. Later we will see that A* can be adapted to retain optimality in spite of a heuristic that is not consistent - there will be a performance price to be paid however.\n\nSlide 2.6.29\n\nThe key step needed to enable A* to cope with inconsistent heuristics is to detect when an overly optimistic heuristic estimate has caused us to expand a node prematurely, that is, before the shortest path to that node has been found. This is basically analogous to what we have been doing when we find a shorter path to a state already in Q, except we need to do it to states in the Expanded list. In this modified algorithm, the use of the Expanded list is not strict: we allow re-visiting states on the Expanded list.\n\nTo implement this, we will keep in the Expanded list not just the expanded states but the actual node that was expanded. In particular, this records the actual path length at the time of expansion.\n\nA* (with strict expanded list)\n\nNote that this heuristic is admissible and consistent\n\n| Q             | Expanded |\n|--------------|-----------|\n| (90) S       | \u2205         |\n| (95.1) (101.4) S,A  | S        |\n| (102.2) C,A (104) C,S  | C,A,S   |\n| (200) G,C         | G,C,A,S |\n\nAdded paths in blue; underlined paths are those chosen for expansion.\n\n(A=100 and  C=100 and  S=90 and  G=0)\n\n---\n\nDealing with inconsistent heuristic\n\n\u2022 What can we do if we have an inconsistent heuristic but we still want optimal paths?\n\u2022 Modify A* so that it detects and corrects when inconsistency has led us astray."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.6.30\n- \\[Dealing with inconsistent heuristic\\]\n  - What can we do if we have an inconsistent heuristic but we still want optimal paths?\n  - Modify A* so that it detects and corrects when inconsistency has led us astray.\n     - Assume we are adding node1 to Q and node2 is present in Expanded list with node, state = node2, state.\n\nLet's consider in detail the operation of the Expanded list if we want to handle inconsistent heuristics while guaranteeing optimal paths.\n\nAssume that we are adding a node, call it node1, to Q when using an Expanded list. So, we check to see if a node with the same state is present in the Expanded list and we find node2 which matches.\n\nSlide 2.6.31\nWith a strict Expanded list, we simply discard node1; we do not add it to Q.\n\n- \\[Dealing with inconsistent heuristic\\]\n  - Strict:\n    - Do not add node1 to Q.\n\nSlide 2.6.32\nWith a non-strict Expanded list, the situation is a bit more complicated. We want to make sure that node2 has not found a better path to the state than node1. If a better path has been found, we remove the old node from Expanded (since it does not represent the optimal path) and add the new node to Q.\n\n- \\[Dealing with inconsistent heuristic\\]\n  - Non-Strict Expanded list:\n    - If node1, path_length < node2, path_length, then\n      - Delete node2 from Expanded list\n      - Add node1 to Q\n\nSlide 2.6.33\nLet's think a bit about the worst case complexity of A*, in terms of the number of nodes expanded (or visited).\n\nAs we've mentioned before, it is customary in AI to think of search complexity in terms of some \"depth\" parameter of the domain such as the number of steps in a plan of action or the number of moves in a game. The state space for such domains (planning or game playing) grows exponentially in the \"depth;\" that is, because at each depth level there is some branching factor (e.g., the possible actions) and so the number of states grows exponentially with the depth.\n\nWe could equally well speak instead of the number of states as a fixed parameter, call it N, and state our complexity in terms of N. We just have to keep in mind then that in many applications, N grows exponentially with respect to the depth parameter.\n\n- \\[Worst Case Complexity\\]\n  - The number of states in the search space may be exponential in some \"depth\" parameter, e.g. number of actions in a plan, number of moves in a game."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 2.6.34**\n\nWorst Case Complexity\n\n- The number of states in the search space may be exponential in some \"depth\" parameter, e.g., number of actions in a plan, number of moves in a game.\n- All the searches, with or without visited or expanded lists, may have to visit (or expand) each state in the worst case.\n- So, all searches will have worst case complexities that are at least proportional to the number of states and therefore exponential in the \"depth\" parameter.\n- This is the bottom-line irreducible worst case cost of systematic searches.\n\nIn the worst case, when the heuristics are not very useful or the nodes are arranged in the worst possible way, all the search methods may end up having to visit or expand all of the states (up to some depth). In practice, we should be able to avoid this worst case but in many cases one comes pretty close.\n\n---\n\n**Slide 2.6.35**\n\nThe problem is that if we have no memory of what states we've visited or expanded, then the worst case for a densely connected graph can be much, much worse than this. One may end up doing exponentially more work.\n\n- The number of states in the search space may be exponential in some \"depth\" parameter, e.g., number of actions in a plan, number of moves in a game.\n- All the searches, with or without visited or expanded lists, may have to visit (or expand) each state in the worst case.\n- So, all searches will have worst case complexities that are at least proportional to the number of states and therefore exponential in the \"depth\" parameter.\n- This is the bottom-line irreducible worst case cost of systematic searches.\n- Without memory of what states have been visited (expanded), searches can do (much) worse than visit every state.\n\n---\n\n**Slide 2.6.36**\n\nWorst Case Complexity\n\n- A state space with N states may give rise to a search tree that has a number of nodes that is exponential in N, as in this example.\n\n-- Graph illustration --\n\nWe've seen this example before. It shows that a state space with N states can generate a search tree with 2^N nodes.\n\n---\n\n**Slide 2.6.37**\n\nA search algorithm that does not keep a visited or expanded list will do exponentially more work that necessary. On the other hand, if we use a strict expanded list, we will never expand more than the (unavoidable) N states.\n\nWorst Case Complexity\n\n- A state space with N states may give rise to a search tree that has a number of nodes that is exponential in N, as in this example.\n\n-- Graph illustration --\n\n- Searches without a visited (expanded) list may, in the worst case, visit (expand) every node in the search tree.\n- Searches with strict visited (expanded lists) will visit (expand) each state only once."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Optimality & Worst Case Complexity**\n\n| Algorithm | Heuristic   | Expanded List | Optimality Guaranteed? | Worst Case \u03b8 Expansions |\n|-----------|-------------|---------------|------------------------|-------------------------|\n| Uniform Cost | None      | Strict        | Yes                    | N                       |\n| A*        | Admissible  | None          | Yes                    | >N                      |\n| A*        | Consistent  | Strict        | Yes                    | N                       |\n| A*        | Admissible  | Strict        | No                     | N                       |\n| A*        | Admissible  | Non Strict    | Yes                    | >N                      |\n\nN is number of states in graph\n\nHere we summarize the optimality and complexity of the various algorithms we have been examining.\n\n---\n\n6.034 Notes: Section 2.7\n\n**Slide 2.7.1**\n\nThis set of slides goes into more detail on some of the topics we have covered in this chapter.\n\n**Optional Topics**\n\n- These slides go into more depth on a variety of topics we have touched upon:\n  - Optimality of A*\n  - Impact of a better heuristic on A*\n  - Why does consistency guarantee optimal paths for A* with strict expanded list\n  - Algorithmic issues for A*\n- These are not required and are provided for those interested in pursuing these topics.\n\n**Slide 2.7.2**\n\nFirst topic:\n\nLet's go through a quick proof that A* actually finds the optimal path. Start by assuming that A* has selected a node G.\n\n**Optimality of A***\n\n- Assume A* has expanded a path to goal node G"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_012.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.7.3\nThen, we know from the operation of A* that it has expanded all nodes N whose cost f(N) is strictly less than the cost of G. We also know that since the heuristic is admissible, its value at a goal node must be 0 and thus, f(G) = g(G)+h(G) = g(G). Therefore, every unexpanded node N must have f(N) greater or equal to the actual path length to G.\n\nSlide 2.7.4\nSince h is admissible, we know that any path through an unexpanded node N that reaches some alternate goal node G' must have a total cost estimate f(N) that is not larger than the actual cost to G', that is g(G').\n\nOptimality of A*\n\u2022 Assume A* has expanded a path to goal node G\n\u2022 Then, A* has expanded all nodes N where f(N) < f(G). Since h is admissible, f(G) = g(G). So, every unexpanded node has f(N) \u2265 g(G).\n\nOptimality of A*\n\u2022 Assume A* has expanded a path to goal node G\n\u2022 Then, A* has expanded all nodes N where f(N) < f(G). Since h is admissible, f(G) = g(G). So, every unexpanded node has f(N) \u2265 g(G).\n\u2022 Since h is admissible, we know that any path through H that reaches a goal node G' has value g(G') \u2265 f(N)\n\nSlide 2.7.5\nCombining these two statements we see that the path length to any other goal node G' must be greater or equal to the path length of the goal node A* found, that is, G.\n\nSlide 2.7.5\nCombining these two statements we see that the path length to any other goal node G' must be greater or equal to the path length of the goal node A* found, that is, G.\n\nSlide 2.7.6\nNext topic:\n\nWe can also show that a better heuristic in general leads to improved performance of A* (or at least no decrease). By performance, we mean number of nodes expanded. In general, there is a tradeoff in how much effort we do to compute a better heuristic and the improvement in the search time due to reduced number of expansions.\n\nLet's postulate a \"perfect\" heuristic which computes the actual optimal path length to a goal. Call this heuristic h*.\n\nImpact of better heuristic\n\u2022 Let h* be the \"perfect\" heuristic - returns actual path cost to goal."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.7.7  \nThen, assume we have a heuristic h1 that is always numerically less than another heuristic h2, which is (by admissibility) less than or equal to h*.\n\nImpact of better heuristic\n- Let h* be the \u201cperfect\u201d heuristic \u2013 returns actual path cost to goal.\n- If h1(N) \u2264 h2(N) \u2264 h*(N) for all non-goal nodes, then h2 is a better heuristic than h1.\n\nSlide 2.7.8\nThe key observation is that if we have two versions of A*, one using h1 and the other using h2, then every node expanded by the second one is also expanded by the first.\n\nThis follows from the observation we havemade earlier that at a goal, the heuristic estimates all \nagree (they are all 0) and so we know that both versions will expand all nodes whose value of f is less than the actual path length of G*.\n\nNow, every node expanded by A*2 will have a path cost no greater than the actual cost to the goal G. Such a node will have a smaller cost using h1 and so it will definitely be expanded by A*1 as well.\n\n \nSlide 2.7.9 \nSo, A*1 expands at least as many nodes as A*2. We say that A*2 is better informed than A*1 to refer to this situation.\n\n\nSlide 2.7.10\nSince uniform-cost search is simply A* with a heuristic of 0, we can say that A* is generally better informed than UC and we expect it to expand fewer nodes. But, A* will expand additional effort computing the heuristic value \u2013 a good heuristic can more than pay back that extra effort.\n\nImpact of better heuristic\n- Let h* be the \u201cperfect\u201d heuristic \u2013 returns actual path cost to goal.\n- If h1(N) \u2264 h2(N) \u2264 h*(N) for all non-goal nodes, then h2 is a better heuristic than h1.\n- If A*1 uses h1 and A*2 uses h2, then every node expanded by A*2 is also expanded by A*1  \n  - f1(N) \u2264 f2(N)=g(N)+h2(N) \u2264g(G)\n\u21d2  f1(N)=g(N)+h1(N)\u2264f2(N)=g(N)+h2(N)\u2264g(G)\n- That is, A*1 expands at least as many nodes as A*2 and we say that A*2 is better informed than A*1.\n - Note that: A* with any non-zero admissible heuristic is better informed \n(and therefore it typically expands fewer nodes) than Uniform Cost \nsearch."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 2.7.11\n\nNew topic:\n\nWhy does consistency allow us to guarantee that A* will find optimal paths? The key insight is that consistency ensures that f values of expanded nodes will be non-decreasing over time.\n\nConsider two nodes N\u1d62 and N\u2c7c such that the latter is a descendant of the former in the search tree. Then, we can write out the values of f as shown here, involving the actual path length g(N)\u2096, the cost of the edge between the nodes c(N\u1d62, N\u2c7c) and the heuristic values of the two corresponding states.\n\n---\n\n[Diagram: Consistency \u2192 Non-decreasing f]\n\nN\u1d62                      N\u2c7c\n\nN\u2c7c is a descendant of N\u1d62 in the search tree\n\nf(N\u1d62) = g(N\u1d62)+h(N\u1d62)=g(N\u2c7c)+c(N\u1d62,N\u2c7c)+h(N\u1d62)\n\nf(N\u2c7c) = g(N\u2c7c)+h(N\u2c7c)\n\n---\n\nSlide 2.7.12\n\nBy consistency of the heuristic estimates, we know that the heuristic estimate cannot decrease more than the edge cost. So, the value of f in the descendant node cannot go down; it must stay the same or go up.\n\nBy this reasoning we can conclude that whenever A* expands a node, the new nodes' f values must be greater or equal to that of the expanded node. Also, since the expanded node must have had an f value that was a minimum of the f values in Q, this means that no nodes in Q after this expansion can have a lower f value than the most recently expanded node. That is, if we track the series of f values of expanded nodes over time, this series is non-decreasing.\n\n---\n\nSlide 2.7.13\n\nNow we can show that if we have nodes expanded in non-decreasing order of f, then the first time we expand a node whose state is s, then we have found the optimal path to the state. If you recall, this was the condition that enabled us to use the strict Expanded list, that is, we never need to re-visit (or re-expand) a state.\n\n---\n\nSlide 2.7.14\n\nNon-decreasing f \u2192 first path is optimal\n\nA* with consistent heuristic expands nodes N in non-decreasing order of f(N)  \nThen, when a node N is expanded, we have found the shortest path to the corresponding state\n\nImagine that we later found another node N' with the same corresponding state s\nthen we know that\n  * g(N') \u2265 g(N)\n  * f(N') = g(N') + h(s)\n  * f(N') \u2265 g(N') + h(s)\n\n---\n\nSlide 2.7.14\n\nTo prove this, let\u2019s assume that we later found another node N' that corresponds to the same state as a previously expanded node N. We have shown that the f value of N' is greater or equal that of N. But, since the heuristic values of these nodes must be the same \u2013 since they correspond to the same underlying graph state \u2013 the difference in f values must be accounted by a difference in actual path length.\n\n---\n\n[End of text from slides]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 2.7.15\nSo, we can conclude that the second path cannot be shorter than the first path we already found, and so we can ignore the new path!\n\nNon-decreasing f \u2192 first path is optimal\n- A* with consistent heuristics expands nodes N in a non-decreasing order of f(N)\n  - Then, when a node N is expanded, we have found the shortest path to the corresponding state gstat(N)\n- Imagine that we later found another node N' with the same corresponding state s\n  - We then know that\n    f(N') \u2265 f(N)\n    f(N') = g(N') + h(N')\n    f(N) = g(N) + h(N) \u2264 f(N')\n- So, we can conclude that\ng(N') \u2265 g(N)\nAnd we can safely ignore the second path to s as we would with the strict Expanded list.\n\nSlide 2.7.16\nFinal topic:\nLet's analyze the behavior of uniform-cost search with a strict Expanded List. This algorithm is very similar to the well known Dijkstra's algorithm for shortest paths in a graph, but we will keep the name we have been using. This analysis will apply to A* with a strict Expanded list, since in the worst case they are the same algorithm.\n\nTo simplify our approach to the analysis, we can think of the algorithm as boiled down to three steps.\n1. Pulling paths off of Q.\n2. Checking whether we are done and\n3. Adding the relevant path extensions to Q.\n\nIn what follows, we assume that the Expanded list is not a \"real\" list but some constant-time way of checking that a state has been expanded (e.g., by looking at a mark on the state or via a hash-table).\n\nWe also assume that Q is implemented as a hash table, which has constant time access (and insertion) cost. This is so we can find whether a node with a given state is already on Q.\n\nSlide 2.7.17\nLater, it will become important to distinguish the case of \"sparse\" graphs, where the states have a nearly constant number of neighbors and \"dense\" graphs where the number of neighbors grows with the number of states. In the dense case, the total number of edges is O(N^2), which is substantial.\n\nUniform Cost + Strict Expanded List\n(order of time growth in worst case)\n\nOur simple algorithm can be summarized as follows:\n1. Take the best search node from Q\n2. Are we there yet?\n3. Add path extensions to Q\nAssume strict Expanded \u201clist\u201d is implemented as a hash table, which gives constant time access. Q also implemented as a hash table.\n\nUniform Cost + Strict Expanded List\n\nOur simple algorithm can be summarized as follows:\n1. Take the best search node from Q\n2. Are we there yet?\n3. Add path extensions to Q\nAssume strict Expanded \u201clist\u201d is implemented as a hash table, which gives constant time access. Q also implemented as a hash table.\n\nAssume we have a graph with N nodes and L links. Graphs where nodes have O(N) links are dense. Graphs where the nodes have a nearly constant number of links are sparse. For dense graphs, L is O(N^2)."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_016.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 2.7.18**\n\nSo, let's ask the question, how many nodes are taken from Q (expanded) over the life of the algorithm (in the worst case)? Here we assume that when we add a node to Q, we check whether a node already exists for that state and keep only the node with the shorter path. Given this and the use of a strict Expanded list, we know that the worst-case number of expansions is N, the total number of states.\n\n---\n\n**Slide 2.7.19**\n\nWhat's the cost of expanding a node? Assume we scan Q to pick the best paths. Then the cost is of the order of the number of paths in Q, which is O(N) also, since we only keep the best path to a state.\n\n---\n\n**Slide 2.7.20**\n\nHow many times do we (attempt to) add paths to Q? Well, since we expand every state at most once and since we only add paths to direct neighbors (links) of that state, then the total number is bounded by the total number of links in the graph.\n\n---\n\n**Slide 2.7.21**\n\nAdding to the Q, assuming it is a hash table, as we have been assuming here, can be done in constant time.\n\n---\n\n**Uniform Cost + Strict Expanded List**\n(order of the growth in worst case)\n\nOur simple algorithm can be summarized as follows:\n1. Take the best search node from Q\n2. Are we there yet?\n3. Add path extensions to Q\n\nAssume strict Expanded \"list\" is implemented as a hash table, which gives constant time access. Q also implemented as a hash table.\n\nAssume we have a graph with N nodes and L links. Graphs where nodes have O(N) links are dense. Graphs where the nodes have a nearly constant number of links are sparse. For dense graphs, L is O(N^2).\n\nNodes taken from Q  - O(N)\n\nCost of picking a node from Q using linear scan?  - O(N)\n\nAttempts to add nodes to Q (many are rejected)?  - O(L)\n\nCost of adding a node to Q:  - O(1)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_017.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 2.7.22\n\nPutting it all together gives us a total cost on the order of O(N^2 + L) which, since L is at worst O(N^2) is essentially O(N^2).\n\n---\n\nSlide 2.7.23\n\nIf you know about priority queues, you might think that they are natural as implementation of Q, since one can efficiently find the best element in such a queue.\n\n---\n\nSlide 2.7.24\n\nNote, however, that adding elements to such a Q is more expensive than adding elements to a list or a hash table. So, whether it's worth it depends on how many additions are done. As we said, this is order of L, the number of links.\n\n---\n\nSlide 2.7.25\n\nFor a dense graph, where L is O(N^2), then the priority queue will not be worth it. But, for a sparse graph it will.\n\n---\n\nA*  \n(order of growth in worst case)\n\nOur simple algorithm can be summarized as follows:\n1. Take the best search node from frontier\n2. Are we there yet?\n3. Add path extensions to Q\n\nAssume eight Expanded *List* is implemented as a hash table, which gives\nconstant time access. O also implemented as a hash table.\n\nAssume we have a graph with nodes and L links. Graphs where nodes have O(N)\nlinks are dense. Graphs where the nodes have nearly constant number of links\nare sparse. For sparse graphs, L is O(N).\n\n Nodes taken from Q? O(N)\n Cost of picking a node from Q using linear scan? O(N)\n Additions made to nodes? (many are rejected)? O(L)\n Cost of adding a node to Q? O(1)\n\n Total cost? O(N^2 + L)\n\n---\n\nShould we use a Priority Queue?\n\n* A priority queue is a data structure that makes it efficient to identify the\n\u201cbest\u201d element of a set. A PQ is typically implemented as a balanced tree.\n\n* The time to find the best element in a PQ grows as O(log N) for a set of size N.\n  This is very much better than N for a regular list. Also, note that even if we don't\ndiscard paths to Expanded nodes, the access is still O(log N), since O(log\nN)+ O(log N).\n\n* However, adding elements to a PQ also has time that grows as O(log N).\n\n* Our algorithm does up to N \u201cfind best\u201d operations and it does up to L\n  \u201cadd\u201d operations. If Q is a PQ, then cost is O(N log N + L log N)\n\n* If graph is dense, and L is O(N^2), then a PQ is not advisable.\n\n* If graph is sparse (the more common case), and L is O(N), then a PQ is\n  highly desirable.\n\n---"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch2_search3\\page_018.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.  \n\nSlide 2.7.26  \n\n**Cost and Performance**  \nSearching a tree with N nodes and L links\n\n**Search Method** | **Worst Time** (Dense) | **Worst Time** (Sparse) | **Worst Space** | **Guaranteed to find shortest path**\n\nUniform Cost | O(Ng) | O(N log N) | O(N) | Yes\n\nA* | O(Ng) | O(N log N) | O(N) | Yes\n\nSearching a tree with branching factor b and depth d  \nL = N = b^d  \n\nWorst case time is proportional to number of nodes created  \nWorst case space is proportional to maximal length of Q (and Expanded)\n\n---\n\nHere we summarize the worst-case performance of UC (and A*, which is the same). Note, however, that we expect A* with a good heuristic to outperform UC in practice since it will expand at most as many nodes as UC. The worst case cost (with an uninformative heuristic) remains the same.\n\nBy the way, in talking about space we have focused on the number of entries in Q but have not mentioned the length of the paths. One might think that this would actually be the dominant factor. But, recall that we are unrolling the graph into the search tree and each node only needs to have a link to its unique ancestor in the tree and so a node really requires constant space.\n\nAs before, you can think of the performance of these algorithms as a low-order polynomial (N^2) or as an intractable exponential, depending on how one describes the search space."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 3.1\n\n---\n\nSlide 3.1.1\n\nIn this presentation, we'll take a look at the class of problems called Constraint Satisfaction Problems (CSPs). CSPs arise in many application areas: they can be used to formulate scheduling tasks, robot planning tasks, puzzles, molecular structures, sensory interpretation tasks, etc.\n\nIn particular, we'll look at the subclass of Binary CSPs. A binary CSP is described in term of a set of Variables (denoted V_i), a domain of Values for each of the variables (denoted D_i) and a set of constraints involving the combinations of values for two of the variables (hence the name \"binary\"). We'll also allow \"unary\" constraints (constraints on a single variable), but these can be seen simply as cutting down the domain of that variable.\n\nWe can illustrate the structure of a CSP in a diagram, such as this one, that we call a **constraint graph** for the problem.\n\n---\n\n[Diagram with circles and lines labeled with general class of problems and a constraint graph]\n\n*Constraint Satisfaction Problems*\n\nGeneral class of Problems: \u2022 Binary CSP\n\nUnary constraint \nBinary constraint arc\nVariables V_i with values in domain D_i\nThis diagram is called a constraint graph\n\n---\n\nSlide 3.1.2\n\nThe solution of a CSP involves finding a value for each variable (drawn from its domain) such that all the constraints are satisfied. Before we look at how this can be done, let's look at some examples of CSP.\n\n---\n\n[Diagram with similar constraints and domains]\n\nConstraint Satisfaction Problems\n\nGeneral class of Problems: Binary CSP\nUnary constraint, cut down domain\nBinary constraint arc\nVariables V_i with values in domain D_i\n\nThis diagram is called a constraint graph\n\nBasic problem:\nFind a (xi) \u03b5 Di for each Vi s.t. all constraints satisfied (finding consistent labeling for variables)\n\n---\n\nSlide 3.1.3\n\nA CSP that has served as a sort of benchmark problem for the field is the so-called N-Queens problem, which is that of placing N queens on an NxN chessboard so that no two queens can attack each other.\n\nOne possible formulation is that the variables are the chessboard positions and the values are either Queen or Blank. The constraints hold between any two variables representing positions that are on a line. The constraint is satisfied whenever the two values are not both Queen.\n\nThis formulation is actually very wasteful, since it has N^2 variables. A better formulation is to have variables correspond to the columns of the board and values to the index of the row where the Queen for that column is to be placed. Note that no two queens can share a column and that every column must have a Queen on it. This choice requires only N variables and also fewer constraints to be checked.\n\nIn general, we'll find that there are important choices in the formulation of a CSP.\n\n---\n\n[Diagram of a chessboard with constraints]\n\nPlace N queens on an NxN chessboard so that none can attack the other.\n\nVariables are board positions in NxN chessboard\nDomains Queen or blank\nConstraints Two positions on a line (vertical, horizontal, diagonal) cannot both be Q"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.1.4\n\nLine labeling as CSP\n\nThe problem of labeling the lines in a line-drawing of blocks as being either convex, concave or boundary, is the problem that originally brought the whole area of CSPs into prominence. Waltz\u2019s approach to solving this problem by propagation of constraints (which we will discuss later) motivated much of the later work in this area.\n\nIn this problem, the variables are the junctions (that is, the vertices) and the values are a combination of labels (+, -, \u2192) attached to the lines that make up the junction. Some combinations of these labels are physically realizable and others are not. The basic constraint is that junctions that share a line must agree on the label for that line.\n\nNote that the more natural formulation that uses lines as the variables is not a BINARY CSP, since all the lines coming into a junction must be simultaneously constrained.\n\nVariables - are line junctions\nDomains - are set of legal labels for that junction type\nConstraints - shared lines between adjacent junctions must have same label.\n\nSlide 3.1.5\n\nScheduling as CSP\n\nScheduling actions that share resources is also a classic case of a CSP. The variables are the activities, the values are chunks of time and the constraints enforce exclusion on shared resources as well as proper ordering of the tasks.\n\nChoose time for activities e.g.,\nobservations on Hubble telescope, or times to take\nrequired classes.\n\nVariables - are activities\nDomains - sets of start times (or \"chunks\" of time)\nConstraints - Do not use the same resource concurrently\n Precedences satisfied\n\nSlide 3.1.6\n\nGraph Coloring as CSP\n\nPick colors for map regions, avoiding coloring adjacent regions with the same color\n\nVariables - regions\nDomains - colors allowed\nConstraints - adjacent regions must have different colors\n\nAnother classic CSP is that of coloring a graph given a small set of colors. Given a set of regions with defined neighbors, the problem is to assign a color to each region so that no two neighbors have the same color (so that you can tell where the boundary is). You might have heard of the famous Four Color Theorem that shows that four colors are sufficient for any planar map. This theorem was a conjecture for more than a century and was not proven until 1976. The CSP is not proving the general theorem, just constructing a solution to a particular instance of the problem.\n\nSlide 3.1.7\n\nA very important class of CSPs is the class of boolean satisfiability problems. One is given a formula over boolean variables in conjunctive normal form (a set of ORs connected with ANDs). The objective is to find an assignment that makes the formula true, that is, a satisfying assignment.\n\nSAT problems are easily transformed into the CSP framework. And, it turns out that many important problems (such as constructing a plan for a robot and many circuit design problems) can be turned into (huge) SAT problems. So, a way of solving SAT problems efficiently in practice would have great practical impact.\n\nHowever, SAT is the problem that was originally used to show that some problems are NP-complete, that is, as hard as any problem whose solution can be checked in polynomial time. It is generally believed that there is no polynomial time algorithm for NP-complete problems. That is, that any guaranteed algorithm has a worst-case running time that grows exponentially with the size of the problem. So, at best, we can only hope to find a heuristic approach to SAT problems. More on this later.\n\n3-SAT as CSP\nThe original NP-complete problem\n\n(A or B or C) and (A or C or B) \u2026\n\nVariables - clauses\nDomains - boolean variable assignments that make clause true\nConstraints - clauses with shared boolean variables must agree on value of variable\n\nFind values for boolean variables A,B,C,\u2026 that satisfy the formula."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nModel-based recognition as CSP\n\nFind given model in edge image, with rotation and translation allowed.\n\nModel\nImage\n\nVariables: Edges in model\nDomains: Set of edges in image\nConstraints: Angle between model & image edges must match\n\n---\n\nSlide 3.1.8\n\nModel-based recognition is the problem of finding an instance of a known geometric model, described, for example, as a line-boundary in an image which has been pre-processed to identify and fit lines to the boundaries. The position and orientation of the instance, if any, is not known.\n\nThere are a number of constraints that need to be satisfied by edges in the image that correspond to edges in the model. Notably, the angles between pairs of edges must be preserved.\n\n---\n\nSlide 3.1.9\n\nSo, looking through these examples of CSPs we have some good news and bad news. The good news is that CSP is a very general class of problems containing many interesting practical problems. The bad news is that CSPs include many problems that are intractable in the worst case. So, we should not be surprised to find that we do not have efficient guaranteed solutions for CSP. At best, we can hope that our methods perform acceptably in the class of problems we are interested in. This will depend on the structure of the domain of applicability and will not follow directly from the algorithms.\n\nGood News / Bad News\n\nGood News - very general & interesting class problems\nBad News - includes NP-Hard (intractable) problems\n\nSo, good behavior is a function of domain not the formulation as CSP.\n\n---\n\nSlide 3.1.10\n\nCSP Example\n\nGiven 40 courses (8.01, 8.02, ..., 6.034) & 10 terms (Fall 1, Spring 1, ..., Spring 5). Find a legal schedule.\n\n---\n\nSlide 3.1.11\n\nThe constraints we need to represent and enforce are as follows:\n\nThe pre-requisites of a course were taken in an earlier term (we assume the list contains all the pre-requisites).\nSome courses are only offered in the Fall or the Spring term.\nWe want to limit the schedule to a feasible load such as 4 courses a term.\nAnd, we want to avoid time conflicts where we cannot sign up for two courses offered at the same time.\n\n---\n\nCSP Example\n\nGiven 40 courses (8.01, 8.02, ..., 6.034) & 10 terms (Fall 1, Spring 1, ..., Spring 5). Find a legal schedule.\n\nConstraints\n- Pre-requisites\n- Courses offered on limited terms\n- Limited number of courses per term\n- Avoid time conflicts"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.1.11\n**CSP Example**\n- Given 40 courses (0.81, 0.82, ... 6.804) & 10 terms (Fall 1, Spring 1, ..., Spring 5). Find a legal schedule.\n\n**Constraints**\n- Pre-requisites\n- Course offered on limited terms\n- Limited number of courses per term\n- Avoid time conflicts\n\nNote, CSPs are not for expressing (soft) preferences e.g., minimize difficulty, balance subject, etc.\n\nSlide 3.1.12\nNote that all of these constraints are either satisfied or not. CSPs are not typically used to express preferences but rather to enforce hard and fast constraints.\n\nSlide 3.1.13\nOne key question that we must answer for any CSP formulation is \u201cWhat are the variables and what are the values?\u201d For our class scheduling problem, a number of options come to mind. For example, we might pick the terms as the variables. In that case, the values are combinations of four courses that are consistent, meaning that they are offered in the same term and whose times don\u2019t conflict. The pre-requisite constraint would relate every pair of terms and would require that no course appear in a term before that of any of its pre-requisite course.\n\nThis perfectly valid formulation has the practical weakness that the domains for the variables are huge, which has a dramatic effect on the running time of the algorithms.\n\nSlide 3.1.14\nOne way of avoiding the combinatorics of using 4-course schedules as the values of the variables is to break up each term into \"term-slots\" and assign to each term-slot a single course. This formulation, like the previous one, has the limit on the number of courses per term represented directly in the graph, instead of stating an explicit constraint. With this representation, we will still need constraints to ensure that the courses in a given term do not conflict and the pre-requisite ordering is enforced. The availability of a course in a given term could be enforced by filtering the domains of the variables.\n\nSlide 3.1.15\nAnother formulation turns things around and uses the courses themselves as the variables and then uses the terms (or more likely, term slots) as the values. Let\u2019s look at this formulation in greater detail.\n\nChoice of variables & values\n\n- **VARIABLES**\n  - A. Terms?\n\n- **DOMAINS**\n  - Legal combinations of for example 4 courses (but this is huge set of values).\n\nChoice of variables & values\n\n- **VARIABLES**\n  - A. Terms?\n  - B. Term Slots?\n\n- **DOMAINS**\n  - Legal combinations of for example 4 courses (but this is huge set of values).\n  - Courses offered during that term\n\nChoice of variables & values\n\n- **VARIABLES**\n  - A. Terms?\n  - B. Term Slots?\n  - C. Courses?\n\n- **DOMAINS**\n  - Legal combinations of for example 4 courses (but this is huge set of values).\n  - Courses offered during that term\n  - Terms or term slots allow expressing constraint on limited number of courses / term."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.1.16\n\nConstraints\nUse courses as variables and term slots as values.\n\n( term\nterm before 0 term after\n\nSlide 3.1.16\nOne constraint that must be represented is that the pre-requisites of a class must be taken before the actual class. This is easy to represent in this formulation. We introduce types of constraints called \"term before\" and \"term after\" which check that the values assigned to the variables, for example, 6.034 and 6.001, satisfy the correct ordering.\n\nNote that the undirected links shown in prior constraint graphs are now split into two directed links, each with complementary constraints.\n\nSlide 3.1.17\nThe constraint that some courses are only offered in some terms simply filters illegal term values from the domains of the variables.\n\nSlide 3.1.18\nThe limit on courses to be taken in a term argues for the use of term-slots as values rather than just terms. If we use term-slots, then the constraint is implicitly satisfied.\n\nSlide 3.1.19\nAvoiding time conflicts is also easily represented. If two courses occur at overlapping times then we place a constraint between those two courses. If they overlap in time every term that they are given, we can make sure that they are taken in different terms. If they overlap only on some terms, that can also be enforced by an appropriate constraint.\n\nConstraints\nUse courses as variables and term slots as values.\n\n(2 term\nterm before 0 term after\nCourses offered only in some terms 0 Filter domain\nLimit # courses o\nnot equal 0 0 for all pairs of vars.\nUse term-slots only once\nAvoid time conflicts o o 1 not equal in same term at overlapping times"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_006.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 3.2\n\nSlide 3.2.1\n\nWe now turn our attention to solving CSPs. We will see that the approaches to solving CSPs are\nsome combination of constraint propagation and search. We will look at these in turn and then\nlook at how they can be profitably combined.\n\n**Solving CSPs**\n\nSolving CSPs involves some combination of:\n1. Constraint propagation, to eliminate values that could\n   not be part of any solution\n2. Search, to explore valid assignments\n\nSlide 3.2.2 - Constraint Propagation (aka Arc Consistency)\n\nArc consistency eliminates values from domain of variable that can\nnever be part of a consistent solution.\n\nV_i  \u2014\u2014>  V_j  \n\nDirected arc (V_i, V_j) is arc consistent if\nVx\u2208D_i \u2203y\u2208D_j such that (x,y) is allowed by the constraint on the arc\n\nThe great success of Waltz\u2019s constraint propagation algorithm focused people\u2019s attention on CSPs.\nThe basic idea in constraint propagation is to enforce what is known as \u201cARC CONSISTENCY\u201d, \nthat is, if one looks at a directed arc in the constraint graph, say an arc from V_i to V_j, we say that this \narc is consistent if for every value in the domain of V_i, there exists some value in the domain of V_j \nthat will satisfy the constraint on the arc. \n\nSlide 3.2.3\n\nSuppose there are some values in the domain at the tail of the constraint arc (for V_i) that do not have \nany consistent partner in the domain at the head of the arc (for V_j). We achieve arc consistency by \ndropping those values from D_i. Note, however, that if we change D_i, we now have to check to make \nsure that any other constraint arcs that have D_i at their head are still consistent. It is this \nphenomenon that accounts for the name \u201cconstraint propagation\u201d.\n\n**Constraint Propagation (aka Arc Consistency)**\n\nArc consistency eliminates values from domain of variable that can never be part of a consistent solution.\n\nV_i  \u2014\u2014>  V_j\n\nDirected arc (V_i, V_j) is arc consistent if\nVx\u2208D_i \u2203y\u2208D_j such that (x,y) is allowed by the constraint on the arc\n\nWe can achieve consistency on arc by deleting values form D_i \n(domain of variable at tail of constraint arc) that fail this condition."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_007.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Constraint Propagation (aka Arc Consistency)**\n\nArc consistency eliminates values from domain of variable that can never be part of a consistent solution.\n\n(Vr)\n\nDirected arc (V_i, V_j) is arc consistent if \\[ \\forall x_i \\in D_i, \\exists x_j \\in D_j, such that  (x_i,x_j) is allowed by the constraint on the arc \\]\n\nWe can achieve consistency on arc by deleting values form D_i (domain of variable at tail of constraint arc) that fail this condition.\n\nAssume domains are of size at most d and there are n binary constraints.\n\nA simple algorithm for arc consistency is O(nd^2) \u2014 note that just verifying arc consistency takes O(d^2) for each arc.\n\n---\n\n**Slide 3.2.4**\n\nWhat is the cost of this operation? In what follows we will reckon cost in terms of \"arc tests\": the number of times we have to check (evaluate) the constraint on an arc for a pair of values in the variable domains of that arc. Assuming that domains have at most d elements and that there are at most n binary constraints (arcs), then a simple constraint propagation algorithm takes O(nd^3) arc tests in the worst case.\n\nIt is easy to see that checking for consistency of each arc for all the values in the corresponding domains takes O(d^2) arc tests, since we have to look at all pairs of values in two domains. Going through and checking each arc once requires O(nd^2) arc tests. But, we may have to go through and look at the arcs more than once as the deletions to a node's domain propagate. However, if we look at an arc only when one of its variable domains has changed (by deleting some entry), then no one can require checking more than d times and we have the final cost of O(nd^3) arc tests in the worst case.\n\n**Slide 3.2.5**\n\nLet's look at a trivial example of graph coloring. We have three variables with the domains indicated. Each variable is constrained to have values different from its neighbors.\n\nGraph Coloring:\nInitial Domains are indicated -->\n  No orter-color constraint\n\nV1:\n{ R, G, B }\n\nV2:\n{ R, G, B }\n\nV3:\n{ R, G, B }\n\n---\n\n**Slide 3.2.6**\n\n**Constraint Propagation Example**\n\nWe will now simulate the process of constraint propagation. In the interest of space, we will deal in this example with undirected arcs, which are just a shorthand for two directed arcs between the variables. Each step in the simulation involves examining one of these undirected arcs, seeing if the arc is consistent and, if not, deleting values from the domain of the appropriate variable.\n\nEach undirected constraint arc is really two directed constraint arcs, the effects shown above are from examining BOTH arcs.\n\n---\n\n**Slide 3.2.7**\n\nWe start with the V1-V2 arc. Note that for every value in the domain of V1 (R, G and B) there is some value in the domain of V2 that is consistent with that (that is, it is different from). So, for R in V1 there is a G in V2, for G in V1 there is an R in V2 and for B in V1 there is either R and G in V2. Similarly, for each entry in V2 there is a valid counterpart in V1. So, the arc is consistent and no changes are made.\n\nGraph Coloring \nInitial Domains are indicated\n(\\rightarrow Different-color constraint\n\nV1: { R, G, B }\n\nV2: { R, G, B }\n\nV3: { R, G, B }\n\nArc examined | Value deleted\nV1 \u2013 V2 | none"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_008.jpeg",
    "text": "Slide 3.2.8\nWe move to V\u2081\u2192V\u2083. The situation here is different. While R and B in V\u2081 can co-exist with the G in V\u2083, not so the G in V\u2081. And, so, we remove the G from V\u2081. Note that the arc in the other direction is consistent.\n\n[Diagram: Graph Coloring with initial domains and arc between V\u2081 and V\u2083 examined. Value deleted: V\u2081(G)]\n\nSlide 3.2.9\nMoving to V\u2082\u2192V\u2083, we note similarly that the G in V\u2082 has no valid counterpart in V\u2083 and so we drop it from V\u2082\u2019s domain. Although we have now looked at all the arcs once, we need to keep going since we have changed the domains for V\u2081 and V\u2082.\n\n[Diagram: Graph Coloring with V\u2082\u2192V\u2083 arc examined and value V\u2082(G) deleted]\n\nSlide 3.2.10\nLooking at V\u2081\u2192V\u2082 again we note that R in V\u2081 no longer has a valid counterpart in V\u2082 (since we have deleted G from V\u2082) and so we need to drop R from V\u2081.\n\n[Diagram: Graph Coloring with V\u2081\u2192V\u2082 arc examined and value V\u2081(R) deleted]\n\nSlide 3.2.11\nWe test V\u2081\u2192V\u2083 and it is consistent.\n\n[Diagram: Graph Coloring with arcs examined between V\u2081\u2192V\u2083 and none deleted]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n### Constraint Propagation Example\n\n#### Graph Coloring \n- V1: {Y, R, G}\n- V2: {Y, R}\n- V3: {Y, G}\n- V4: {G, B}\n\n| Arc examined |  Value deleted  |\n|------------------|-------------------|\n| V1 -> V2       | Y  |\n| V1 -> V3       | Y  |\n| V2 -> V3       | R  |\n| V2 -> V4       |  --   |\n| V3 -> V4       |  Y  |\n\n![Graph showing arcs between variables V1, V2, V3, V4](fig: Graph 3)\n\n#### Slide 3.2.12\nWe test V2->V3 and it is consistent.\n\nWe are done; the graph is arc consistent. In general, we will need to make one pass through any arc whose head variable has changed until no further changes are observed before we can stop. If at any point some variable has an empty domain, the graph has no consistent solution.\n\n#### Slide 3.2.13\nNote that whereas arc consistency is required for there to be a solution for a CSP, having an arc-consistent solution is not sufficient to guarantee a unique solution or even any solution at all. For example, this first graph is arc-consistent but there are NO solutions for it (we need at least three colors and have only two).\n\n![Graph Coloring](fig: Graph 4) \n- R, G\n- Arc Consistent but no solutions\n\n#### Slide 3.2.14\nThis next graph is also arc consistent but there are 2 distinct solutions: BRG and BGR.\n\n![Graph Coloring](fig: Graph 5) \n- R, G, B\n- Arc Consistent but 2 solutions : B, R, G : B, G, R.\n\n#### Slide 3.2.15\nThis next graph is also arc consistent but it has a unique solution, by virtue of the special constraint between two of the variables.\n\n![Graph Coloring](fig: Graph 6)\n- Arc consistent but no solutions\n- Arc consistent but 2 solutions : B, R, G : B, G, R.\n- Arc consistent but 1 solution\n\n- Assume B, G not allowed\n\n(GRAPHCOLORING DIAGRAM)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 3.2.16**\n\n**But, arc consistency is not enough in general**\n\n*Graph Coloring*\n\nIn general, if there is more than one value in the domain of any of the variables, we do not know whether there is zero, one, or more than one answer that is globally consistent. We have to search for an answer to actually know for sure.\n\narc consistent but no solution\n\n- $V_1$: 2, $V_2$: 3, $V_3$: R\n\narc consistent but 1 solution\n\n- $V_1$: 2, $V_2$: R, $V_3$: 3\n\narc consistent but 2 solutions\n\n- $V_1$: R, Q, R, - $V_2$: Q, B, Q, R, - $V_3$: B, G, B\n\n**Need to do search to find solutions (if any!)**\n\n---\n\n**Slide 3.2.17**\n\nHow does one search for solutions to a CSP problem? Any of the search methods we have studied is applicable. All we need to realize is that the space of assignments of values to variables can be viewed as a tree in which all the assignments of values to the first variable are descendants of the first node and all the assignments of values to the second variable from the descendants of those nodes and so forth.\n\nThe classic approach to searching such a tree is called \"backtracking\", which is just another name for depth-first search in this tree. Note, however, that we could use breadth-first search or any of the heuristic searches on this problem. The heuristic value could be used to either guide the search to termination or bias it towards desired solution based on preferences for certain assignments. Uniform-cost and A* would make sense also if there were a non-uniform cost associated with a particular assignment of a value to a variable (note that this is another (better but more expensive) way of incorporating preferences).\n\nHowever, you should observe that these CSP problems are different from the graph search problems you looked at before, in that we don't really care about the path to some state but just the final state itself.\n\n---\n\n**Slide 3.2.18**\n\n**Searching for solutions \u2014 backtracking (BT)**\n\nWhen we have too many values in domain (and/or constraints are weak) arc consistency doesn't do much, so we need to search. Simplest approach is pure backtracking (depth-first search).\n\n- $V_0$ assignments\n  \n   \\ \\ \\ \n\n- $V_1$ assignments\n  \n   \\ \\ \\  $\n\n  \\ _/_$ V_1 assignments\n\n  V_1\n\n**Slide 3.2.19**\n\nSo, we consider assigning $V_2=G$, which is consistent with the value for $V_1$. We then move to $V_3=R$. Since we have a constraint between $V_1$ and $V_3$, we have to check for consistency and find it is not consistent, and so we backup to consider another value for $V_3$.\n\n---\n\n**Searching for solutions \u2014 backtracking (BT)**\n\nWhen we have too many values in domain (and/or constraints are weak) arc consistency doesn't do much, so we need to search. Simplest approach is pure backtracking (depth-first search).\n\n- $V_0$ assignments\n\n  \\ \\ \\ $\n\n- V_1 assignments\n\n  \\ \\ \\ \n\n  Inconsistent with $V_1=\\n$\n\n  Backup at inconsistent assignment"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 3.2.20**\n\n**Searching for solutions \u2013 backtracking (BT)**\n\n_When we have too many values in domain (and/or constraints are weak) arc consistency doesn\u2019t do much, so we need to search. Simplest approach is pure backtracking (depth-first search)._\n\n**V\u2082, assignments**\n\n\\[diagram with circles and lines illustrating assignments\\]\n\n**V\u2083, assignments**\n\nInconsistent with V\u2082 \u2260 G\n\n\\[diagram showing failure\\]\n\n**Backup on inconsistent assignment**\n\nBut V\u2083 = G is inconsistent with V\u2082 = G, and so we have to backup. But there are no more pending values for V\u2083 or for V\u2082 and so we fail back to the V\u2081 level.\n\n---\n\n**Slide 3.2.21**\n\nThe process continues in that fashion until we find a solution. If we continue past the first success, we can find all the solutions for the problem (two in this case).\n\n---\n\n**Slide 3.2.22**\n\nWe can use some form of backtracking search to solve CSP independent of any form of constraint propagation. However, it is natural to consider combining them. So, for example, during a backtracking search where we have a partial assignment, where a subset of all the variables each has unique values assigned, we could then propagate these assignments throughout the constraint graph to obtain reduced domains for the remaining variables. This is, in general, advantageous since it decreases the effective branching factor of the search tree.\n\n---\n\n**Slide 3.2.23**\n\nBut, how much propagation should we do? Is it worth doing the full arc-consistency propagation we described earlier?\n\n---\n\n**Combine Backtracking & Constraint Propagation**\n\nA node in BT tree is a partial assignment in which the domain of each variable has been set (tentatively) to singleton set.\n\nUse constraint propagation (arc-consistency) to propagate the effect of this tentative assignment, i.e., eliminate values inconsistent with current values.\n\n---\n\n**Combine Backtracking & Constraint Propagation**\n\nA node in BT tree is a partial assignment in which the domain of each variable has been set (tentatively) to singleton set.\n\nUse constraint propagation (arc-consistency) to propagate the effect of this tentative assignment, i.e., eliminate values inconsistent with current values.\n\n**Question:** How much propagation to do?\n\n\\[Ref pg 39\\] \n\\[Ref page 39\\]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_012.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nCombine Backtracking & Constraint Propagation\n\nA node in BT tree is partial assignment in which the domain of each\nvariable has been set (tentatively) to singleton set.\n\nUse constraint propagation (arc-consistency) to propagate the effect of\nthis tentative assignment, i.e., eliminate values inconsistent with current\nvalues. \n\nQuestion: How much propagation to do?\n\nAnswer: Not much, just local propagation from domains with unique\nassignments, which is called forward checking (FC). This conclusion is not\nnecessarily obvious, but if it actually holds in practice.\n\nSlide 3.2.24\n\nThe answer is USUALLY no. It is generally sufficient to only propagate to the immediate neighbors \nof variables that have unique values (the ones assigned earlier in the search). That is, we eliminate \nfrom consideration any values for future variables that are inconsistent with the values assigned to \npast variables. This process is known as forward checking (FC) because one checks values for \nfuture variables (forward in time), as opposed to standard backtracking which checks value of past \nvariables (backwards in time, hence back-checking).\n\nWhen the domains at either end of a constraint arc each have multiple legal values, odds are that the \nconstraint is satisfied, and so checking the constraint is usually a waste of time. This conclusion \nsuggests that forward checking is usually as much propagation as we want to do. This is, of course, \nonly a rule of thumb.\n\nSlide 3.2.25\n\nLet's step through a search that uses a combination of backtracking with forward checking. We start \nby considering an assignment of V1=R.\n\nBacktracking with Forward Checking (BT-FC)\n\nWhen examining assignment V1=d1, remove any values inconsistent\nwith that assignment from neighboring domains in constraint graph.\n\n  V2 assignments\n  \n  V3 assignments\n\nSlide 3.2.26\n\nWe then propagate to the neighbors of V1 in the constraint graph and eliminate any values that are\ninconsistent with that assignment, namely the value R. That leaves us with the value G in the\ndomains of V2 and V3. So, we make the assignment V2=G and propagate.\n\nSlide 3.2.27\n\nBut, when we propagate to V3 we see that there are no remaining valid values and so we have found\nan inconsistency. We fail and backup. Note that we have failed much earlier than with simple \nbacktracking, thus saving a substantial amount of work.\n\nBacktracking with Forward Checking (BT-FC)\n\nWhen examining assignment V2=d2, remove any values inconsistent\nwith that assignment from neighboring domains in constraint graph.\n\n  V2 assignments\n\n  V3 assignments\n\n  V3 assignments\n\nWe have a conflict \nwhenever a domain\nbecomes empty."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Backtracking with Forward Checking (BT-FC)**\n\nWhen examining assignment V_i \u2192d_i, remove any values inconsistent with that assignment from neighboring domains in constraint graph.\n\nV\u2081 assignments\n\nV\u2082 assignments\n\nV\u2083 assignments\n\n[Image showing relations between V\u2081, V\u2082, V\u2083 in a constraint graph]\n\n**Slide 3.2.28**\nWe now consider V\u2081=G and propagate.\n\n**Slide 3.2.29**\nThat eliminates G from V\u2082 and V\u2083.\n\n**Backtracking with Forward Checking (BT-FC)**\n\nWhen examining assignment V_i \u2192d_i, remove any values inconsistent with that assignment from neighboring domains in constraint graph.\n\nV\u2081 assignments\n\nV\u2082 assignments\n\nV\u2083 assignments\n\n[Image showing relations between V\u2081, V\u2082, V\u2083 in a constraint graph]\n\n**Backtracking with Forward Checking (BT-FC)**\n\nWhen examining assignment V_i \u2192d_i, remove any values inconsistent with that assignment from neighboring domains in constraint graph.\n\nV\u2081 assignments\n\nV\u2082 assignments\n\nV\u2083 assignments\n\n[Image showing relations between V\u2081, V\u2082, V\u2083 in a constraint graph]\n\n**Slide 3.2.30**\nWe now consider V\u2082=R and propagate.\n\n**Slide 3.2.31**\nThe domain of V\u2083 is empty, so we fail and backup."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Backtracking with Forward Checking (BT-FC)**\n\n_When examining assignment \\( V_i=d \\), remove any values inconsistent with that assignment from neighboring domains in constraint graph._\n\n\\[ V_2 \\text{ assignments} \\]  \n\\[ V_1 \\text{ assignments} \\]  \n\\[ V_3 \\text{ assignments} \\]  \n\n*[Illustration of a constraint graph with V_1 connected to V_2 and V_3, with possible values R, G, B for each variable, and V_1 being assigned B in the diagram]*\n\n**Slide 3.2.32**\nSo, we move to consider \\( V_1 = B \\) and propagate.\n\n--- \n**Slide 3.2.33**\nThis propagation does not delete any values. We pick \\( V_2 = R \\) and propagate.\n\n---\n**Backtracking with Forward Checking (BT-FC)**\n\n_When examining assignment \\( V_i=d \\), remove any values inconsistent with that assignment from neighboring domains in constraint graph._\n\n\\[ V_2 \\text{ assignments} \\]  \n\\[ V_1 \\text{ assignments} \\]  \n\\[ V_3 \\text{ assignments} \\]  \n\n*[Illustration of a constraint graph similar to previous, but now V_2 is highlighted with R assigned]*\n\n**Slide 3.2.34**\nThis removes the R values in the domains of \\( V_1 \\) and \\( V_3 \\).\n\n---\n**Backtracking with Forward Checking (BT-FC)**\n\n_When examining assignment \\( V_i=d \\), remove any values inconsistent with that assignment from neighboring domains in constraint graph._\n\n\\[ V_2 \\text{ assignments} \\]  \n\\[ V_1 \\text{ assignments} \\]  \n\\[ V_3 \\text{ assignments} \\]  \n\n*[Illustration showing the change where V_2 having R removed from V_1 and V_3 domains]*\n\n**Slide 3.2.35**\nWe pick \\( V_3 = G \\) and have a consistent assignment.\n\n---\n**Backtracking with Forward Checking (BT-FC)**\n\n_When examining assignment \\( V_i=d \\), remove any values inconsistent with that assignment from neighboring domains in constraint graph._\n\n\\[ V_2 \\text{ assignments} \\]  \n\\[ V_1 \\text{ assignments} \\]  \n\\[ V_3 \\text{ assignments} \\]  \n\n*[Illustration showing V_3 assigned G, completing the propagation]*"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Backtracking with Forward Checking (BT-FC)**\n- When examining assignment V\u1d62=d\u2096, remove any values inconsistent with that assignment from neighboring domains in constraint graph.\n\nV\u2082 assignments\n\n\u2192 V\u2082\n\nV\u2083 assignments\n\n\u2192 V\n\nV\u2081 assignments\n\nSlide 3.2.36\nWe can continue the process to find the other consistent solution.\n\nSlide 3.2.37\nNote that when doing forward checking there is no need to check new assignments against previous assignments. Any potential inconsistencies have been removed by the propagation. BT-FC is usually preferable to plain BT because it eliminates from consideration inconsistent assignments once and for all rather than discovering the inconsistency over and over again in different parts of the tree. For example, in pure BT, an assignment for Y\u2083 that is inconsistent with a value of V\u2081 would be \"discovered\" independently for every value of V\u2082. Whereas FC would delete it from the domain of V\u2083 right away.\n\n---\n\n**Backtracking with Forward Checking (BT-FC)**\n- When examining assignment V\u1d62=d\u2096, remove any values inconsistent with that assignment from neighboring domains in constraint graph.\n\nV\u2082 assignments\n\n\u2192 V\n\nV\u2083 assignments\n\n\u2192 V\n\nV\u2081 assignments\n\n- No need to check previous assignments\n- Generally preferable to pure BT\n\n---\n\n**6.034 Notes: Section 3.3**\n\nSlide 3.3.1\nWe have been assuming that the order of the variables is given by some arbitrary ordering. However, the order of the variables (and values) can have a substantial effect on the cost of finding the answer. Consider, for example, the course scheduling problems using courses given in the order that they should ultimately be taken and assume that the term values are ordered as well. Then a depth first search will tend to find the answer very quickly.\n\nOf course, we generally don\u2019t know the answer to start off with, but there are more rational ways of ordering the variables than alphabetical or numerical order. For example, we could order the variables before starting by how many constraints they have. But, we can do even better by dynamically re-ordering variables based on information available during a search.\n\n---\n\n**BT-FC with dynamic ordering**\nTraditional backtracking uses fixed ordering of variables & values, e.g random order or place variables with many constraints first.\n\nYou can usually do better by choosing an order dynamically as the search proceeds."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_016.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.3.1\n\nBT-FC with dynamic ordering\n\nTraditional backtracking uses fixed ordering of variables & values, e.g., random order or place variables with many constraints first.\n\nYou can usually do better by choosing an order dynamically as the search proceeds.\n\n- Most constrained variable when doing forward-checking, pick variable with fewest legal values to assign next (minimizes branching factor).\n\nSlide 3.3.2\n\nFor example, assume we are doing backtracking with forward checking. At any point, we know the size of the domain of each variable. We can order the variables below that point in the search tree so that the most constrained variable (smallest valid domain) is next. This will have the effect of reducing the average branching factor in the tree and also cause failures to happen sooner.\n\nSlide 3.3.3\n\nFurthermore, we can count for each value of the variable the impact on the domains of its neighbors, for example the minimum of the resulting domains after propagation. The value with the largest minimum resulting domain size (or average value or sum) would be one that least constrains the remaining choices and is least likely to lead to failure.\n\nOf course, value ordering is only worth doing if we are looking for a single answer to the problem. If we want all answers, then all values will have to be tried eventually.\n\nSlide 3.3.4\n\nBT-FC with dynamic ordering\n\nTraditional backtracking uses fixed ordering of variables & values, e.g., random order or place variables with many constraints first.\n\nYou can usually do better by choosing an order dynamically as the search proceeds.\n\n- Most constrained variable when doing forward-checking, pick variable with fewest legal values to assign next (minimizes branching factor)\n\n- Least constraining value chose value that rules out the fewest values from neighboring domains\n\nE.g., this combination improves feasible n-queens performance from about n = 30 with just FC to about n = 1000 with FC & ordering.\n\nSlide 3.3.5\n\nThis combination of variable and value ordering can have dramatic impact on some problems.\n\nColors: R, G, B, Y\n\nA = Green\nB = Blue\nC = Red\n\nThis example of the 4-color map-coloring problem illustrates a simple situation for variable and value ordering. Here, A is colored Green, B is colored Blue and C is colored Red. What country should we color next, D or E or F?\n\nWhich country should we color next?\n\nWhat color should we pick for it?\n\n[Illustration of colored regions with a map highlighting countries A, B, C, D, E, and F applying constraints R, G, B, Y for colors]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_017.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.3.6\n\nColors: R, G, B, Y\n\nA=Green\nB=Blue\nC=Red\n\nE\nA\n\nGreen,\nBlue,\nYellow\nC\nRed,\n\nWhich country should we color next?\n\nE most-constrained variable (smallest domain)\n\nWhat color should we pick for it?\n\nWell, E is more constrained (has fewer) legal values so we should try it next. Which of E\u2019s values should we try next?\n\nSlide 3.3.7\n\nBy picking RED, we keep open the most options for D and F, so we pick that.\n\nSlide 3.3.8\n\nAll of the methods for solving CSPs that we have discussed so far are systematic (guaranteed searches). More recently, researchers have had surprising success with methods that are not systematic (they are randomized) and do not involve backup.\n\nThe basic idea is to do incremental repair of a nearly correct assignment. Imagine we had some heuristic that could give us a 'good' answer to any of the problems. By 'good' we mean one with relatively few constraint violations. In fact, this could even be a randomly chosen solution.\n\nThen, we could take the following approach. Identify a random variable involved in some conflict. Pick a new value for that variable that minimizes the number of resulting conflicts. Repeat.\n\nThis is a type of local 'greedy' search algorithm.\n\nThere are variants of this strategy that use this heuristic to do value ordering within a backtracking search. Remarkably, this type of ordering (in connection with a good initial guess) leads to remarkable behavior for benchmark problems. Notably, the systematic versions of this strategy can solve the million-queen problem in minutes. After this, people decided N-queens was not interesting...\n\nIncremental Repair (min-conflict heuristic)\n1. Initialize a candidate solution using 'greedy' heuristic - get solution 'near' correct one.\n2. Select a variable in conflict and assign it a value that minimizes the number of conflicts (peak less randomly).\n\nCan use this heuristic as part of systematic backtracker that uses heuristics to do value ordering or in local hill-climber (without backup).\n\nSec:\n(Support 16)\n\nPerformance on n-queens, with good initial guesses.\n\nSize (X)\n\n10^0\n10^1\n10^2\n10^5\n10^6\n10^7\n10^8\n\nY\n\n10^4\n10^2\n10^1\n10^0\n10^5"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games1\\page_018.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.3.9\n\nThe pure \"greedy\u201d hill-climber can readily fail on any problem (by finding a local minimum where any change to a single variable causes the number of conflicts to increase). We'll look at this a bit in the problem set.\n\nThere are several ways of trying to deal with local minima. One is to introduce weights on the violated constraints. A simpler one is to re-start the search with another random initial state. This is the approach taken by GSAT, a randomized search process that solves SAT problems using a similar approach to the one described here.\n\nGSAT\u2019s performance is nothing short of remarkable. It can solve SAT problems of mind-boggling complexity. It has forced a complete reconsideration of what it means when we say that a problem is \u201chard\". It turns out that for SAT, almost any randomly chosen problem is \"easy\u201d. There are really hard SAT problems but they are difficult to find. This is an area of active study.\n\nMin-conflict heuristic\n\nThe pure hill climber (without backtracking) can get stuck in local minima. Can add random moves to attempt getting out of minima \u2013 generally quite effective. Can also use weights on violated constraints & increase weight every cycle it remains violated.\n\nGSAT\n\nRandomized hill climber used to solve SAT problems. One of the most effective methods ever found for this problem.\n\nSlide 3.3.10\n\nGSAT as Heuristic Search\n\n\u2022 State space: Space of all full assignments to variables\n\u2022 Initial state: A random full assignment\n\u2022 Goal state: A satisfying assignment\n\u2022 Actions: Flip value of one variable in current assignment\n\u2022 Heuristic: The number of satisfied clauses (constraints); we want to maximize this. Alternatively, minimize the number of unsatisfied clauses (constraints).\n\nGSAT can be framed as a heuristic search strategy. Its state space is the space of all full assignments to the variables. The initial state is a random assignment, while the goal state is any assignment that satisfies the formula. The actions available to GSAT are simply to flip one variable in the assignment from true to false or vice-versa. The heuristic value used for the search, which GSAT tries to maximize, is the number of satisfied clauses (constraints). Note that this is equivalent to minimizing the number of conflicts, that is, violated constraints.\n\nSlide 3.3.11\n\nHere we see the GSAT algorithm, which is very simple in sketch. The critical implementation challenge is that of finding quickly the variable whose flip maximizes the score. Note that there are two user-specified variables: the number of times the outer loop is executed (MAXTRIES) and the number of times the inner loop is executed (MAXFLIPS). These parameters guard against local minima in the search, simply by starting with a new, randomly chosen assignment and trying a different sequence of flips. As we have mentioned, this works surprisingly well.\n\nGSAT(F)\n\n\u2022 For i=1 to Maxtries\n   \u2022 Select a complete random assignment A\n   \u2022 Score = number of satisfied clauses\n   \u2022 For j=1 to Maxflips\n      \u2013 If A (satisfies all clauses in F) return A\n      \u2013 Else flip a variable that maximizes score\n      \u2013 Flip a randomly chosen variable if no variable flip increases the score.\n\nSlide 3.3.12\n\nWALKSAT(F)\n\n\u2022 For i=1 to Maxtries\n   \u2022 Select a complete random assignment A\n   \u2022 Score = number of satisfied clauses\n   \u2022 For j=1 to Maxflips\n      \u2013 If A satisfies all clauses in F return A\n      \u2013 Else\n         \u2013 With probability P/ GSAT */ flip a variable that maximizes score * Flip a randomly chosen variable if no variable flip increases the score.\n         \u2013 With probability 1-P */ Random Walk */ Pick a random unsatisfied clause C * Flip a randomly chosen variable in C\n\nAn even more effective strategy turns out to add even more randomness. WALKSAT basically performs the GSAT algorithm some percentage of the time and the rest of the time it does a random walk in the space of assignments by randomly flipping variables in unsatisfied clauses (constraints).\n\nIt\u2019s both depressing to think that such simple randomized strategies can be so much more effective than clever deterministic strategies. There are signs at present that some of the clever deterministic strategies are becoming competitive or superior to the randomized ones. The story is not over."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 3.4\n\nSlide 3.4.1\nIn this section, we will look at some of the basic approaches for building programs that play two-person games such as tic-tac-toe, checkers and chess.\n\nMuch of the work in this area has been motivated by playing chess, which has always been known as a \"thinking person's game\". The history of computer chess goes way back. Claude Shannon, the father of information theory, originated many of the ideas in a 1949 paper. Shortly after, Alan Turing did a hand simulation of a program to play checkers, based on some of these ideas. The first programs to play real chess didn't arrive until almost ten years later, and it wasn't until Greenblatt's Machack 6 that a computer chess program defeated a good player. Slow and steady progress eventually led to the defeat of reigning world champion Garry Kasparov against IBM's Deep Blue in May 1997.\n\nGame Tree Search\n\n\u2022 Initial state: initial board position and player\n\u2022 Operators: one for each legal move\n\u2022 Goal states: winning board positions\n\u2022 Scoring function: assigns numeric value to states\n* Game tree: encodes all possible games\n\n\u2022 We are not looking for a path, only the next move to make\n  (that hopefully leads to a winning position)\n\u2022 Our best move depends on what the other player does\n\nSlide 3.4.2\nGame playing programs are another application of search. The states are the board positions (and the player whose turn it is to move). The operators are the legal moves. The goal states are the winning positions. A scoring function assigns values to states and also serves as a kind of heuristic function. The game tree (defined by the states and operators) is like the search tree in a typical search and it encodes all possible games.\n\nThere are a few key differences, however. For one thing, we are not looking for a path through the game tree, since that is going to depend on what moves the opponent makes. All we can do is choose the best move to make next.\n\nSlide 3.4.3\nLet's look at the game tree in more detail. Some board position represents the initial state and it's now our turn. We generate the children of this position by making all of the legal moves available to us. Then, we consider the moves that our opponent can make to generate the descendants of each of these positions, etc. Note that these trees are enormous and cannot be explicitly represented in their entirety for any complex game.\n\nBoard Games & Search\n\nMove generation      1949 Shannon paper\nStatic Evaluation    1951 Turing program\nMin Max              1958 Bernstein program\nAlpha Beta           55-60 Simon-Newell program\nPractical matters    61 MacHack 6     (a-I-*McCarthy*)\n                     61 Soviet Program\n                     66 \u2013 67 Machack 6 (MIT AI)\n                     70\u2019s NW Chess 4.5\n                     80\u2019s Cray Blitz\n                     90\u2019s Belle, Hitech, Deep Thought,\n                          Deep Blue\n\n                        by - spring04 - 1\n\nMove Generation\n\nGAME TREE\n\nb = branching factor\n\nd = depth\n\n[ GRAPHIC ]\n\nb = 36\n\n40    36    is big!\nd > 40\n\nChess\n\nby - spring04 - 2                          by - spring04 - 3"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.4.4\nPartial Game Tree for Tic-Tac-Toe\nHere\u2019s a little piece of the game tree for Tic-Tac-Toe, starting from an empty board. Note that even for this trivial game, the search tree is quite big.\n\nSlide 3.4.5\nA crucial component of any game playing program is the scoring function. This function assigns a numerical value to a board position. We can think of this value as capturing the likelihood of winning from that position. Since in these games one person\u2019s win is another\u2019s person loss, we will use the same scoring function for both players, simply negating the values to represent the opponent's scores.\n\n[Scoring function diagram with likelihood of winning from here]\n\nSlide 3.4.6\nStatic Evaluation\nA typical scoring function is a linear function in which some set of coefficients is used to weight a number of \"features\" of the board position. Each feature is also a number that measures some characteristic of the position. One that is easy to see is \"material\", that is, some measure of which pieces one has on the board. A typical weighting for each type of chess piece is shown here. Other types of features try to encode something about the distribution of the pieces on the board.\n\nc\u2081 = 1.0 material  \n+ c\u2082 = 0.5 pawn structure\n+ c\u2083 = 3.0 mobility  \n+ c\u2084 = 3.5 king safety  \n+ c\u2085 = 5.0 center control\n\nToo weak to predict ultimate success \n\nIn some sense, if we had a perfect evaluation function, we could simply play chess by evaluating the positions produced by each of our legal moves and picking the one with the highest score. In principle, such a function exists, but no one knows how to write it or compute it directly.\n\nSlide 3.4.7\nThe key idea that underlies game playing programs (presented in Shannon\u2019s 1949 paper) is that of limited look-ahead using the Min-Max algorithm.\n\nLet\u2019s imagine that we are going to look ahead in the game-tree to a depth of 2 (or 2 ply as it is called in the literature on game playing). We can use our scoring function to see what the values are at the leaves of this tree. These are called the \"static evaluations\". What we want is to compute a value for each of the nodes above this one in the tree by \"backing up\" these static evaluations in the tree.\n\nThe player who is building the tree is trying to maximize their score. However, we assume that the opponent (who values board positions using the same static evaluation function) is trying to minimize the score (or think of this as minimizing the negative of the score). So, each layer of the tree can be classified into either a maximizing layer or a minimizing layer. In our example, the layer right above the leaves is a minimizing layer, so we assign to each node in that layer the minimum score of any of its children. At the next layer up, we're maximizing so we pick the maximum of the scores available to us, that is, 7. So, this analysis tells us that we should pick the move that gives us the best guaranteed score, independent of what our opponent does. This is the MIN-MAX algorithm.\n\n[Limited look ahead + scoring diagram showing MIN-MAX and back-up scores]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 3.4.8**\n\nHere is pseudo-code that implements Min-Max. As you can see, it is a simple recursive alternation of maximization and minimization at each layer. We assume that we count the depth value down from the max depth so that when we reach a depth of 0, we apply our static evaluation to the board.  \n\n``` \nMin-Max  \n\n// initial call is MAX-VALUE(state, MAX-DEPTH)  \n\nfunction MAX-VALUE (state, depth)  \n  if (depth == 0) then return EVAL (state)  \n    v = -\u221e  \n    for each s in SUCCESSORS (state) do  \n      v = MAX(v, MIN-VALUE(s, depth-1))  \n    end  \n    return v  \nend  \n\nfunction MIN-VALUE (state, depth)  \n  if (depth == 0) then return EVAL (state)  \n    v = \u221e  \n    for each s in SUCCESSORS (state) do  \n      v = MIN(v, MAX-VALUE(s, depth-1))  \n    end  \n    return v  \nend  \n```\n\n---\n\n**Slide 3.4.9**\n\nThe key idea is that the more lookahead we can do, that is, the deeper in the tree we can look, the better our evaluation of a position will be, even with a simple evaluation function. In some sense, if we could look all the way to the end of the game, all we would need is an evaluation function that was 1 when we won and -1 when the opponent won.  \n\nThe truly remarkable thing is how well this idea works. If you plot how deep computer programs can search games trees versus their ranking, we see a graph that looks something like this. The earliest serious chess program (MacHack6), which had a ranking of 1200, searched on average to a depth of 4. Belle, which was one of the first hardware-assisted chess programs doubled the depth and gained about 800 points in ranking. Deep Blue, which searched to an average depth of about 13 beat the world champion with a ranking of about 2900.  \n\nAt some level, this is a depressing picture, since it seems to suggest that brute-force search is all that matters.  \n\n[Graph showing USCF rating vs. depth: \"World champ 2900\", \"Deep Blue 2600\", \"Belle 2000\", \"Expert 1900\", \"MacHack 1200\"]\n\n---\n\n**Slide 3.4.10**\n\nDeep Blue  \n\nAnd Deep Blue is brute indeed... It had 256 specialized chess processors coupled into a 32 node supercomputer. It examined around 30 billion moves per minute. The typical search depth was 13-ply, but in some dynamic situations it could go as deep as 30.  \n\n- 32 SP2 processors\n  - each with 8 dedicated chess processors\n  - 256 CP\n- 50 - 100 billion moves in 3 min\n  - 13-30 ply search.\n\n---\n\n**Slide 3.4.11**\n\nThere's one other idea that has played a crucial role in the development of computer game-playing programs. It is really only an optimization of Min-Max search, but it is such a powerful and important optimization that it deserves to be understood in detail. The technique is called alpha-beta pruning, from the Greek letters traditionally used to represent the lower and upper bound on the score.  \n\nHere's an example that illustrates the key idea. Suppose that we have evaluated the sub-tree on the left (whose leaves have values 2 and 7). Since this is a minimizing level, we choose the value 2. So, the maximizing player at the top of the tree knows at this point that he can guarantee a score of at least 2 by choosing the move on the left.  \n\nNow, we proceed to look at the subtree on the right. Once we look at the leftmost leaf of that subtree and see a 1, we know that if the maximizing player makes the move to the right then the minimizing player can force him into a position that is worth no more than 1. In fact, it might be much worse. The next leaf we look at might bring an even nastier surprise, but it doesn't matter what it is: we already know that this move is worse than the one to the left, so why bother looking any further? In fact, it may be that this unknown position was a great one for the maximizer, but then the minimizer would never choose it. So, no matter what happens at that leaf, the maximizer's choice will not be affected.  \n\n[Diagram illustrating alpha-beta pruning with max and min steps. \"\u03b1 = lower bound on score\", \"\u03b2 = upper bound on score\"]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\naffected.\n\nSlide 3.4.12\nHere's some pseudo-code that captures this idea. We start out with the range of possible scores (as defined by alpha and beta) going from minus infinity to plus infinity. Alpha represents the lower bound and beta the upper bound. We call Max-Value with the current board state. If we are at a leaf, we return the static value. Otherwise, we look at each of the successors of this state (by applying the legal move function) and for each successor, we call the minimizer (Min-Value) and we keep track of the maximum value returned in alpha. If the value of alpha (the lower bound on the score) ever gets to be greater or equal to beta (the upper bound) then we know that we don't need to keep looking - this is called a cutoff - and we return alpha immediately. Otherwise we return alpha at the end of the loop. The Minimizer is completely symmetric.\n\nfunction MAX-VALUE (state, \u03b1, \u03b2)\n    if (depth == 0) then return EVAL (state)\n    for each s in SUCCESSORS (state) do\n        \u03b1 = MAX (\u03b1, MIN-VALUE (s, \u03b1, \u03b2,depth-1))\n        if \u03b1 \u2265 \u03b2 then return \u03b1 // cutoff\n    end\n    return \u03b1\n\nfunction MIN-VALUE (state, \u03b1, \u03b2, depth)\n    if (depth == 0) then return EVAL (state)\n    for each s in SUCCESSORS (state) do\n        \u03b2 = MIN (\u03b2, MAX-VALUE (s, \u03b1, \u03b2, depth-1))\n        if \u03b2 \u2264 \u03b1 then return \u03b2 // cutoff\n    end\n    return \u03b2\n\nSlide 3.4.13\nLet's look at this program in operation on our previous example. We start with an initial call to Max-Value with the initial infinite values of alpha and beta, meaning that we know nothing about what the score is going to be.\n\n[-\u221e \u221e]\n\nSlide 3.4.14\nMax-Value now calls Min-Value on the left successor with the same values of alpha and beta. Min-Value now calls Max-Value on its leftmost successor.\n\nmin\n\n[-\u221e \u221e][-\u221e \u221e]\n\n2   7   1\n\n[10-lyph36 -1q]\n[10-lyph37 - 1q]\n[10-lyph38 -1q]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.4.15\nMax-Value is at the leftmost leaf, whose static value is 2 and so it returns that.\n\nSlide 3.4.16\nThis first value, since it is less than infinity, becomes the new value of beta in Min-Value.\n\nSlide 3.4.17\nSo, now we call Max-Value with the next successor, which is also a leaf whose value is 7.\n\nSlide 3.4.18\n7 is not less than 2 and so the final value of beta is 2 for this node.\n\n--- (figure of a tree illustrating the alpha-beta pruning) ---\nalpha-beta\n-\u221e \u221e\n      max\n            min\n                  2\n                  \n            2     \n                  7  1\n\n--- (figure of tree progression) ---\nalpha-beta\n-\u221e \u221e\n      max\n            min\n      -\u221e   2\n                  \n            2     \n                 7  1\n\n--- (repeat the above progression for different slides showing evolution) ---"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_006.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.4.19\nMin-Value now returns this value to its caller.\n\n(max)\n(min)\n2\n\nSlide 3.4.20\nThe calling Max-Value now sets alpha to this value, since it is bigger than minus infinity. Note that the range of [alpha beta] says that the score will be greater or equal to 2 (and less than infinity).\n\n(max)\n(min)\n2 7 1\n\u03b1 - \u03b2\n2 \u221e\n2\n-\u221e 2\n\nSlide 3.4.21\nMax-Value now calls Min-Value with the updated range of [alpha beta].\n\n(max)\n(min)\n2 7 1\n\u03b1 - \u03b2\n2 \u221e\n2\n-\u221e 2\n\nSlide 3.4.22\nMin-Value calls Max-Value on the left leaf and it returns a value of 1.\n\n(max)\n(min)\n2 7 1\n\u03b1 - \u03b2\n2 \u221e\n2\n-\u221e 2 2 \u221e"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_007.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.4.23\nThis is used to update beta in Min-Value, since it is less than infinity. Note that at this point we have \na range where alpha (2) is greater than beta (1).\n\nSlide 3.4.24\nThis situation signals a cutoff in Min-Value and it returns beta (1), without looking at the right leaf.\n\nSlide 3.4.25\nSo, basically we had already found a move that guaranteed us a score greater or equal to 2 so that when we got into a situation where the score was guaranteed to be less than or equal to 1, we could stop. So, a total of 3 static evaluations were needed instead of the four we would have needed under pure Min-Max.\n\nSlide 3.4.26\nWe can write alpha-beta in a more compact form that captures the symmetry between the Max-Value and Min-Value procedures. This is sometimes called the NegaMax form (instead of the Min-Max form). Basically, this exploits the idea that minimizing is the same as maximizing the negatives of the scores.\n\n// a = best score for MAX, b = best score for MIN\n// initial call is Alpha-Beta(state, \u2013\u221e, \u221e ,MAX-DEPTH)\nfunction Alpha-Beta (state, a, b,depth)\n    if (depth == 0) then return REAL (state)\n    for each s in SUCCESSORS (state) do\n        a = MAX(a, -Alpha-Beta (s, -b, -a, depth-1))\n        if a \u2265 b then return a // cutoff\n    end\nreturn a\n\nFigures show decision trees with alpha-beta pruning where evaluation cuts off further exploration of non-promising branches, optimizing the search process."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_008.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.4.27\nThere are a couple of key points to remember about alpha-beta pruning. It is guaranteed to return exactly the same value as the Min-Max algorithm. It is a pure optimization without any approximations or tradeoffs. \n\nIn a perfectly ordered tree, with the best moves on the left, alpha-beta reduces the cost of the search from order b^d to order b^(d/2), that is, we can search twice as deep! We already saw the enormous impact of deeper search on performance. So, this one simple algorithm can almost double the search depth.\n\nNow, this analysis is optimistic, since if we could order moves perfectly, we would not need alpha-beta. But, in practice, performance is close to the optimistic limit.\n\n                    \u03b1 - \u03b2\n\n1. Guaranteed same value as Max-Min\n2. In a perfectly ordered tree, expected work is O(b^(d/2), vs\n    O (b^d) for Max-Min, so can search twice as deep with \n    the same effort!\n3. With good move ordering, the actual running time is\n    close to the optimistic estimate.\n\nSlide 3.4.28\n\n          Game Program     \n     \u272a Move generator (ordered moves)\n     \u272a Static evaluation\n     \u272a Search control\n                        \n                   \nTime     50%\n              40%\n              10%\n\n(images)\n\n      databases\n\n       \u272a\u272a\u272a\u272a\u272a\n\n     20\t\tspeedsspeaker\tyears\n     \n        \n[all in place by late 60's]\n\nSlide 3.4.28\n\nIf one looks at the time spent by a typical game program, about half the time goes into generating the legal moves ordered (heuristically) in such a way to take maximum advantage of alpha-beta. Most of the remaining time is spent evaluating leaves. Only about 10% is spent on the actual search. \n\nWe should note that, in practice, chess programs typically play the first few moves and also complex end games by looking up moves in a database. \n\nThe other thing to note is that all these ideas were in place in MacHack in the late 60's. Much of the increased performance has come from increased computer power. The rest of the improvements come from a few other ideas that we'll look at later. First, let's look at a bit more of two components that account for the bulk of the time.\n\nSlide 3.4.29\nThe Move Generator would seem to be an unremarkable component of a game program, and this would be true if its only function were to list the legal moves. In fact, it is a crucial component of the program because its goal is to produce ordered moves. We saw that if the moves are ordered well, then alpha-beta can cutoff much of the search tree without even looking at it. So, the move generator actually encodes a fair bit of knowledge about the game.\n\nThere are a few criteria used for ordering the moves. One is to order moves by the value of the captured piece minus the value of the attacker, this is called the \"Most valuable victim/Least valuable attacker\" ordering, so obviously \"pawn-takes-Queen\" is the highest ranked move in chess under this ordering.\n\nFor non-capture moves, we need other ways of ordering them. One such strategy is known as the \"Killer heuristic\". The basic idea is to keep track of a few moves at each level of the search that cause cutoffs (killer moves) and try them first when considering subsequent moves at that level. Imagine a position with white to move. After white's first move we go into the next recursion of Alpha-Beta and find a move for Black which causes a beta cutoff for black. The reasoning is then that move F is a good move for black, a 'killer.' So when we try the next white move it seems reasonable to try move F next, before all others\n   \n       Move Generator\n\nLegal moves\nOrdered by\nMost valuable victim\nLeast valuable aggressor\nKiller heuristic\n\nSlide 3.4.30\nThe static evaluation function is the other place where substantial game knowledge is encoded. In the early chess players, the evaluation functions were very complex (and buggy). Over time it was discovered that using a simple, reliable evaluator (for example, just a weighted count of pieces on the board) and deeper search provided better results. Today, systems such as Deep Blue use static evaluators of medium complexity implemented in hardware. Not surprisingly, the \"cheap\" PC programs, which can't search as deeply as Deep Blue rely on quite complex evaluation functions. In general, there is a trade-off between the complexity of the evaluated and the depth of the search.\n\n             Static Evaluation\nInitially:  Very Complex\n  70's     Very simple\n                (material)\n\n          _   _\n       \\     /  ____\n      / \\  |    \\---------------\n               Deep searchers: moderately\n               complex (hardware)\n\n                PC programs: elaborate,\n                hand tuned\n               \\    /\\\n                   \\.--------------\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch3_csp_games2\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 3.4.31\nAs one can imagine in an area that has received as much attention as game playing programs, there are a million and one techniques that have been tried and which make a difference in practice. Here we touch on a couple of the high points.\n\nChess and other such games have incredibly large trees with highly variable branching factor (especially since alpha-beta cutoffs affect the actual branching of the search). If we picked a fixed depth to search, as we've suggested earlier, then much of the time we would finish too quickly and at other times take too long. A better approach is to use iterative deepening and thus always have a move ready and then simply stop after some allotted time.\n\nOne of the nice side effects of iterative deepening is that the results of the last iteration of the search can be used to help in the next iteration. For example, we can use the last search to order the moves. A somewhat less obvious advantage is that we can use the previous results to pick an initial value of alpha and beta. Instead of starting with alpha and beta at minus and plus infinity, we can start them in a small window around the values found in the previous search. This will help us cutoff more irrelevant moves early. In fact, it is often useful to start with the tightest possible window, something like [alpha, alpha+epsilon] which is simply asking \"is the last move we found still the best move?\" In many cases, it is.\n\nAnother issue in fixed depth searches is known as the \"horizon effect\". That is, if we pick a fixed depth search, we could miss something very important right over the horizon. So, it would not do to stop searching just as your queen is about to be captured. Most game programs attempt to assess whether a \"leaf\" node is in fact \"static\" or \"quiescent\" before allowing the search to finish. If the situation looks dynamic, the search is continued. In Deep Blue, as I mentioned earlier, some moves are searched to a depth of 30 ply because of this.\n\nObviously, Blue relied heavily on the use of parallelization in its search. This turns out to be surprisingly hard to do effectively and was probably the most significant innovation in Deep Blue.\n\nPractical matters\n\n- Iterative deepening\n  1) order best move from last search first\n  2) use previous backed up value to initialize (a, B)\n  3) keep track of repeated positions (transposition tables)\n\n- Horizon effect\n  \u2022 quiescence\n  \u2022 pulling the evaluation over search horizon\n\n- Parallelization\n\nSlide 3.4.32\nIn this section, we have focused on chess. There are a variety of other types of games that remain tantalizing on account of the relentless increase in computing power, and other games that require a different treatment.\n\nBackgammon is interesting because of the randomness introduced by the dice. Humans are not so good at building computer players for this directly, but a machine learning system (that essentially did a lot of search and used the results to build a very good evaluation function) was able to draw the human world-champion.\n\nRubber bridge is interesting because its hidden information (the other players\u2019 cards) and communication with a partner in a restricted language. Computer players, using search, excel now in the card-play phase of the game, but are still not too good at the bidding phase (which involves all the quirks of communication with another human).\n\nGo is actually in the same class of games as chess: there is no randomness, hidden information, or communication. But the branching factor is enormous and it seems not to be susceptible to search-based methods that work well in chess. Go players seem to rely more on a complex understanding of spatial patterns, which might argue for a method that is based more strongly on a good evaluation function than on brute-force search.\n\n- Backgammon\n  \u2022 Randomness \u2013 dice rolls\n  \u2022 Machine learning player was able to draw the human champion\n\n- Bridge\n  \u2022 Hidden information \u2013 other players' cards and communication through bidding\n  \u2022 Computer players play hard for card-play\n\n- Go\n  \u2022 No new elements but huge branching factor\n  \u2022 No good computer players exist\n\nSlide 3.4.33\nThere are a few observations about game playing programs that actually are observations about the whole symbolic approach to AI. The great successes of machine intelligence come in areas where the rules are clear and people have a hard time doing well, for example, mathematics and chess. What has proven to be really hard for AI are the more nebulous activities of language, vision and common sense, all of which evolution has been selecting for. Most of the research in AI today focuses on these less well defined activities.\n\nThe other observation is that it takes many years of gradual refinement to achieve human-level competence even in well-defined activities such as chess. We should not expect immediate success in attacking any of the great challenges of AI.\n\nOBSERVATIONS\n\n* Computers excel in well-defined activities where rules are clear\n  \u2022 chess\n  \u2022 mathematics\n\n* Success comes after a long period of gradual refinement\n\nFor more detail on building game programs visit: http://www1.ics.uci.edu/~eppstein/180a/ai99.html\n\n\u00a9 MIT"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n6.034 Notes: Section 4.1\n\nSlide 4.1.1\nSo far, we have talked a lot about building systems that have knowledge represented in them explicitly. One way to acquire that knowledge is to build it by hand. But that can be time-consuming and error prone. And many times, humans just don\u2019t have the relevant information available to them. \n\nFor instance, you are all experts in visual object recognition. But it\u2019s unlikely that you could sit down and write a program that would do the job even remotely as well as you can.\n\nBut, in lots of cases, when we don\u2019t have direct access to a formal description of a problem, we can learn something about it from examples.\n\nLearning\n- It is often hard to articulate the knowledge we need to build AI systems\n- Often, we don\u2019t even know it!\n- Frequently, we can arrange to build systems that learn it themselves\n\n---\n\nWhat is Learning?\n* memorizing something\n* learning facts through observation and exploration\n* improving motor and/or cognitive skills through practice\n* organizing new knowledge into general, effective representations\n\n---\n\nSlide 4.1.2\nThe word \u201clearning\u201d has many different meanings. It is used, at least, to describe\n- memorizing something\n- learning facts through observation and exploration\n- development of motor and/or cognitive skills through practice\n- organization of new knowledge into general, effective representations\n\n---\n\nSlide 4.1.3\nHerb Simon (an important historical figure in AI and in Economics) gave us this definition of learning:\n\n*Learning denotes changes in the system that are adaptive in the sense that they enable the system to do the task or tasks drawn from the same population more efficiently and more effectively the next time.*\n\nThis is not entirely precise, but it gives us the idea that learning systems have to acquire some information from examples of a problem, and perform better because of it (so we wouldn\u2019t call a system that simply logs all the images it has ever seen a learning system, because, although, in some sense, it knows a lot, it can\u2019t take advantage of its knowledge).\n\nWhat is Learning?\n* memorizing something\n* learning facts through observation and exploration\n* improving motor and/or cognitive skills through practice\n* organizing new knowledge into general, effective representations\n\n\u201cLearning denotes changes in the system that are adaptive in the sense that they enable the system to do the task or tasks drawn from the same population more efficiently and more effectively the next time.\u201d  \u2014 Herb Simon"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nInduction\n\nPiece of bread 1 was nourishing when I ate it.\nPiece of bread 2 was nourishing when I ate it.\nPiece of bread 100 was nourishing when I ate it.\n\nTherefore, all pieces of bread will be nourishing if I eat them.\n\nImage of David Hume removed due to copyright restrictions.\n\nDavid Hume\n\n---\n\nSlide 4.1.4\nOne of the most common kinds of learning is the acquisition of information with the goal of making predictions about the future. But what exactly gives us license to imagine we can predict the future? Lots of philosophers have thought about this problem. David Hume first framed the problem of induction as follows:\n\nPiece of bread number 1 was nourishing when I ate it.\nPiece of bread number 2 was nourishing when I ate it.\nPiece of bread number 3 was nourishing when I ate it.\nPiece of bread number 100 was nourishing when I ate it.\nTherefore, all pieces of bread will be nourishing if I eat them.\n\n---\n\nSlide 4.1.5\nAnd here, for no good reason, is a photo of David Hume's Tomb, just because I saw it once in Edinburgh.\n\n---\n\nWhy is Induction Okay?\n\nImage of Bertrand Russell removed due to copyright restrictions.\n\nBertrand Russell\n\nhttp://www.ditext.com/russell/russ.html\n\n---\n\nSlide 4.1.6\nWhen and why is it okay to apply inductive reasoning??\n\nMy favorite quote on the subject is the following, excerpted from Bertrand Russell's \"On Induction\":  \n\n(see http://www.ditext.com/russell/russ.html for the whole thing)\n\nIf asked why we believe the sun will rise tomorrow, we shall naturally answer, \u2018Because it has always risen every day\u2019. We have a firm belief that it will rise in the future, because it has risen in the past.\n\nThe real question is: Do any number of cases of a law being fulfilled in the past afford evidence that it will be fulfilled in the future?\n\nIt has been argued that we have reason to know the future will resemble the past, because what was the future has constantly become the past, and has always been found to resemble the past, so that we really have experience of the future, namely of times which were formerly future, which we may call past futures. But such an argument really begs the very question at issue. We have experience of past futures, but not of future futures, and the question is: Will future futures resemble past futures?\n\nWe won't worry too much about this problem. If induction is not, somehow, justified, then we have no reason to get out of bed in the morning, let alone study machine learning! \n\n---\n\nImage of David Hume removed due to copyright restrictions.\n\nImage removed due to copyright restrictions."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.1.7\nWhen viewed technically, there are lots of different kinds of machine learning problems. We'll just sketch them out here, so you get an idea of their range.\n\nSlide 4.1.8\nSupervised learning is the most common learning problem. Let's say you are given the weights and lengths of a bunch of individual salmon fish, and the weights and lengths of a bunch of individual tuna fish. The job of a supervised learning system would be to find a predictive rule that, given the weight and length of a fish, would predict whether it was a salmon or a tuna.\n\nSlide 4.1.9\nAnother, somewhat less well-specified, learning problem is clustering. Now you\u2019re given the descriptions of a bunch of different individual animals (or stars, or documents) in terms of a set of features (weight, number of legs, presence of hair, etc), and your job is to divide them into groups, or possibly into a hierarchy of groups that \u201cmakes sense.\u201d What makes this different from supervised learning is that you\u2019re not told in advance what groups the animals should be put into; just that you should find a natural grouping.\n\nSlide 4.1.10\nAnother learning problem, familiar to most of us, is learning motor skills, like riding a bike. We call this reinforcement learning. It\u2019s different from supervised learning because no-one explicitly tells you the right thing to do; you just have to try things and see what makes you fall over and what keeps you upright.\n\nSupervised learning is the most straightforward, prevalent, and well-studied version of the learning problem, so we are going to start with it, and spend most of our time on it. Most of the fundamental insights into machine learning can be seen in the supervised case.\n\n---\n\nKinds of Learning\n- Supervised learning: given a set of example input/output pairs, find a rule that does a good job of predicting the output associated with a new input.\n- Clustering: given a set of examples, but no labeling of them, group the examples into \u201cnatural\u201d clusters\n- Reinforcement learning: an agent interacting with the world makes observations, takes actions, and is rewarded or punished; it should learn to choose actions in such a way as to obtain a lot of reward\n\n---\n\nSome slides repeat parts as follows:\n\nKinds of Learning\n- Supervised learning: given a set of example input/output pairs, find a rule that does a good job of predicting the output associated with a new input.\n\n- Clustering: given a set of examples, but no labeling of them, group the examples into \u201cnatural\u201d clusters"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.1.11\nOne way to think about learning is that you're trying to find the definition of a function, given a bunch of examples of its input and output. This might seem like a pretty narrow definition, but it actually covers a lot of cases.\n\nLearning a Function\nGiven a set of examples of input/output pairs, find a function that does a good job of expressing the relationship\n\nSlide 4.1.12\nLearning how to pronounce words can be thought of as finding a function from letters to sounds.\nLearning how to throw a ball can be thought of as finding a function from target locations to joint torques.\nLearning to recognize handwritten characters can be thought of as finding a function from collections of image pixels to letters.\nLearning to diagnose diseases can be thought of as finding a function from lab test results to disease categories.\n\nSlide 4.1.13\nThe problem of learning a function from examples, is complicated. You can think of at least three different problems being involved: memory, averaging, and generalization. We'll look at each of these problems, by example.\n\nAspects of Function Learning\n- memory\n- averaging\n- generalization\n\nSlide 4.1.14\nExample problem\nWhen to drive the car?  Depends on:\n- temperature\n- expected precipitation\n- day of the week\n- whether she needs to shop on the way home\n- what she's wearing\n\nWe'll do this in the context of a silly example problem. Imagine that I'm trying to predict whether my neighbor is going to drive into work tomorrow, so I can ask for a ride. Whether she drives into work seems to depend on the following attributes of the day: the temperature, what kind of precipitation is expected, the day of the week, whether she needs to shop on the way home, and what she's wearing."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.1.15\nOkay, Let's say we observe our neighbor on three days, which are described in the table, which\nspecifies the properties of the days and whether or not the neighbor walked or drove.\n\n---\n\nMemory  \n| temp | precip | day | shop | clothes |  \n| 80    | none   | sat | no   | casual  | walk |  \n| 19    | snow   | mon | yes  | casual  | drive |  \n| 65    | none   | tues| no   | casual  | walk |\n\n---\n\nSlide 4.1.16\nNow, we find ourselves on a snowy 19-degree Monday, when the neighbor is wearing casual clothes and going shopping. Do you think she's going to drive?\n\n---\n\nSlide 4.1.17\nThe standard answer in this case is \"yes.\" This day is just like one of the ones we've seen before, and so it seems like a good bet to predict \"yes.\" This is about the most rudimentary form of learning, which is just to memorize the things you've seen before. Still, it can help you do a better job in the future, if those same cases arise again.\n\n---\n\nMemory  \n| temp | precip | day | shop | clothes |  \n| 80    | none   | sat | no   | casual  | walk |  \n| 19    | snow   | mon | yes  | casual  | drive |  \n| 65    | none   | tues| no   | casual  | walk |\n\nNew Observation:  \n| 19    | snow   | mon | yes  | casual  | drive |\n\n---\n\nSlide 4.1.18\nAveraging  \nDealing with noise in the data  \n| temp | precip | day | shop | clothes |  \n| 80    | none   | sat | no   | casual  | walk |  \n| 80    | none   | sat | no   | casual  | drive |  \n| 65    | none   | mon | no   | casual  | drive |  \n| 80    | none   | sat | yes  | casual  | drive |  \n| 80    | none   | sat | no   | casual  | drive |  \n| 80    | none   | sat | no   | casual  | walk |  \n| 80    | none   | sat | no   | casual  | drive |  \n| 80    | none   | sat | no   | casual  | walk |\n\nSlide 4.1.18\nThings are not always as easy as they were in the previous case. What if you get this set of data?"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_006.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.1.19\nNow, we ask you to predict what\u2019s going to happen. You\u2019ve certainly seen this case before. But the problem is that it has had different answers. Our neighbor is not entirely reliable.\n\nSlide 4.1.20\nThere are a couple of plausible strategies here. One would be to predict the majority outcome. The neighbor walked more times than she drove in this situation, so we might predict \u201cwalk\u201d. Another option would be to decline to commit to one prediction, but to generate a probability instead. You might answer that you think, with probability 5/7, she will walk.\n\nIn this course, we\u2019ll give a treatment of machine learning that doesn\u2019t really rely on probability. But in many cases, the algorithms and concepts we talk about have probabilistic foundations, and there are many more sophisticated techniques that rely on a deep understanding of probability. So, if you like machine learning, take probability, and then take the machine learning class!\n\nAveraging\nDealing with noise in the data\n```\ntemp\\tprecip\\tday\\tshop\\tclothes\n80\\tnone\\tsat\\tno\\tcasual\\twalk\n78\\tnone\\tsat\\tno\\tformal\\twalk\n82\\tnone\\tsat\\tno\\tcasual\\tdrive\n18\\tnone\\tsat\\tno\\tcasual\\twalk\n21\\tnone\\tsat\\tno\\tformal\\twalk\n17\\tnone\\tsat\\tt no\\tcasual\\tdrive\n```\n\nSlide 4.1.21\nHere\u2019s another situation in which noise is an issue. This time, the noise has corrupted our descriptions of the situation (our thermometer is somewhat unreliable, as is our ability to assess the formality of the neighbor\u2019s clothing, given her peculiar taste).\n\nSlide 4.1.22\nIn this case, we\u2019d like to recognize that the day that we\u2019re asked to do the prediction for is apparently similar to cases we\u2019ve seen before. It is ultimately impossible to tell whether this day is really different from those other days, or whether our sensors are just acting up. We\u2019ll just have to accept that uncertainty.\n\nSo, we\u2019ll have to treat this as an instance of the more general problem of generalization.\n\nSensor noise\nDealing with noise in the data\n```\ntemp\\tprecip\\tday\\tshop\\tclothes\n80\\tnone\\tsat\\tno\\tcasual\\twalk\n82\\tnone\\tsat\\tno\\tcasual\\twalk\n78\\tnone\\tsat\\tno\\tcasual\\tdrive\n21\\tnone\\tsat\\tno\\tcasual\\tdrive\n18\\tnone\\tsat\\tno\\tcasual\\tdrive\n17\\tnone\\tsat\\tno\\tcasual\\tdrive\n```"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_007.jpeg",
    "text": "Slide 4.1.23  \nTake a good look at this data set.\n\n---\n\n| temp | precip | day | shop | clothes |\n|------|--------|-----|------|---------|\n| 71   | none   | prep | casual | drive  |\n| 36   | none   | sun  | yes    | casual | walk   |\n| 62   | rain   | wed  | no     | casual | walk   |\n| 63   | none   | mon  | no     | formal | drive  |\n| 55   | none   | wed  | no     | casual | drive  |\n| 58   | snow   | mon  | yes    | formal | walk   |\n| 65   | none   | mon  | no     | casual | walk   |\n\n---\n\nSlide 4.1.24  \nNow, consider this day. It\u2019s 58 degrees and raining on a Monday. The neighbor is wearing casual clothing and doesn\u2019t need to shop. Will she walk or drive?\n\nThe first thing to observe is that there\u2019s no obviously right answer. We have never seen this case before. We could just throw up our hands and decline to make a prediction. But ultimately, we have to decide whether or not to call the neighbor. So, we might fall back on some assumptions about the domain. We might assume, for instance, that there\u2019s a kind of smoothness property: similar situations will tend to have similar categories.\n\nYou might plausibly make any of the following arguments:\n\n- She\u2019s going to walk because it\u2019s raining today and the only other time it rained, she walked.\n- She\u2019s going to drive because she has always driven on Mondays.\n- She\u2019s going to walk because she only drives if she is wearing formal clothes, or if the temperature is above 90 or below 20.\n\nThe question of which one to choose is hard. It\u2019s one of the deep underlying problems of machine learning. We\u2019ll look at some examples to try to get more intuition, then come back and revisit it a bit more formally in the next section.\n\n---\n\nSlide 4.1.25\nImagine that you were given all these points, and you needed to guess a function of their x, y coordinates that would have one output for the red ones and a different output for the black ones.\n\n[Image of a scatter plot with red and black points scattered]\n\nThe Red and the Black"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_008.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nWhat's the right hypothesis?\n\n_Slide 41.26_\nIn this case, it seems like you could do pretty well by defining a line that separates the two classes.\n\n<_Diagram: A set of black and red dots, separated by a diagonal line.>\n\n---\n\n_Slide 41.27_\nNow, what if we have a slightly different configuration of points? We can't divide them conveniently with a line.\n\n---\n\nNow, what's the right hypothesis?\n\n<_Diagram: A set of black and red dots.>\n\n---\n\n_Slide 41.28_\nBut this parabola-like curve seems like it might be a reasonable separator.\n\n<_Diagram: A set of black and red dots, separated by a curved line.>\n\n---\n\n_Slide 41.29_\nNow, what? This seems much trickier. There are at least a couple of reasonable kinds of answers.\n\n<_Diagram: A set of black and red dots in a mixed arrangement.>\n\nHow about now?"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 4.1.30**  \nHere's an answer. It successfully separates the reds from the blacks. But there's something sort of unsatisfactory about it. Most people have an intuition that it's too complicated.\n\n**Slide 4.1.31**  \nHere's another answer. It's not a perfect separator; it gets some of the points wrong. But it's prettier.\n\nIn general, we will always be faced with making trade-offs between hypotheses that account for the data perfectly and hypotheses that are, in some sense, simple. There are some mathematical ways to understand these trade-offs; we'll avoid the math, but try to show you, intuitively, how they come up and what to do about them.\n\n**Variety of Learning Methods**  \nLearning methods differ in terms of:\n- the form of the hypothesis\n- the way the computer finds a hypothesis given the data  \n\n**Slide 4.1.32**  \nThere are lots and lots of different kinds of learning algorithms. I want to show you cartoon versions of a couple of them first, and then we'll get down to the business of understanding them at a detailed algorithmic level.\n\nThe learning methods differ both in terms of the kinds of hypotheses they work with and the algorithms they use to find a good hypothesis given the data.\n\n**Slide 4.1.33**  \nHere's the most simple-minded of all learning algorithms (and my personal favorite (which might imply something about me!)). It's called nearest neighbor (or lazy learning). You simply remember all the examples you've ever seen.\n\n**Nearest Neighbor**  \n- Remember all your data\n\n---\n\n**Figures:**\n1. How about now? Answer 1: Shows a diagram with black and red points separated by a wavy blue line.\n2. How about now? Answer 2: Shows a diagram with black and red points separated by a straight line."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n## Nearest Neighbor  \n- Remember all your data\n- When someone asks a question,\n  - find the nearest old data point\n  - return the answer associated with it  \n\n**Slide 4.1.34**  \nWhen someone asks you a question, you simply find the existing point that's nearest the one you were asked about, and return its class.\n\n---\n\n**Slide 4.1.35**\nSo, in this case, the nearest point to the query is red, so we'd return red.\n\n## Nearest Neighbor  \n- Remember all your data\n- When someone asks a question,\n  - find the nearest old data point\n  - return the answer associated with it\n\n---\n\n## Decision Trees  \nUse all the data to build a tree of questions with answers at the leaves\n\n**Slide 4.1.36**  \nAnother well-loved learning algorithm makes hypotheses in the form of decision trees. In a decision tree, each node represents a question, and the arcs represent possible answers. We can use this decision tree to find out what prediction we should make in the drive/walk problem.\n\n---\n\n## Decision Trees\nUse all the data to build a tree of questions with answers at the leaves\n\n- none  \n  - clothes\n    - formal\n      - drive\n    - casual\n      - shop?\n        - yes\n          - drive\n        - no\n          - temp > 90?\n            - yes\n               - walk\n            - no\n               - weekend?\n                 - yes\n                   - walk\n                 - no\n                   - drive\n  - precip\n    - rain\n      - walk\n    - snow\n      - drive\n\n**Slide 4.1.37**\nWe'd start by asking what the current precipitation is. If it's snow, we just stop asking questions and drive."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Decision Trees**\n\nUse all the data to build a tree of questions with answers at the leaves\n\nFigure description:\n- root node: none\n  - If precip: walk (leaf node)\n  - If no precip: formal\n    - If formal: drive (leaf node)\n    - else If casual: temp > 90?\n      - If yes: walk (leaf node)\n      - If no: shop?\n        - If yes: weekend?\n          - If no: walk (leaf node)\n          - If yes: drive (leaf node)\n        - If no shop: drive (leaf node)\n\n**Slide 4.1.38**\nIf there's no precipitation, then we have to ask what kind of clothes the neighbor is wearing. If they're formal, she'll drive. If not, we have to ask another question.\n\n**Slide 4.1.39**\nWe can always continue asking and answering questions until we get to a leaf node of the tree; the leaf will always contain a prediction.\n\nHypotheses like this are nice for a variety of reasons. One important one is that they're relatively easily interpretable by humans. So, in some cases, we run a learning algorithm on some data and then show the results to experts in the area (astronomers, physicians), and they find that the learning algorithm has found some regularities in their data that are of real interest to them.\n\n---\n\n**Neural Networks**\n\n- Represent hypotheses as combinations of simple computations\n- Neurophysiologically plausible (sort of)\n\n**Slide 4.1.40**\nMost people have heard of neural networks. They can represent complicated hypotheses in high-dimensional continuous spaces. They are attractive as a computational model because they are composed of many small computing units. They were motivated by the structure of neural systems in parts of the brain. Now it is understood that they are not an exact model of neural function, but they have proved to be useful from a purely practical perspective.\n\n**Slide 4.1.41**\nIn neural networks, the individual units do a simple, fixed computation that combines the values coming into them with \"weights\" on the arcs. The learning process consists of adjusting the weights on the arcs in reaction to errors made by the network.\n\nLearning through weight adjustment"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_012.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nMachine Learning Successes\n- assessing loan credit risk\n- detecting credit card fraud\n- cataloging astronomical images\n- deciding and diagnosing manufacturing faults\n- helping NBA coaches analyze performance\n- personalizing news and web searches\n- steering an autonomous car across the US\n\nSlide 4.1.2\nMachine learning methods have been successfully fielded in a variety of applications, including:\n\n- assessing loan credit risk\n- detecting credit card fraud\n- cataloging astronomical images\n- deciding which catalogs to send to your house\n- helping NBA coaches analyze players' performance\n- personalizing news and web searches\n- steering an autonomous car across the US\n\nNow, we'll roll up our sleeves, and see how all this is done.\n\n---\n\n6.034 Notes: Section 4.2\n\nSlide 4.2.1\nA supervised learning problem is made up of a number of ingredients. The first is the data (sometimes called the training set). The data, D, is a set of m input-output pairs. We will write the ith element of the data set as (x\u1d62,y\u1d62). In the context of our old running example, an x\u1d62 would be a vector of values, one for each of the input features, and a y\u1d62 would be an output value (walk or drive).\n\nGiven data (training set)\nD = {(x\u00b9, y\u00b9), (x\u00b2, y\u00b2), ..., (x\u1d50, y\u1d50)}\n\nSlide 4.2.2\nEach x\u1d62 is a vector of n values. We'll write x\u1d62^j for the jth feature of the ith input-output pair. We'll consider different kinds of features. Sometimes we'll restrict ourselves to the case where the features are only 0s and 1s. Other times, we'll let them be chosen from a set of discrete elements (like \"snowy,\" \"rain,\" \"none\"). And, still other times, we'll let them be real values, like temperature, or weight."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.3\nSimilarly, the output, y[i] might be a boolean, a member of a discrete set, or a real value. For a \nparticular problem, the y[i] will all be of a particular type.\n\nWhen y[i] is a boolean, or a member of a discrete set, we will call the problem a classification \nproblem. When y[i] is real-valued, we call this a regression problem.\n\nSlide 4.2.4\nSupervised Learning\n\n\u2022 Given data (training set)\n\n  D = { (x1\u2019, y1\u2019), (x2\u2019, y2\u2019), \u2026\u2026..(x*m*, y*m*)}\n\n                  input               output\n\n\u2022 Classification: discrete Y\n\u2022 Regression: continuous Y\n\nSlide 4.2.5\nSupervised Learning\n\n\u2022 Given data (training set)\n\n  D = { (x1\u2019, y1\u2019), (x2\u2019, y2\u2019), \u2026\u2026..(x*m*, y*m*)}\n\n        input               output\n\n\u2022 Classification: discrete Y\n\u2022 Regression: continuous Y\n\n\u2022 Goal: find a hypothesis h in hypothesis class H that \n  does a good job of mapping x to y\n\nSlide 4.2.4\nNow, the goal of learning will be to find a hypothesis, h, that does a good job of describing the \nrelationship between the inputs and the outputs. So, a part of the problem specification is capital H, \nthe hypothesis class. H is the set of possible hypotheses that we want our learning algorithm to \nchoose from. It might be something like decision trees with 6 nodes, or lines in two-dimensional \nspace, or neural networks with 3 components.\n\nSlide 4.2.5\nSo, now we have a bunch of data, and a class of possible answers (hypotheses) and we\u2019re supposed \nto return the best one. In order to do that, we need to know exactly what is meant by \u201cbest\u201d.\n\nSlide 4.2.6\nBest Hypothesis\n\nHypothesis should\n\n\u2022 do a good job of describing the data\n\n\u2022 not be too complex\n\nSlide 4.2.6\nThere are typically two components to the notion of best. The hypothesis should do a good job of \ndescribing the data and it shouldn\u2019t be too complicated."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.7\nIdeally, we would like to find a hypothesis h such that, for all data points i, h(x^i) = y^i. We will not always be able to (or maybe even want to) achieve this, so perhaps it will only be true for most of the data points, or the equality will be weakened into \"not too far from\". \n\nWe can sometimes develop a measure of the \u201cerror\u201d of a hypothesis to the data, written E(h, D). It might be the number of points that are miscategorized, for example.\n\nSlide 4.2.8\nAnother issue is the complexity of the hypothesis. We can measure the complexity of a decision tree by the number of nodes it has, or a line by how bendy it is. In general, we'll define a measure of the complexity of hypotheses in H, C(h).\n\nSlide 4.2.9\nWhy do we care about hypothesis complexity? We have an intuitive sense that, all things being equal, simpler hypotheses are better. Why is that? There are lots of statistical and philosophical and information-theoretic arguments in favor of simplicity. \n\nSo, for now, let\u2019s just make recourse to William of Ockham, a 14th century Franciscan theologian, logician, and heretic. He is famous for \"Ockham's Razor\", or the principle of parsimony: \"Non sunt multiplicanda entia praeter necessitatem\". That is, \"entities are not to be multiplied beyond necessity\". People interpret this to mean that we should always adopt the simplest satisfactory hypothesis.\n\nSlide 4.2.10\nSo, given a data set, D, a hypothesis class H, a goodness-of-fit function E, and a complexity measure C, our job will be to find the h in H that minimizes E(h, D) + alpha * C(h), where alpha is a number that we can vary to change how much we value fitting the data well versus having a complex hypothesis.\n\nWe won\u2019t, in general, be able to do this efficiently, so, as usual, we'll have to make a lot of approximations and short-cuts. In fact, most learning algorithms aren't described exactly this way. But it\u2019s the underlying principle behind most of what we do.\n\n---\n\n**Best Hypothesis**\nHypothesis should:\n- do a good job of describing the data\n  - ideally: h(x^i) = y^i\n  - number of errors: E(h, D)\n- not be too complex\n  - measure: C(h)\n\n---\n\n***Trade-off***\nMinimize E(h, D) + \u03b1C(h)\n\nImage of William of Ockham removed due to copyright restrictions.\nWilliam of Ockham\n\n---\n\nNon sunt multiplicanda\nentia praeter necessitatem\n\"Entities are not to be multiplied beyond necessity\"\nWilliam of Ockham"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright\u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.11\nHow can we go about finding the hypothesis with the least value of our criterion? We can certainly think of this as a search problem, although many learning algorithms don't seem to have the character of doing a search.\n\nFor some carefully-chosen combinations of hypothesis classes and error criteria, we can calculate the optimal hypothesis directly.\n\nIn other cases, the hypothesis space can be described with a vector of real-valued parameters, but for which we can't calculate the optimal values. A common practice for these problems is to do a kind of local hill-climbing search called gradient ascent (or descent).\n\nFinally, sometimes it is possible to construct a hypothesis iteratively, starting with a simple hypothesis and adding to it in order to make it fit the data better. These methods are often called \u201cgreedy\", because they start by trying to pick the best simple hypothesis, and then make the best addition to it, etc. The result isn't optimal, but it is often very good. We will start by looking at some\nmethods of this kind.\n\nLearning as Search\n\n\u2022 How can we find the hypothesis with the lowest \nvalue of E(h,D) + c(h) ? Search!\n\n\u2022 For some hypothesis classes we can calculate\nthe optimal h directly: (linear separators)\n\n\u2022 For others, do local search (gradient descent in\nneural networks)\n\n\u2022 For some structured hypothesis spaces,\nconstruct one greedily\n\nSlide 4.2.12\nIn the following sections, we speak about a set of problems in which the examples are described using vectors of binary variables. In such a space, it\u2019s convenient to characterize the hypotheses using propositional logic. We will talk about logic a lot in the last part of the course, but we just need to introduce some notation here.\n\nWe'll use single letters to stand for variables that can be true or false (often we'll use 1 for \"true\" and 0 for \u201cfalse\u201d).\n\n\u2018^\u2019 a \u201ccaret\u201d B says that A and B both have to be true.\n\n\u2018v\u2019 a \u201cvee\u201d B says that either A or B (or both) have to be true.\n\nThis thing that\u2019s sort of like a bent minus sign before a variable means that it has to be false.\n\nSlide 4.2.13\nLet\u2019s start with a very simple problem, in which all of the input features are Boolean (we\u2019ll represent them with 0\u2019s and 1\u2019s) and the desired output is also Boolean.\n\nSlide 4.2.14\nOur hypothesis class will be conjunctions of the input features.\n\nLearning Conjunctions\n\u2022 Boolean features and output\n\u2022 H = conjunctions of features"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_016.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.15\nHere's an example data set. It is described using 4 features, which we'll call f\u2081, f\u2082, f\u2083, and f\u2084.\n\n[Table with features and outputs is shown, specifically:\nBoolean features and output:\nf\u2081 f\u2082 f\u2083 f\u2084 y\n0 0 0 0 0\n1 0 1 1 1\n0 1 0 0 1\n1 1 0 0 1\n0 0 1 1 0\n1 0 0 0 0]\n\nSlide 4.2.16\nSo, to understand the hypothesis space let's consider the hypothesis f\u2081 \u2227 f\u2083. It would be true on examples 2 and 3, and false on all the rest.\n\n[Repetition of the table from slide 4.2.15 is shown with markings in purple on examples 2 and 3.]\n\nSlide 4.2.17\nWe will measure the error of our hypothesis as the number of examples it gets wrong. It marks one negative example as positive, and two positives as negative. So, the error of the hypothesis f\u2081 \u2227 f\u2083 on this data set would be 3.\n\nSlide 4.2.18\nFinally, we'll measure the complexity of our hypothesis by the number of features mentioned in the conjunction. So the hypothesis f\u2081 \u2227 f\u2083 has complexity 2.\n\nFurthermore, let's assume that alpha is set to be very small, so that our primary goal is to minimize error, but, errors being equal, we'd like to have the smallest conjunction.\n\nSet alpha so we're looking for smallest h with 0 error."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_017.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.19\nThere's now an obvious way to proceed. We could do a general-purpose search in the space of hypotheses, looking for the one that minimizes the cost function. In this case, that might work out okay, since the problem is very small. But in general, we'll want to work in domains with many more features and much more complex hypothesis spaces, making general-purpose search infeasible.\n\nAlgorithm\n\u2022 Could search in hypothesis space using tools we've already studied\n\nSlide 4.2.20\nInstead, we\u2019ll be greedy! (When I talk about this and other algorithms being greedy, I\u2019ll mean something slightly different than what we meant when we talked about \u201cgreedy search\u201d. That was a particular instance of a more general idea of greedy algorithm. In greedy algorithms, in general, we build up a solution to a complex problem by adding the piece to our solution that looks like it will help the most, based on a partial solution we already have. This will not, typically, result in an optimal solution, but it usually works out reasonably well, and is the only plausible option in many cases (because trying out all possible solutions would be much too expensive)).\n\nAlgorithm\n\u2022 Could search in hypothesis space using tools we've already studied\n\u2022 Instead, be greedy!\n\nSlide 4.2.21\nWe'll start out with our hypothesis set to True (that's the empty conjunction). Usually, it will make some errors. Our goal will be to add as few elements to the conjunction as necessary to make no errors.\n\nAlgorithm\n\u2022 Could search in hypothesis space using tools we've already studied\n\u2022 Instead, be greedy!\n\u2022 Start with h = True\n\nSlide 4.2.22\nNotice that, because we've started with the hypothesis equal to True, all of our errors are on negative examples. So, one greedy strategy would be to add the feature into our conjunction that rules out as many negative examples as possible without ruling out any positive examples.\n\nAlgorithm\n\u2022 Could search in hypothesis space using tools we've already studied\n\u2022 Instead, be greedy!\n\u2022 Start with h = True\n\u2022 All errors are on negative examples\n\u2022 On each step, add conjunct that rules out most new negatives (without excluding positives)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_018.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.23\nHere is pseudo-code for our algorithm. We start with N equal to all the negative examples and the hypothesis equal to true. Then we loop, adding conjuncts that rule out negative examples, until N is empty.\n\nPseudo-Code\nN = negative examples in D\nh = True\nLoop until N is empty\n\nSlide 4.2.24\nInside the loop, we consider every feature that would not rule out any positive examples. We pick the one that rules out the most negative examples (here\u2019s where our greed kicks in), conjoin it to h, and remove the examples that it excludes from N.\n\nPseudo-Code\nN = negative examples in D\nh = True\nLoop until N is empty\n  For every feature fj that does not have value 0 on any positive examples:\n    nj = number of examples in N for which fj = 0\n    j* = fj for which nj is maximized\n    h = h ^ fj*\n    N = N \u2013 examples in N for which fj* = 0\n  If no such feature found, fail\n\nSlide 4.2.25\nLet's simulate the algorithm on our data. We start with N equal to x^1, x^3, and x^5, which are the negative examples. And h starts as True. We\u2019ll cover all the examples that the hypothesis makes true pink.\n\nSimulation\nf1 f2 f3 f4 f5 f6\ne1 0 1 0 0 1 0\ne2 1 0 1 0 0 0\ne3 0 1 0 0 0 0\ne4 1 1 0 1 0 0\ne5 1 0 1 0 0 1\ne6 0 1 1 1 0 0\n\nSlide 4.2.26\nSimulation\nN = {x^1, x^3, x^5 },   h = True\n\nNow, we consider all the features that would not exclude any positive examples. Those are features f5 and f4. f5 would exclude 1 negative example; f4 would exclude 2. So we pick f4."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_019.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.27\nNow we remove the examples from N that are ruled out by f\u2715 and add f\u2715 to h.\n\nSimulation\n\n\\[\n\\begin{array}{ccccccc}\n\\text{f}_{1} & \\text{f}_{2} & \\text{f}_{3} & \\text{f}_{4} & \\text{f}_{5} & \\text{y} & \\text{h} \\\\\n\\hline\n0 & 0 & 1 & 0 & 0 & \\text{True} & \\text{} \\\\\n0 & 1 & 0 & 1 & 0 & \\text{True} & \\text{} \\\\\n1 & 0 & 1 & 0 & 1 & \\text{False} & \\text{} \\\\\n1 & 1 & 0 & 1 & 1 & \\text{False} & \\text{} \\\\\n\\end{array}\n\\]\n\n\\text{N} = \\{x^1,x^3,x^5\\}, \\text{h = True}\n\nSlide 4.2.28\nNow, based on the new N, $\\text{n}_3 = 1$ and $\\text{n}_4 = 0$. So we pick f$_3$.\n\nSimulation\n\n\\[\n\\begin{array}{ccccccc}\n\\text{f}_{1} & \\text{f}_{2} & \\text{f}_{3} & \\text{f}_{4} & \\text{f}_{5} & \\text{y} & \\text{h} \\\\\n\\hline\n0 & 0 & 1 & 0 & 0 & \\text{True} & \\text{} \\\\\n0 & 1 & 0 & 1 & 0 & \\text{True} & \\text{} \\\\\n1 & 0 & 1 & 0 & 1 & \\text{False} & \\text{} \\\\\n1 & 1 & 0 & 1 & 1 & \\text{False} & \\text{} \\\\\n\\end{array}\n\\]\n\n\\text{N} = \\{x^3\\}, \\text{h} = \\text{f}_3\n\nSlide 4.2.29\nBecause f$_3$ rules out the last remaining negative example, we're done!\n\nSimulation\n\n\\[\n\\begin{array}{ccccccc}\n\\text{f}_{1} & \\text{f}_{2} & \\text{f}_{3} & \\text{f}_{4} & \\text{f}_{5} & \\text{y} & \\text{h} \\\\\n\\hline\n0 & 0 & 1 & 0 & 0 & \\text{True} & \\text{} \\\\\n0 & 1 & 0 & 1 & 0 & \\text{True} & \\text{} \\\\\n1 & 0 & 1 & 0 & 1 & \\text{False} & \\text{} \\\\\n1 & 1 & 0 & 1 & 1 & \\text{False} & \\text{} \\\\\n\\end{array}\n\\]\n\nSlide 4.2.30\nA Harder Problem\n\nWe made one negative into a positive\n\n\\[\n\\begin{array}{ccccccc}\n\\text{f}_{1} & \\text{f}_{2} & \\text{f}_{3} & \\text{f}_{4} & \\text{f}_{5} & \\text{y} & \\text{h} \\\\\n\\hline\n0 & 0 & 1 & 0 & 0 & \\text{True} & \\text{} \\\\\n1 & 0 & 1 & 0 & 1 & \\text{True} & \\text{} \\\\\n0 & 1 & 0 & 1 & 0 & \\text{True} & \\text{} \\\\\n1 & 1 & 0 & 1 & 1 & \\text{False} & \\text{} \\\\\n\\end{array}\n\\]\n\nSlide 4.2.30\nWhat if we have this data set? In which we just made one negative example into a positive?"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_020.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.31\nIf we simulate the algorithm, we will arrive at the situation in which we still have elements in N, but \nthere are no features left that do not exclude positive examples. So we're stuck.\n\nSlide 4.2.32\nWhat's going on? The problem is that this hypothesis class simply can't represent the data we have \nwith no errors. So now we have a choice: we can accept the hypothesis we've got, or we can \nincrease the size of the hypothesis class. In practice, which one you should do often depends on \nknowledge of the domain. But the fact is that pure conjunctions is a very limiting hypothesis class. \nSo let's try something a little fancier.\n\n 1 0 1 1 0 0 0\nf1 f2 f3 f4 f5 f6 f7\n0 1 1 0 0 0 1\n+ 1 1 1 1 0 1 0\n 0 0 0 0 0 0 0\n 0 1 0 0 1 0 1\n 1 0 1 0 0 0 1\n 1 1 1 1 1 0 1\n 0 0 1 0 1 0 1\n\nA Harder Problem\n\u2022 We made one negative into a \n  positive\n\u2022 Only usable feature is f3\n\u2022 Can't add any more features to h\n\u2022 We're stuck\n\nSlide 4.2.33\nYou can kind of think of a conjunction as narrowing down on a small part of the space. And if all \nthe positive examples can be corralled into one group this way, everything is fine. But for some \nconcepts, it might be necessary to have multiple groups.\n\nThis corresponds to a form of Boolean hypothesis that's fairly easy to think about: disjunctive \nnormal form. Disjunctive normal form is kind of like CNF, only this time it has the form (A and B \nand C) or (D and E and F)\n\nAs a hypothesis class, it's like finding multiple groups of positive examples within the negative \nspace.\n\nDisjunctive Normal Form\n\u2022 Like the opposite of conjunctive normal form (but, \n  for now, without negations of the atoms)\n   (A\u2227B\u2227C)\u2228(D\u2227A)\u2228E\n\u2022 Think of each disjunct as narrowing in on a subset \n  of the positive examples\n\nSlide 4.2.34\nSo, let's look at our harder data set and see if we can find a good hypothesis in DNF.\n\n 1 0 1 1 0 0 0\nf1 f2 f3 f4 f5 f6 f7\n0 1 1 0 0 0 1\n+ 1 1 1 1 0 1 0\n 0 0 0 0 0 0 0\n 0 1 0 0 1 0 1\n 1 0 1 0 0 0 1\n 1 1 1 1 1 0 1\n 0 0 1 0 1 0 1\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_021.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.35\nIt's pretty to see that one way to describe it is f\u2085 ^ f\u2084 ^ f\u2081 ^ f\u2082. Now let's look at an algorithm for finding it.\n\nSlide 4.2.36\nDisjunctive Normal Form\n\u2022 Like the opposite of conjunctive normal form (but, for now, without negations of the atoms)\n  (A \u2227 B \u2227 C) \u2228 (D \u2227 A) \u2228 E\n\u2022 Think of each disjunct as narrowing in on a subset of the positive examples\n\n  f\u2085  f\u2084  f\u2083  f\u2082  f\u2081\n  1   0   0   1   0\n  1   1   0   0   1\n  0   0   0   0   0\n  0   0   1   0   1\n  1   1   1   0   0  \n\n  (f\u2085 ^ f\u2084) \u2228 (f\u2082 ^ f\u2085)\n\nSlide 4.2.36\nWe'll let our hypothesis class be expressions in disjunctive normal form. What should our measure of complexity be? One reasonable choice is the total number of mentions of features (so f\u2085 ^ f\u2084 ^ f\u2081 ^ f\u2082 would have complexity 4).\n\nSlide 4.2.37\nLearning DNF\n\u2022 Let H be DNF expressions\n\u2022 C(h) : number of mentions of features\n  C((f\u2085 ^ f\u2084) \u2228 (f\u2082 ^ f\u2085)) = 4\n  \u2022 Really hard to search this space, so be greedy again!\n\nSlide 4.2.37\nNow, searching in this space would be much harder than searching in the space of conjunctions. So we'll use another greedy algorithm.\n\nSlide 4.2.38\nLearning DNF\n\u2022 Let H be DNF expressions\n\u2022 C(h) : number of mentions of features\n  C((f\u2085 ^ f\u2084) \u2228 (f\u2082 ^ f\u2085)) = 4\n  \u2022 Really hard to search this space, so be greedy again!\n\n\u2022 A conjunction covers an example if all of the features mentioned in the conjunction are true in the example.\n\nSlide 4.2.38\nWe'll say a conjunction covers an example if all of the features mentioned in the conjunction are true in the example."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_022.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 4.2.39**\nHere's pseudocode for the algorithm. It has two main loops. The inner loop constructs a conjunction (much like our previous algorithm). The outer loop constructs multiple conjunctions and disjoins them.\n\nThe idea is that each disjunct will cover or account for some subset of the positive examples. So in the outer loop, we make a conjunction that includes some positive examples and no negative examples, and add it to our hypothesis. We keep doing that until no more positive examples remain to be covered.\n\n**Algorithm**\nP = set of all positive examples\nh = False\nN = set of all negative examples\nloop until P is empty\n r = True\n loop until N is empty\n  If all features are in r, fail\n  Else, select a feature f\u1d62 to add to r\n  r = r ^ f\u1d62\n  N = = examples in r for which f\u1d62 = 0\n  h = h v r\n  Covered = examples in P covered by r\n  If Covered is empty, fail\n  Else P := P - Covered\nend\n\n**Slide 4.2.40**\nWe also haven\u2019t said how to pick which feature to add to r. It's trickier here, because we can't restrict our attention to features that allow us to cover all possible examples. Here's one heuristic for selecting which feature to add next.\n\n**Choosing a Feature**\nHeuristic:   v\u2c7c = \u03b7\u2c7c / max(\u03b7\u2c7c, 0.001)\n\n**Slide 4.2.41**\nLet \u03b7\u2c7c be n\u2c7c\u1d56 / n\u2c7c\u207f, where n\u2c7c\u1d56 is the total number of so-far uncovered positive examples that are covered by rule r ^ f\u2c7c and n\u2c7c\u207f is the total number of thus-far not-ruled-out negative examples that are covered by rule r ^ f\u2c7c. The intuition here is that we'd like to add features that cover a lot of positive examples and exclude a lot of negative examples, because that's our overall goal.\n\nThere's one additional problem about what to do when n\u2c7c\u207f is 0. In that case, this is a really good feature, because it covers positives and doesn't admit any negatives. We'll prefer features with zero in the denominator over all others; if we have multiple such features, we'll prefer ones with bigger numerator. To make this fit easily into our framework, if the denominator is zero, we just return as a score 1 million times the numerator.\n\n**Choosing a Feature**\nHeuristic:   v\u2c7c = \u03b7\u2c7c / max(\u03b7\u2c7c, 0.001)\n\n\u03b7\u2c7c = # not yet covered positive examples covered by r ^ f\u2c7c\nn\u2c7c = # not yet ruled out negative examples covered by r ^ f\u2c7c\nChoose feature with largest value of v\u2c7c\n\n**Slide 4.2.42**\nThen we can replace this line in the code with one that says: Select the feature f\u2c7c with the highest value of v\u2c7c to add to r. And now we have a whole algorithm."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_023.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.2.43\nLet's simulate it on our data set. Here's a trace of what happens. It's important to remember that when we compute xp, we consider only positive and negative examples not covered so far.\nWe start with h = false, and so our current hypothesis doesn't make any examples true.\n\nSimulation\n```\nh = False, P = {x\u00b2, x\u00b3, x\u2074, x\u2075}\n\n    x\u2081 x\u2082 x\u2083 x\u2084 x\u2085 y\n 1: 1  0  1  1  0  1  \n 2: 0  1  0  1  1  0\n 3: 1  0  1  0  1  0\n 4: 0  1  0  0  0  1\n 5: 0  0  1  0  1  1  \n 6: 1  0  0  1  0  1\n```\n\nSlide 4.2.44\nAfter the first iteration of the inner loop, our hypothesis covers the examples shown in pink. There's still another positive example to get.\n\nSlide 4.2.45\nAfter another iteration, we add a new rule, which covers the example shown in blue. And we're done.\n\nSimulation\n\n```\nh = False, P = {x\u00b2, x\u00b3, x\u2074, x\u2075}\n\n\u2751 r = True, N = {x\u00b9, x\u2076}\n\u2751 v\u2081=2/1, v\u2082=2/1, v\u2083=4/1, v\u2084=3/1, v\u2085=3/1\n\u2751 r = T, N = {x\u00b9}\n\u2751 v\u2081=2/0, v\u2082=2/1, v\u2083=3/0\n\u2751 r=f, N=\u00f8\n\u2751 h=f, N=\u00f8\n\u2751 h=f\u00b9\u039bf\u00b2\n\n\u2751 r= T, N = (x\u00b9, x\u2075)\n\u2751 v\u2081=1/1, v\u2082=1/1, v\u2083=1/1, v\u2084=0/1\n\u2751 r=f, N = (x\u00b3)\n\u2751 v\u2081=1/0, v\u2082=1/0, v\u2083=0/1\n\u2751 r=f, N= \u00f8\n\u2751 h = (f\u00b9\u028c f\u00b2)\u028c (f\u2075 v f\u00b9\u028c f\u00b2), P = {}\n```\n\n6.034 Notes: Section 4.3"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_024.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.1\nOnce our learning algorithm has generated a hypothesis, we'd like to be able to estimate how well it is going to work out in the \"real world\". What is the real world? Well, we can't expect the hypothesis to perform well in every possible world; we can expect it to perform well in the world that generated the data that was used to train it.\n\n---\n\nSlide 4.3.2\nJust by looking at the training data, we can't tell how well the hypothesis will work on unseen data. We have constructed the hypothesis so that it has zero error on the training data. But unless we're wildly optimistic, we can't expect to do so well on new data.\n\n---\n\nSlide 4.3.3\nThe only way to know how well the hypothesis will do on new data is to save some of the original data in a set called the \"validation set\" or the \"test set\". The data in the validation set are not used during training. Instead, they are \"held out\" until the hypothesis has been derived. Then, we can test the hypothesis on the validation set and see what percentage of the cases it gets wrong. This is called \"test set error\".\n\n---\n\nSlide 4.3.4\nSometimes you'd like to know how well a particular learning algorithm is working, rather than evaluating a single hypothesis it may have produced. Running the algorithm just once and evaluating its hypothesis only gives you one sample of its behavior, and it might be hard to tell how well it really works from just one sample.\n\nIf you have lots of data, you can divide it up into batches, and use half of your individual batches to train new hypotheses and the other half of them to evaluate the hypotheses. But you rarely have enough data to be able to do this.\n\n---\n\nFigures:\n\nHow Well Does it Work?\n\u2022 We'd like to know how well our h will perform on new data (drawn from the same distribution as the training data)\n\nHow Well Does it Work?\n\u2022 We'd like to know how well our h will perform on new data (drawn from the same distribution as the training data)\n\u2022 Performance of hypothesis on the training set is not indicative of its performance on new data\n\u2022 Save some data as a test set; performance on h is a reasonable estimate of performance on new data\n\nCross Validation\nTo evaluate performance of an algorithm as a whole (rather than a particular hypothesis):\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_025.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.5\nOne solution is called cross-validation. You divide your data up into some number (let\u2019s say 10 for now) of chunks. You start by feeding chunks 1 through 9 to your learner and evaluating the resulting hypothesis using chunk 10.\n\nNow, you feed all but chunk 9 into the learner and evaluate the resulting hypotheses on chunk 9. And so on.\n\n[Figure: Description appears in slide 4.3.6]\n\nCross Validation\nTo evaluate performance of an algorithm as a whole (rather than a particular hypothesis):\n\u2022 Divide data into k subsets\n\u2022 k different times\n  - train on k-1 of the subsets\n  - test on the held-out subset\n\u2022 return average test score over all k tests\n\nSlide 4.3.6\nAt the end of this process, you will have run the learning algorithm 10 times, getting 10 different hypotheses, and tested each of the hypotheses on a different test set. You can now average the error results from each of these trials to get an aggregate picture of how well your learning algorithm performs. (For those of you who might be statistically sophisticated, you have to be careful with these statistics because the samples are not independent.) Remember that this is a method for comparing algorithms on a data set, not for evaluating the quality of a particular hypothesis.\n\nSlide 4.3.7\nOne interesting thing to see is how well the learning process works as a function of how much data is available to it. I made a learning curve by secretly picking a target function to generate the data, drawing input points uniformly at random and generating their associated y using my secret function. First I generated 10 points and trained a hypothesis on them, then evaluated it. Then I did it for 20 points, and so on.\n\nThis is the learning curve for a fairly simple function: f_134 = f_9 V f_100 ^ f_775, in a domain with 1000 features. The x axis is the amount of training data. The y axis is the number of errors made by the resulting hypothesis, on a test set of data that is drawn uniformly at random, as a percentage of the total size of the test set.\n\nIt takes surprisingly few samples before we find the correct hypothesis. With 1000 features, there are 2^1000 possible examples. We are getting the correct hypothesis after seeing a tiny fraction of the whole space.\n\n[Graph: 1000 input dimensions; function = f_134 = f_9 V f_100 ^ f_775]\n\nLearning Curves\n\nSlide 4.3.8\nNow, let\u2019s look at a much more complex target function. It\u2019s (f_7 ^ f_8 ^ f_9 ^ f_14) V (f_6 ^ f_9 ^ f_6) V (f_6 ^ f_10 ^ f_11 ^ f_12) V (f_13 ^ f_14 ^ f_15 ^ f_16) V (f_17 ^ f_19 ^ f_19 ^ f_23) V (f_22 ^ f_22 ^ f_25 ^ f_33). It takes a lot more data to learn. Why is that?\n\n[Graph: Simple vs. Complex]\nNumber of training examples -\n\n1000 input dimensions; function = (f_7 ^ f_8 ^ f_9 ^ f_14) V (f_6 ^ f_9 ^ f_6) V (f_6 ^ f_10 ^ f_11 ^ f_12)\n(six disjuncts with 4 conjuncts each)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_026.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.9\nFirst, it\u2019s important to remember that, at every point in the graph, the learning algorithm has found a hypothesis that gets all of the training data correct.\n\nSlide 4.3.10\nWhen we have just a little bit of data, there are many possible functions that agree with it all. How does our algorithm choose which answer to give? It tries to give a simple one. So, we would say that the algorithm has a \u201cbias\u201d in favor of simple hypotheses.\n\nSlide 4.3.11\nSo, when the target hypothesis is simple, we discover it quickly (the other simple hypotheses are ruled out with just a little bit of data, because there are so few of them).\n\nSlide 4.3.12\nIf we thought our problems were going to have complex answers, then we might think about biasing our algorithm to prefer more complex answers. But then we start to see the wisdom of Ockham. There are lots more complex hypotheses than there are simple ones. So there will be huge numbers of complex hypotheses that agree with the data and no basis to choose among them, unless we know something about the domain that generated our data (maybe some features are thought, in advance, to be more likely to have an influence on the outcome, for example).\n\nSimple Gifts\n  \n- At every point in the graph, we\u2019ve found an h that gets the whole training set correct (call it apparently correct)\n- When input dimensionality is high and size of training set is small, there are lots of apparently correct hypotheses\n- Our algorithm tries to return (but doesn\u2019t always) the simplest apparently correct hypothesis\n- So, when the target hypothesis is simple, we discover it quickly (the other simple hypotheses are ruled out quickly because there are so few)\n- When the target hypothesis is complex, it\u2019s hard to rule out all of the (many) other competitors, so it takes more data to learn"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_027.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.13\nThere is, in fact, a theorem in machine learning called the \"No Free Lunch\" theorem. It basically\nstates that, unless you know something about the process that generated your data, any hypothesis\nthat agrees with all the data you've seen so far is as good a guess as any other one.\n\nThe picture will get a bit more complicated when we look at data that are noisy.\n\nNo Free Lunch\n\u25b6 Unless you know something about the distribution\n   of problems your learning algorithm will encounter,\n   any hypothesis that agrees with all your data\n   is as good as any other.\n\n\u25b6 You can't learn anything unless you already know\n   something.\n\nSlide 4.3.14\nIt is actually very rare to encounter a machine learning problem in which there is a deterministic\nfunction that accounts for y's dependence on x in all cases. It is much more typical to imagine that\nthe process that generates the x's and y's is probabilistic, possibly corrupting measurements of the\nfeatures, or simply sometimes making different decisions in the same circumstances.\n\nNoisy Data\n\u2022 Have to accept non-zero error on training data\n\nSlide 4.3.15\nWe can easily handle noise in the framework we have introduced. We left the door open for it, so to\nspeak, in our formulation of the problem we were trying to solve. We want to find a hypothesis h\nthat minimizes error plus complexity. When there's noise, we'll have to accept that our hypothesis\nwill probably have non-zero error on the training set; and we'll have to be very careful about how we\nweight these two criteria, as we will see.\n\nNoisy Data\n\u2022 Have to accept non-zero error on training data\n\u2022 Weaken DNF learning algorithm\n\u2022 Don't require the hypothesis to cover all positive\n   examples\n\u2022 Don't require each rule to exclude all negative\n   examples\n\nSlide 4.3.16\nWe can use our DNF learning algorithm almost unchanged. But, instead of requiring each rule to\nexclude all negative examples, and requiring the whole hypothesis to cover all positive examples,\nwe will only require them to cover some percentage of them. Here is the pseudocode, with the\nchanges highlighted in blue.\n\nPseudo Code: Noisy DNF Learning\np \u21d2 the set of positive examples\nnp \u21d2 False\nne \u21d2 epsilon * number of examples in P\nm \u21d2 epsilon * number of examples in N\nLoop until p has fewer than m elements\n   k \u21d2 True\n   N \u21d2 the set of negative examples\n   Repeat until N has fewer than n elements\n      Select a feature f_i to add to k\n      If p = 1\n         k \u21d2 f_i\n      R \u21d2 example in N for which f_i = 0\n      n \u21d2 R \u2013 elements in N covered by f\n\n allow epsilon\n percentage error"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_028.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.17\nWe also have to add further conditions for stopping both loops. If there is no feature that can be\nadded that reduces the size of N, then the inner loop must terminate. If the inner loop cannot\ngenerate a rule that reduces the size of P, then the outer loop must terminate and fail.\n\n(pseudo code slide)\nPseudo Code: Noisy DNF Learning\n- P = the set of positive examples\n- N = false\n- n = epsilon * number of examples in P\n- m = epsilon * number of examples in N\nLoop until N has fewer than m elements or not progressing.\n  M := true\n  N = the set of negative examples\n  Repeat until N has fewer than n elements or not progress.\n    Select a feature f, to add to I:\n    If r = epsilon,\n      N := examples in N for which f = 0\n      P := P - elements in P covered by I\n\nSlide 4.3.18\nNow we have a new parameter, epsilon, which is the percentage of examples we're allowed to get wrong. (Well, we'll actually get more wrong, because each disjunct is allowed to get epsilon percent of the negatives wrong).\n\nSlide 4.3.19\nEpsilon is our Delta\n- Parameter epsilon is the percentage error allowed\n- The higher the epsilon, the simpler and more error-prone the hypothesis\n\nSlide 4.3.20\nEpsilon is our Delta\n- Parameter epsilon is the percentage error allowed\n- The higher epsilon, the simpler and more error-prone the hypothesis\n- If epsilon is small and the data is noisy, the algorithm may fail to find an acceptable hypothesis\n\nSlide 4.3.20\nIf epsilon is small and the data is noisy, the algorithm may fail to find an acceptable hypothesis."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_029.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.21\nLet's see what happens in a moderately noisy domain. This curve shows the problem of learning our easy DNF function in a domain with 100 features, on a training set of size 100. The data has 10 percent noise, which means that, in 10 percent of the cases, we expect the output set to be the opposite of the one specified by the target function. The x axis is epsilon, ranging from 0 to 1. At setting 0, it is the same as our original algorithm. At setting 1, it's willing to make an arbitrary number of errors, and will always return the hypothesis False. The y axis is percentage error on a freshly drawn set of testing data. Note that, because there is 10 percent error in the data (training as well as testing), our very best hope is to generate a hypothesis that has 10 percent error on average on testing data.  \n\nOverfitting Curve\n200 input dimensions; function = f22/55 \u2228 f99/134\n% error on training set = 0\n% error on test set. 0.2 % error on test set. 0.4 % error on test set. 0.6 % error on test set. 0.8 % error on test set. 1.0\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0\n1.0 - X\n0 = X\nX\n- x X\n-- X\n0.6\nE=0.01 Spring 04 -27a\n\nSlide 4.3.22\nWe get the best performance for a value of epsilon that's between 0 and 1. What's the matter with setting it to some value near 0? We run into the problem of overfitting. In overfitting, we work very hard to reduce the error of our hypothesis on the training set, but we may have spent a lot of effort modeling the noise in the data and, often, had poor only the test set. That's what's happening here. It is said, in this case, that our algorithm has high variance. Another way to think about the problem is to see that if we get another noisy data set generated by the same target function, we are likely to get a wildly different answer. By tuning epsilon up a bit, we generate simpler hypotheses that are not so susceptible to variations in the data set, and so can get better generalization performance.\n\nSlide 4.3.23\nOf course, turning epsilon up too high will keep us from building a hypothesis of sufficient complexity. Then we'd do poorly because we are unable to even represent the right answer. It is said, in this case, that our algorithm has high bias.\n\nOverfitting Curve\n200 input dimensions; function = f22/55 \u2228 f99/134\n% error on training set = 0\n0 test set. 0.2 % error on test set. 0.4 % error on test set. 0.6 % error on test set. 0.8\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0\n0.4\n0.6\n0.8\n0\nhigh variance\n\nSlide 4.3.24\nIt's also instructive to see how the complexity of the resulting hypothesis varies as a function of epsilon. When epsilon is 0, we are asking for a hypothesis with zero error on the training set. We are able to find one, but it's very complex (31 literals). This is clearly a high variance situation; that hypothesis is completely influenced by the particular training set and would change radically on a newly drawn training set (and therefore, have high error on a testing set, as we saw on the previous slide).\n\nHypothesis Complexity\n200 input dimensions; function = f22/55 \u2228 f99/134\nc(H)\n35\n30\n25\n20\n15\n10\n5\n0\n0\n1.0\n0.2\nhigh variance\n0.4\n0.6\n0.8\n-8 (2:0x00 Spring 04 - 274\n \n1000\nhigh bias"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_030.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.25  \nAs epsilon increases, we rapidly go down to a complexity of 1, which is incapable of representing  \nthe target hypothesis. The low-point in the error curve was at epsilon = 0.2, which has a complexity  \nof 4 here. And it\u2019s no coincidence that the target concept also has a complexity of 4.\n\n(Chart: Hypothesis Complexity, 200 input dimensions; function = f22+f55 \u221a f99\u221a f34, graph showing high variance and high bias regions)\n\nSlide 4.3.26\nThis problem is called the bias/variance trade-off. Because this is such an important idea in machine  \nlearning, let's look at it one more time in the context of our old pictures of dots on the plane.\n\n(Slide 4.3.26: Title - Bias vs Variance, 20 black dots and 20 red dots scattered on a plane with no separation line)\n\nSlide 4.3.27\nHere's some noisy data.  \n\nIf we separate it with a line, we have low complexity, but lots of error.\n\n(Slide 4.3.27: Showing a plane with black and red dots separated by a simple straight line)\n\nSlide 4.3.28\nIf we separate it with a squiggly snake, we can reduce the error but add a lot of complexity.\n\n(Slide 4.3.28: Showing a plane with black and red dots separated by a curvy line, a squiggly snake)\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_031.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.29\nSo, what, exactly, is so bad about complexity? Well, let's draw a bunch more points from the same underlying distribution and add them to our data set.\n\nSlide 4.3.30\nWe wouldn't need to change our line hypothesis very much to do our best to accommodate them.\n\nSlide 4.3.31\nBut we'd have to change the snake hypothesis radically. This is what it means to have high variance.\n\nThe line, although it was making mistakes on the original data set, performs better on unseen data.\n\nSlide 4.3.32\nIn practice, it is common to allow hypotheses of increased complexity as the amount of available training data increases. So, in this situation, we might feel justified in choosing a moderately complex hypothesis."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_032.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.33\nSo, given a data set, how should we choose epsilon? This is where cross-validation can come in handy. You select a number of different values of epsilon, and use cross-validation to estimate the error when using that epsilon. Then, you pick the epsilon value with the least estimated error, and train your learning algorithm with all the available data using that value of epsilon. The resulting hypothesis is probably the best you can do (given what you know).\n\nSlide 4.3.34\nSo, is this algorithm actually good for anything? Yes. There are lots of domains that can be encoded as vectors of binary features with a binary classification. Here are some examples.\n\nPicking Epsilon\n- Pick epsilon using cross validation\n- Try multiple different values of epsilon\n- See which gives the lowest cross-validation error\n- Use that value of epsilon to learn a hypothesis from the whole training set\n- Return that hypothesis as your best answer\n\nSlide 4.3.35\nDomains\n\u2022 Congressional voting: given a congressperson\u2019s voting record, where the features are the individual\u2019s votes on various bills, can you predict whether they are a republican or democrat?\n\nDomains\n\u2022 Congressional voting: given a congressperson\u2019s voting record (list of 1s and 0s), predict party\n\nSlide 4.3.36\nDomains\n\u2022 Congressional voting: given a congressperson\u2019s voting record (list of 1s and 0s), predict party\n\u2022 Gene splice: predict the beginning of a coding section of the genome; input is vector of elements chosen from the set {ACGT}; encode each element with two bits (or possibly with 4)\n\nSlide 4.3.36\nGene splicing: given a sequence of bases (A, C, G, or T), predict whether the mid-point is a splice junction between a coding segment of DNA and a non-coding segment. In order to apply our algorithms to this problem, we would have to change the representation somewhat. It would be easy to take each base and represent it as two binary values: A is 00, C is 01, G is 10 and T is 11. Even though it seems inefficient, for learning purposes, it is often better to represent discrete elements using a unary code; so A would be 0001, C would be 0010, G would be 0100 and T would be 1000."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_033.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.37\nSpam filtering: is this email message spam? Represent a document using a really big feature space,\nwith one feature for each possible word in the dictionary (leaving out some very common ones, like\n\"a\" and \"the\"). The feature for a word has value 1 if that word exists in the document and 0\notherwise.\n\n* Congressional voting: given a congressperson\u2019s voting record (list of 1s and 0s), predict party  \n* Gene splice: predict the beginning of a coding section of the genome; input is vector of elements  \n  chosen from the set {ACGT}; encode each element with one bit (or possibly with 4)  \n* Spam filtering: encode every message as a vector of features, one per word; a feature is on if that  \n  word occurs in the message; predict whether or not the message is spam  \n\nSlide 4.3.38\nMarketing: is this grocery-store customer likely to buy beer? Represent a person's buying history\nusing a big feature space, with one feature for each product in the supermarket. The feature for a\nproduct has value 1 if the person has ever bought that product and 0 otherwise.  \n \nDomains\n\n* Congressional voting: given a congressperson\u2019s voting record (list of 1s and 0s), predict party  \n* Gene splice: predict the beginning of a coding section of the genome; input is vector of elements  \n  chosen from the set {ACGT}; encode each element with one bit (or possibly with 4)  \n* Spam filtering: encode every message as a vector of features, one per word; a feature is on if that  \n  word occurs in the message; predict whether or not the message is spam  \n* Marketing: predict whether a person will buy beer based on previous purchases; encode buying habits\n  with a feature for all products, set to 1 if previously purchased  \n\nSlide 4.3.39\nJust for fun, we ran the DNF learning algorithm on the congressional voting data. A lot of the\nrecords had missing data (those congress folks are always out in the hallways talking instead of in\ntheir voting); we just deleted those records (though in a machine learning class we would study\nbetter ways of handling missing data items). There were 232 training examples, each of which had\n15 features. The features were the individuals\u2019 votes on these bills (unfortunately, we don\u2019t know any\ndetails about the bills, but the names are evocative).\n\nCongressional Voting\n\n0. handicapped-infants\n1. water-project-cost-sharing\n2. adoption-of-the-budget-resolution\n3. physician-fee-freeze\n4. el-salvador-aid\n5. religious-groups-in-schools\n6. anti-satellite-test-ban\n7. aid-to-nicaraguan-contras\n8. mx-missile\n9. immigration\n10. synfuels-corporation-cutback\n11. education-spending\n12. superfund-right-to-sue\n13. duty-free-exports\n14. export-administration-act-south-africa\n\n232 data points\n\nSlide 4.3.40\nWe are trying to predict whether a person is a Republican, based on their voting history during part\nof 1984. Here is a plot of the training set error as a function of epsilon. What I should really do is\ncross-validation, but instead I just ran the algorithm on the whole data set and I'm plotting the error\non the training set.\n\nSomething sort of weird is going on. Training set error should get smaller as epsilon decreases. Why\ndo we have it going up for epsilon equal zero? It's because of the greediness of our algorithm. We're\nnot actually getting the best hypothesis in that case.\n\n[Plot graph image]\n\nCongressional Voting: Republican?\n\n% error on training set\n0.04 0.3 0.5 0.6 0.8\n\n(training set)\n\nepsilon"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch4_learnintro\\page_034.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 4.3.41\nHere\u2019s the hypothesis we get at this point. It\u2019s kind of complicated (but mentions a lot of Republican things, like aid to the Nicaraguan contras and the mx-missile).\n\n---[Chart Image Described]---\n\"Congressional Voting: Republican?\"\nY-axis: % error training set\nX-axis: epsilon\n[Chart with several lines indicating error rates for various hypotheses]\n- (water-project-cost-sharing ^ duty-free exports) v\n- (budget ^ synfuels-cutback) v\n- (aid-to-nicaraguan-contras ^ budget) v\n- (synfuels-cutback ^ mx-missile)\n\nSlide 4.3.42\nHere's a simpler hypothesis (with slightly worse error).\n\n---[Chart Image Described]---\n\"Congressional Voting: Republican?\"\nY-axis: % error training set\nX-axis: epsilon\n[Chart with a red line indicating error rate]\n- adopt-budget v mx-missile\n\nSlide 4.3.43\nAnd here's the best hypothesis of complexity 1 for predicting whether someone is a Republican: did they vote \"yes\" for the MX missile?\n\nAnd, just because you\u2019re all too young to remember this, a little history. The MX missile, also called the \u201cPeacekeeper\u201d was our last big nuclear missile project. It\u2019s 71 feet long, with 10 warheads, and launched from trains. It was politically very contentious. Congress killed it at least twice before it was finally approved.\n\n---[Chart Image Not Described Due to Copyright]---\n---[Text: Image removed due to copyright restrictions.]---\n\n\"Congressional Voting: Republican?\"\nY-axis: % error training set\nX-axis: epsilon\n[Empty area where image would be]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n6.034 Notes: Section 5.1\n\nSlide 5.1.1\nThe learning algorithm for DNF that we saw last time is a bit complicated and can be inefficient. \nIt\u2019s also not clear that we\u2019re making good decisions about which attributes to add to a rule, especially when there\u2019s noise.\n\nSo now we\u2019re going to look at an algorithm for learning decision trees. We\u2019ll be changing our hypothesis class (sort of), our bias, and our algorithm. We\u2019ll continue to assume, for now, that the input features and the output class are boolean. We\u2019ll see later that this algorithm can be applied much more broadly.\n\n* Decision Trees\n  - DNF learning algorithm is a bit cumbersome and inefficient. Also, the exact effect of the heuristic is unclear.\n  - Still assume binary inputs and output, but much more broadly applicable.\n\nSlide 5.1.2\n\nHypothesis Class\n- Internal nodes: feature name\n- One child for each value of the feature\n- Leaf nodes: output\n\nOur hypothesis class is going to be decision trees. A decision tree is a tree (big surprise!). At each internal (non-leaf) node, there is the name of a feature. There are two arcs coming out of each node, labeled 0 and 1, standing for the two possible values that the feature can take on.\n\nLeaf nodes are labeled with decisions, or outputs. Since our y\u2019s are Boolean, the leaves are labeled with 0 or 1.\n\nSlide 5.1.3\nTrees represent Boolean functions from x\u2019s (vectors of feature values) to Booleans. To compute the output for a given input vector x, we start at the root of the tree. We look at the feature there, let\u2019s say it's feature j, and then look to see what the value of x_j is. Then we go down the arc corresponding to that value. If we arrive at another internal node, we look up that feature value, follow the correct arc, and so on. When we arrive at a leaf node, we take the label we find there and generate that as an output.\n\nSo, in this example, input [0 1 1 0] would generate an output of 0 (because the third element of the input has value 1 and the first has value 0, which takes to a leaf labeled 0).\n\nDiagram of a Hypothesis Class:\nNode f_1 has children leading to:\n- Node f_2 has children leading to leaves 0 and 1.\n- Node f_3 has children leading to leaves 1 and 0.\n\nh(0, 1, 1, 0) = 0"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Hypothesis Class**\n\nh = ((\u00acf\u2081 \u2227 f\u2082) \u2228 (\u00acf\u2081 \u2227 \u00acf\u2082))\n\n---\n\n**Slide 5.1.4**\n\nDecision trees are a way of writing down Boolean expressions. To convert a tree into an expression, you can make each branch of the tree with a 1 at the leaf node into a conjunction, describing the condition that led to both of the input in order for you to have gotten to that leaf of the tree. Then, you take this collection of expressions (one for each leaf) and disjoin them.\n\nSo, our example tree describes the boolean function not f\u2081 and f\u2082 or f\u2081 and not f\u2082.\n\n---\n\n**Slide 5.1.5**\n\nIf we allow negations of primitive features in DNF, then both DNF and decision trees are capable of representing any boolean function. Why, then, should we bother to change representations in this way? The answer is that we're going to define our bias in a way that is natural for trees, and use an algorithm that is tailored to learning functions in the tree representation.\n\n---\n\n**Tree Bias**\n\n- Both decision trees and DNF with negation can represent any Boolean function. So why bother with trees?\n- Because we have a nice algorithm for growing trees that is consistent with a bias for simple trees (few nodes)\n- Too hard to find the smallest good tree, so we\u2019ll be greedy again\n- Have to watch out for overfitting\n\n---\n\n**Slide 5.1.6**\n\nApplication of Ockham\u2019s razor will lead us to prefer trees that are small, measured by the number of nodes. As before, it will be computationally intractable to find the minimum-sized tree that satisfies our error criteria. So, as before, we will be greedy, growing the tree in a way that seems like it will make it best on each step.\n\nWe\u2019ll consider a couple of methods for making sure that our resulting trees are not too large, so we can guard against overfitting.\n\n---\n\n**Slide 5.1.7**\n\nIt's interesting to see that a bias for small trees is different from a bias for small DNF expressions. Consider this tree. It corresponds to a very complex function in DNF.\n\n---\n\n![Diagram of a Decision Tree labeled \"Trees vs DNF\"]\n\n**Trees vs DNF**\n\n(\u00acF \u2227 \u00acH) \u2228 (\u00acF \u2227 \u00acA) \u2228 (\u00acF \u2227 \u00acG \u2227 K) \u2228 (F \u2227 G)\n\n---"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.1.8\nBut here's a case with a very simple DNF expression that requires a large tree to represent it. There's no particular reason to prefer trees over DNF or DNF over trees as a hypothesis class. But the tree-growing algorithm is simple and elegant, so we'll study it.\n\n(F \u2227 G) \u2228 (H \u2227 J)\n\nTrees vs DNF\n\n 0 |   0 | 1\n      |        0\n     --- G 1\n     |    \\\\ \n  1  |       \n    F |   \n     0 |      H  --- 0\n          0 | 1   \\\\ \n            |        J  1\n                   0   | 1\n                           \n\nSlide 5.1.9\nThe idea of learning decision trees and the algorithm for doing so was, interestingly, developed independently by researchers in statistics and researchers in AI at about the same time around 1980.\n\nAlgorithm\n\u2022 Developed in parallel in AI by Quinlan and in statistics by Breiman, Friedman, Olsen and Stone\n\nSlide 5.1.10\nWe will build the tree from the top down. Here is pseudocode for the algorithm. It will take as input a data set, and return a tree.\n\nAlgorithm\n\u2022 Developed in parallel in AI by Quinlan and in statistics by Breiman, Friedman, Olsen and Stone\nBuildTree (Data)\n\nSlide 5.1.11\nWe first test to see if all the data elements have the same y value. If so, we simply make a leaf node with that y value and we're done. This is the base case of our recursive algorithm.\n\nAlgorithm\n\u2022 Developed in parallel in AI by Quinlan and in statistics by Breiman, Friedman, Olsen and Stone\nBuildTree (Data)\nif all elements of Data have the same y value, then\n  MakeLeafNode(y)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nAlgorithm\n\u2022 Developed in parallel in AI by Quinlan and in \n  statistics by Breiman, Friedman, Olsen and Stone\n\nBuildTree(Data)  \n  if all elements of Data have the same y value, then \n    MakeLeafNode(y)\n  Else\n    feature = PickBestFeature(Data)\n    MakeInternalNode(feature,  \n      BuildTree(SelectFeature(Data, feature)),  \n      BuildTree(SelectFeature(Data, feature)))\n\nSlide 5.1.12\nIf we have a mixture of different y values, we choose a feature to use to make a new internal node. Then we divide the data into two sets, those for which the value of the feature is 0 and those for which the value is 1. Finally, we call the algorithm recursively on each of these data sets. We use the selected feature and the two recursively created subtrees to build our new internal node.\n\nSlide 5.1.13\nSo, how should we choose a feature to split the data? Our goal, in building this tree, is to separate the negative instances from the positive instances with the fewest possible tests. So, for instance, if there's a feature we could pick that has value of 0 for all the positive instances and 1 for all the negative instances, then we'd be delighted, because that would be the last split we\u2019d have to make. On the other hand, a feature that divides the data into two groups that have the same proportion of positive and negative instances as we started with wouldn\u2019t seem to have helped much.\n\nLet's Split\n\nD: 9 positive 10 negative\n \n             f1            \n   5 positive    10 negative\n\n 4 positive     6 negative\n\n   f2\n\n6 positive     10 negative\n   \n 3 positive     10 negative\n\nSlide 5.1.14\nIn this example, it looks like the split based on f2 will be more helpful. To formalize that intuition, we need to develop a measure of the degree of uniformity of the subsets of the data we'd get by splitting on a feature.\n\nSlide 5.1.15\nWe'll start by looking at a standard measure of disorder, used in physics and information theory, called entropy. We\u2019ll just consider it in the binary case, for now. Let p be the proportion of positive examples in a data set (that is, the number of positive examples divided by the total number of examples). Then the entropy of that data set is:\n\n\\[- p \\log_2 p - (1-p) \\log_2 (1-p)\\]\n\nEntropy\n\np : proportion of positive examples in a data set\n\nH = -p \\log_2 p - (1-p) \\log_2 (1-p)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nEntropy\n\np: proportion of positive examples in a data set\n\nH = -p log2 p - (1 - p) log2 (1 - p)\n\nh\n\n1.0\n\n0.5\n\n0\n\nP\n\nSlide 5.1.16\n\nHere's a plot of the entropy as a function of p. When p is 0 or 1, then the entropy is 0. We have to be a little bit careful here. When p is 1, then 1 log 1 is clearly 0. But what about when p is 0? Log 0 is negative infinity. But 0 wins the battle, so 0 log 0 is also 0.\n\nSo, when all the elements of the set have the same value, either 0 or 1, then the entropy is 0. There is no disorder (unlike in my office!).\n\nThe entropy function is maximized when p = 0.5. When p is one half, the set is as disordered as it can be. There's no basis for guessing what the answer might be.\n\nSlide 5.1.17\n\nWhen we split the data on feature j, we get two data sets. We'll call the set of examples for which feature j has value 1 D^j_1 and those for which j has value 0 D^j_1.\n\nLet's Split\nD: 9 positive\n10 negative\nf5\nf7\n\n5 positive\n5 negative\n\nH=.99\n\n9 positive\n3 negative\n\nH=.78\n\nSlide 5.1.18\n\nWe can compute the entropy for each of these subsets. For some crazy reason, people usually use the letter H to stand for entropy. We'll follow suit.\n\nLet's Split\nD: 9 positive\n10 negative\n\n4 positive\n6 negative\n\nH=.97\n\n6 positive\n4 negative\n\nH=.97\n\n3 positive\n6 negative\n\nH=.92\n\n4 positive\n1 negative\n\nH=.78\n\nSlide 5.1.19\n\nNow, we have to figure out how to combine these two entropy values into a measure of how good splitting on feature j is. We could just add them together, or average them. But what if have this situation, in which there\u2019s one positive example in one data set and 100 each of positive and negative examples in the other? It doesn\u2019t really seem like we\u2019ve done much good with this split, but if we averaged the entropies, we\u2019d get a value of 1/4.\n\nSo, to keep things fair, we'll use a weighted average to combine the entropies of the two sets. Let pj be the proportion of examples in the base set D for which feature j has value 1. We'll compute a weighted average entropy for splitting on feature j as AE(j) = pj H(D^j_1) + (1 - pj)H(D^j_0).\n\nLet's Split\n\n% of obs with f =1\n\nSubset of D with f =1\n\n4 positive\n\n6 negative\n\nH=.97\n\n3 positive\n\n6 negative\n\nH=.78"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_006.jpeg",
    "text": "**Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.**\n\n---\n\n**Let\u2019s Split**\n\n- Different scenarios of positive and negative outcomes are shown branching from a decision node:\n  - 5 positive, 9 negative with H = .97\n  - 4 positive, 6 negative with H = .97\n  - 6 positive, 4 negative with H = .72\n  - 3 positive, 10 negative with H = .78\n\n- Average Entropy:\n  - AE = (9/19).99 + (10/19).97 = .98\n  - AE = (6/19)0 + (13/19).78 = .53\n\n**Slide 5.1.20**\n\n\"So, in this example, we can see that the split on f\u2087 is much more useful, as reflected in the average entropy of its children.\"\n\n**Slide 5.1.21**\n\n\"Going back to our algorithm, then, we\u2019ll pick the feature at every step that minimizes the weighted average entropy of the children.\"\n\n**Slide 5.1.22**\n\n\"As usual, when there is noise in the data, it's easy to overfit. We could conceivably grow the tree down to the point where there's a single data point in each leaf node. (Or maybe not: in fact, if have two data points with the same x values but different y values, our current algorithm will never terminate, which is certainly a problem.) So, at the very least, we have to include a test to be sure that there's a split that makes both of the data subsets non-empty. If there is not, we have no choice but to stop and make a leaf node.\"\n\n**Slide 5.1.23**\n\n\"What should we do if we have to stop and make a leaf node when the data points at that node have different y values? Choosing the majority y value is the best strategy. If there are equal numbers of positive and negative points here, then you can just pick y arbitrarily.\"\n\n**Algorithm**\n\n- Developed in parallel in AI by Quinlan and in statistics by Breiman, Friedman, Olshen and Stone\n\n**BuildTree (Data)**\n\n- if all elements of Data have the same y value, then\n  - MakeLeafNode(y)\n- else\n  - Feature = PickBestFeature(Data)\n  - MakeInternalNode(Feature,\n    - BuildTree(SelectTrue(Data, feature)),\n    - BuildTree(SelectFalse(Data, feature)))\n- Best feature minimizes average entropy of data in the children\n\n**Stopping**\n\n- Image removed due to copyright restrictions.\n\n- Stop recursion if data contains only multiple instances of the same x with different y values\n- Make leaf node with output equal to the y value that occurs in the majority of the cases in the data\n\n- Image removed due to copyright restrictions."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_007.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.1.24\nBut growing the tree as far down as we can will often result in overfitting.\n\nSlide 5.1.25\nThe simplest solution is to change the test on the base case to be a threshold on the entropy. If the entropy is below some value epsilon, we decide that this leaf is close enough to pure.\n\nSlide 5.1.26\nAnother simple solution is to have a threshold on the size of your leaves; if the data set at some leaf has fewer than that number of elements, then don\u2019t split it further.\n\nSlide 5.1.27\nAnother possible method is to only split if the split represents a real improvement. We can compare the entropy at the current node to the average entropy for the best attribute. If the entropy is not significantly decreased, then we could just give up."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_008.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.1.28\nSimulation\n\n- H(D) = .92\n- AE\u2081 = .92, AE\u2082 = .92,\n  AE\u2085 = .81, AE\u2086 = 1\n\n  Let's see how our tree-learning algorithm behaves on the example we used to demonstrate the DNF-learning algorithm. Our data set has a starting entropy of .92. Then, we can compute, for each feature, what the average entropy of the children would be if we were to split on that feature. \n\n  In this case, the best feature to split on is f\u2085.\n\nSlide 5.1.29\nSo, if we make that split, we have these two data sets. The one on the left has only a single output (in fact, only a single point).\n\n111 0\n010 0\n101 0\n\n110 1\n011 1\n111 1\n001 1\n\nSlide 5.1.30\nSo we make it into a leaf with output 0 and consider splitting the data set in the right child.\n\n    f\u2085\n  /   \\\n0      1\n|       \n0\n\n110 1\n011 1\n111 1\n001 1\n\nSlide 5.1.31\nThe average entropies of the possible splits are shown here. Features 1 and 2 are equally useful, and feature 4 is basically no help at all.\n\n- AE\u2081 = .55,\n- AE\u2082 = .55,\n- AE\u2084 = .95"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSimulation\n\nSlide 5.1.32\nSo, we decide, arbitrarily, to split on feature 1, yielding these sub-problems. All of the examples in the right-hand child have the same output.\n\n[Decision Tree]\n\nRight-hand subtree table:\nx f1 f2 f3 y\n1 1 1 0 1\n1 1 0 1 1\n1 1 1 1 1\n\nLeft-hand subtree table:\nx f1 f2 f3 y\n0 1 0 1 0\n0 1 1 0 0\n0 0 0 1 1\n\nSlide 5.1.33\nSo we turn it into a leaf with output 1.\nIn the left child, feature 2 will be the most useful.\n\nSimulation\n\n[Decision Tree]\n\nRight-hand subtree:\nx f1 f2 f3 y\n1 1 1 0 1\n1 1 0 1 1\n1 1 1 1 1\n\nLeft-hand subtree:\nx f1 f2 f3 y\n0 1 0 1 0\n0 1 1 0 0\n0 0 0 1 1\n\nSlide 5.1.34\nSo we split on it, and now both children are homogeneous (all data points have the same output).\n\nSimulation\n\n[Decision Tree]\n\nRight-hand subtree:\nx f1 f2 f3 y\n1 1 1 0 1\n1 1 0 1 1\n1 1 1 1 1\n\nLeft-hand subtree:\nx f1 f2 f3 y\n0 1 0 1 0\n0 1 1 0 0\n0 0 0 1 1\n\nSlide 5.1.35\nSo we make the leaves and we're done!\n\n[Final Decision Tree]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 5.1.36**\n_One class of functions that often haunts us in machine learning are those that are based on the \u201cexclusive OR\u201d. Exclusive OR is the two-input boolean function that has value 1 if one input has value 1 and the other has value 0. If both inputs are 0 or both are 1, then the output is 0. This function is hard to deal with, because neither input feature is, by itself, detectably related to the output. So, local methods that try to add one feature at a time can be easily misled by xor._\n\n**Slide 5.1.37**\n_Let\u2019s look at a somewhat tricky data set. The data set has entropy .92. Furthermore, no matter what attribute we split on, the average entropy is .92. If we were using the stopping criterion that says we should stop when there is no split that improves the average entropy, we\u2019d stop now._  \n\n**Slide 5.1.38**\n_But let\u2019s go ahead and split on feature 1. Now, if we look at the left-hand data set, we\u2019ll see that feature 2 will have an average entropy of 0._  \n\n**Slide 5.1.39**\n_So we split on it, and get homogenous children,_\n\n[Top left image] \"Exclusive OR\"  \n(A \u2192B) \u2228 (B\u0304 \u02c4 A)\n\n[Middle right image] Exclusive OR  \n_AE1 = .92, AE2 = .92,\nAE3 = .92, AE4 = .92,\nAE5 = .92, AE6 = .92_  \n\n[Bottom images]\n_Left_:\n f1  \n0 1\nf2 f3 f4 y  \n0 0 0 1 0\n1 1 0 0 1\n1 0 1 1 0\n0 0 1 1 0\n1 1 0 0 1\n0 1 0 1 0\n0 0 1 1 0\n0 1 0 1 0\n1 0 0 0 1\n\n_Right bottom_:\n f1\n0 1\n0 1 f3 f4 y  101010\n1  0  0 0 1 1 0\n1 f2 0 f3 1 f4 0 y 1\n0  0 1  0\n01 1 00\n \n [bottom right image] Exclusive OR  \n f1  0 1\nf2 f3  f4 y  101110 1 0 1 0001 1 1 00 0 f3 f4  y 1 0010 1 10 f2 01 f3 0 f4 0 y 10010 0 1 01\nf2 f3 f4 y  1 0 1 1 0 1 0 1 1000\n0\n1.36 - Spring 03 - 34\n1.37 - Spring 03 - 35\n1.38 - Spring 03 - 36\n1.39 - Spring 03 - 37\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Exclusive OR**  \n\nSlide 5.1.40  \nWhich we can replace with leaves.\n\nNow, it's also easy to see that feature 2 will again be useful here,  \n\n(Slide 5.1.40 has a diagram of a decision tree with f1, f2, f3, and the terminal leaves showing 0 or 1.)\n\nSlide 5.1.41  \nAnd we go straight to leaves on this side.\n\nSo we can see that, although no single feature could reduce the average entropy of the child data sets, features 1 and 2 were useful in combination.\n\n(Slide 5.1.41 has a similar decision tree with f1, f2, f3, and the terminal leaves showing 1, 0, 0, 1.)\n\nPruning  \n- Best way to avoid overfitting and not get tricked by short-term lack of progress  \n  * Grow tree as far as possible  \n    -- leaves are uniform or contain a single X  \n  * Prune the tree until it performs well on held-out data  \n  * Amount of pruning is like epsilon in the DNF algorithm\n\nSlide 5.1.42  \nMost real decision-tree building systems avoid this problem by building the tree down significantly deeper than will probably be useful (using something like an entropy cut-off or even getting down to a single data-point per leaf). Then, they prune the tree, using cross-validation to decide what an appropriate pruning depth is.\n\nSlide 5.1.43  \nWe ran a simple version of the tree-learning program on the congressional voting database. Instead of pruning, it has a parameter on the minimum leaf size. If it reaches a node with fewer than that number of examples, it stops and makes a leaf with the majority output.\n\nHere's the tree we get when the minimum leaf size of 20. This problem is pretty easy, so this small tree works very well. If we grow bigger trees, we don't get any real increase in accuracy.\n\n(Slide 5.1.43 shows a tree for Congressional Voting with nodes labeled: physician fee freeze, synfuels cutback, mx-missile. Labeled leaves are D and R, with numbers 119, 1, 90, 3.)\n\nmin leaf size = 20"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_012.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nData Mining\n\n- Making useful predictions in (usually corporate) applications\n- Decision trees very popular because\n  - easy to implement\n  - efficient (even on huge data sets)\n  - easy for humans to understand resulting hypotheses\n\nSlide 5.1.44\n\nBecause decision tree learning is easy to implement and relatively computationally efficient, and especially because the hypotheses are easily understandable by humans, this technology is very widely used.\n\nThe field of data mining is the application of learning algorithms to problems of interest in many industries. Decision trees is one of the principle methods used in data mining.\n\nIn the next few sections, we'll see how to broaden the applicability of this and other algorithms to domains with much richer kinds of inputs and outputs.\n\n---\n\n6.034 Notes: Section 5.2\n\n---\n\nSlide 5.2.1\n\nLet's look at one more algorithm, which is called naive Bayes. It's named after the Reverend Thomas Bayes, who developed a very important theory of probabilistic reasoning.\n\n---\n\nNaive Bayes\n\n- Founded on Bayes' rule for probabilistic inference\n- Update probability of hypotheses based on evidence\n- Choose hypothesis with the maximum probability after the evidence has been incorporated\n\n- Algorithm is particularly useful for domains with lots of features\n\nImage of Rev. Thomas Bayes removed due to copyright restrictions.\n\n---\n\nSlide 5.2.2\n\nIt's widely used in applications with lots of features. It was derived using a somewhat different set of justifications than the ones we've given you. We'll start by going through the algorithm, and at the end I'll go through its probabilistic background. Don't worry if you don't follow it exactly. It's just motivational, but it should make sense to anyone who has studied basic probability.\n\nImage of Rev. Thomas Bayes removed due to copyright restrictions. Rev. Thomas Bayes\n\n---\n\n6.034 - Spring 04 - 44\n\n6.034 - Spring 04 - 41\n\n6.034 - Spring 04 - 42"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 5.2.3\nLet's start by looking at an example data set. We're going to try to characterize, for each feature individually, how it is related to the class of an example.\n\nFirst, we look at the positive examples, and count up what fraction of them have feature 1 on and what fraction have feature 1 off. We'll call these fractions R\\( _{1} \\)(1, 1) and R\\( _{1} \\)(0, 1). We can see here that most positive examples have this feature 1 off.\n\nExample:\n\nExample | f\\( _{x} \\)  f\\( _{y} \\)  f\\( _{z} \\)  Y\n--------|---------  ---------  ---------  -----\n        |   0        1        0          1\n        |   0        0        1          1\n        |   1        0        0          1\n        |   0        1        0          1\n        |   0        0        0          1\n        |   1        1        0          0\n        |   1        1        1          0\n        |   1        0        1          0\n        |   1        0        0          0\n        |   1        0        0          0\n\nR\\( _{1} \\)(1,1)=1/5: fraction of all positive examples that have feature 1 on\nR\\( _{1} \\)(0,1)=4/5: fraction of all positive examples that have feature 1 off\n\nSlide 5.2.4\nNow, we look at the negative examples, and figure out what fraction of negative examples have feature 1 on and what fraction have it off. We call these fractions R\\( _{1} \\)(1, 0) and R\\( _{1} \\)(0, 0). Here we see that all negative examples have this feature on.\n\nR\\( _{1} \\)(1,0)=5/5  R\\( _{1} \\)(0,0)=0/5\n\nSlide 5.2.5\nWe can compute these values, as shown here, for each of the other features, as well.\n\nSlide 5.2.6\nThese R values actually represent our hypothesis in a way we'll see more clearly later. But that means that, given a new input x, we can use the R values to compute an output value Y.\n\n\nExample\n-----------\nf\\( _{x} \\)  f\\( _{y} \\)  f\\( _{z} \\)  Y\n\n0 1 0 1  0 1 0 1\n0 0 1 1  1 0 0 1\n1 0 0 1  0 0 1 1\n0 1 0 1  1 1 0 0\n0 0 0 1  1 1 1 0\n1 1 0 0  1 0 1 0\n1 1 0 0  1 0 0 0\n1 0 1 0  0 1 0 0\n1 0 0 0  0 0 1 0\n1 0 0 0  1 0 0 0\n\nR\\( _{1} \\)(1,1)=1/5  R\\( _{2} \\)(0,1)=4/5\nR\\( _{1} \\)(0,1)=4/5  R\\( _{2} \\)(0,0)=3/5\n\nR\\( _{1} \\)(1,0)=5/5  R\\( _{3} \\)(1,0)=1/5\nR\\( _{1} \\)(0,0)=0/5  R\\( _{3} \\)(0,0)=4/5\n\nR\\( _{2} \\)(1,1)=1/5  R\\( _{1} \\)(0,0)=3/5\nR\\( _{2} \\)(1,0)=2/5  R\\( _{1} \\)(0,0)=1/5\nR\\( _{2} \\)(0,1)=4/5\nR\\( _{2} \\)(0,0)=3/5\n\n---\n\nPrediction\n\n- R\\( _{1} \\)(1,1)=1/5  R\\( _{1} \\)(0,1)=4/5\n- R\\( _{1} \\)(0,1)=4/5  R\\( _{1} \\)(0,0)=3/5\n- R\\( _{2} \\)(1,1)=1/5  R\\( _{2} \\)(0,0)=3/5\n- R\\( _{2} \\)(1,0)=2/5  R\\( _{2} \\)(0,0)=1/5\n- R\\( _{3} \\)(1,1)=1/5  R\\( _{3} \\)(0,0)=4/5\n- R\\( _{3} \\)(0,1)=4/5  R\\( _{3} \\)(0,0)=4/5\n- R\\( _{1} \\)(0,1)=4/5  R\\( _{3} \\)(0,0)=1/5"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.2.7\nImagine we get a new x = <0,0,1>. We start out by computing a \"score\" for this example being a positive example. We do that by multiplying the positive R values, one for each feature. So, our x has feature 1 equal to 0, so we use R1 of 0, 1. Has feature 2 equal to zero, so we use R2 of 0, 1. It has feature 3 equal to 1, so we use R3 of 1, 1. And so on. I've shown the feature values in blue to make it clear which arguments to the R functions they're responsible for. Similarly, I've shown the 1's that come from the fact that we're computing the positive score in green.\n\nEach of the factors in the score represents the degree to which this feature tends to have this value in positive examples. Multiplied all together, they give us a measure of how likely it is that this example is positive.\n\nSlide 5.2.8\nWe can do the same thing to compute a score for x being a negative example. Something pretty radical happens here, because we have R3 of 0, 0 equal to 0. We've never seen a negative example with feature 1 of 1, so we have concluded, essentially, that it's impossible for that to happen. Thus, because our x has feature 1 equal to 0, we think it's impossible for x to be a negative example.\n\nSlide 5.2.9\nFinally, we compare score 1 to score 0, and generate output 1 because score 1 is larger than score 0.\n\nSlide 5.2.10\nHere's the learning algorithm written out just a little bit more generally. To compute Rj of 1, 1, we just count, in our data set, how many examples there have been in which feature j has had value 1 and the output was also 1, and divide that by the total number of samples with output 1.\n\n[Box with Predictions Figures]\n* R1(1,1)=1/5;  R1(0,1)=4/5\n* R1(0,0)=5/5;  R0(0,0)=0/5\n* R2(1,1)=2/5;  R1(0,1)*R3(0,1)=4/5\n* R1(1,1)=4/5;  R3(1,1)*R0(1,1)=3/5\n* R1(0,0)=3/5;  R1(1,1)=1/5\n* R1(1,0)=1/5;  R1(1,0)=1/5\n* New x = <0,0,1>\n* S(1) = R1(0,1)*R2(0,1)*R3(1,1)*R1(1,1) = .205\n* S(0) = R1(0,0)*R2(0,0)*R3(1,0)*R1(0,0) = 0\n* S(1) > S(0), so predict class 1\n\nLearning Algorithm\n* Estimate from the data, for all j:\n  * Rj(1,1) = #(x'j = 1 \u2227 y' = 1)/#(y' = 1)\n* R1(0,1)=4/5  R1(0,1)=4/5  R1(1,0)=4/5\n* R1(1,1)=3/5  R1(1,1)=3/5  R1(1,1)=3/5\n* R1(1,0)=4/5."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 5.2.11\nNow, R_j of 0, 1 is just 1 minus R_j of 1, 1.\n\n---\n\nSlide 5.2.12\nSimilarly, R_j of 1, 0 is the number of examples in which feature j had value 1 and the output was 0, divided the total number of examples with output 0. And R_j of 0, 0 is just 1 minus R_j of 1, 0.\n\n---\n\nLearning Algorithm\n* Estimate from the data, for all j:\n\nR_j(1,1) = #(x^j = 1 \u2227 y = 1)\n          / #(y = 1)\n\nR_j(0,1) = 1 - R_j(1,1)\n\nR_j(1,0) = #(x^j = 1 \u2227 y = 0)\n          / #(y = 0)\n\nR_j(0,0) = 1 - R_j(1,0)\n\n---\n\nSlide 5.2.13\nNow, given a new example, x, let the score for class 1, S(1), be the product, over all j, of R_j of 1,1 if x_j = 1 and R_j of 0, 1 otherwise.\n\n---\n\nPrediction Algorithm\n* Given a new x,\n\nS(1) = \u220f_j [R_j(1,1)  if  x_j = 1\n              [R_j(0,1)  otherwise\n\nS(0) = \u220f_j [R_j(1,0)  if  x_j = 1\n              [R_j(0,0)  otherwise\n\n---\n\nSlide 5.2.14\nSimilarly, S(0) is the product, over all j, of R_j of 1, 0 if x_j = 1 and R_j of 0, 0 otherwise."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_016.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.  \n\nSlide 5.2.15\nIf S(1) is greater than S(0), then we'll predict that Y = 1, else 0.\n\nSlide 5.2.16\nWe can run into problems of numerical precision in our calculations if we multiply lots of probabilities together, because the numbers will rapidly get very small. One standard way to deal with this is to take logs everywhere. Now, we\u2019ll output 1 if the log of the score for 1 is greater than the log of score 0. And the log of a product is the sum of the logs of the factors.\n\nPrediction Algorithm\n\u2022 Given a new x,\n  log S(1) = \\(\\sum_i\\) log \\(R_i(1,1)\\)   if \\(x_i = 1\\)\n                         log \\(R_i(0,1)\\)   otherwise\n  log S(0) = \\(\\sum_i\\) log \\(R_i(1,0)\\)  if \\(x_i = 1\\)\n                         log \\(R_i(0,0)\\)  otherwise  \n\u2022 Output 1 if log S(1) > log S(0)\n\n(5.2.16)\n\nSlide 5.2.17\nIn our example, we saw that if we had never seen a feature take value 1 in a positive example, our estimate for how likely that would be to happen in the future was 0. That seems pretty radical, especially when we only have had a few examples to learn from. There\u2019s a standard hack to fix this problem, called the \u201cLaplace correction\u201d. When counting up events, we add a 1 to the numerator and a 2 to the denominator.\n\nIf we\u2019ve never seen any positive instances, for example, our R(1,1) values would be 1/2, which seems sort of reasonable in the absence of any information. And if we see lots and lots of examples, this 1 and 2 will be washed out, and we\u2019ll converge to the same estimate that we would have gotten without the correction.\n\nThere\u2019s a beautiful probabilistic justification for what looks like an obvious hack. But, sadly, it\u2019s beyond the scope of this class.\n\nLaplace Correction\n\u2022 Avoid getting 0 or 1 as an answer:\n\\[R_i(1,1) = \\frac{\\#(x_i = 1 \\land Y = 1) + 1}{#Y = 1 + 2} \\]\n\\[R_i(0,1) = 1 - R_i(1,1)\\]  \n\\[R_i(1,0) = \\frac{\\#(x_i = 1 \\land Y = 0) + 1}{#Y = 0 + 2} \\]\n\\[R_i(0,0) = 1 - R_i(1,0)\\]\n\nSlide 5.2.18\nHere\u2019s what happens to our original example if we use the Laplace correction. Notably, R1 of 0,0 is now 1/7 instead of 0, which is less dramatic.\n\nExample with Correction\n\\[R_1(1,1) = 2/7 \\quad R_1(0,1) = 5/7\\]\n\\[R_1(1,0) = 2/7 \\quad R_1(0,0) = 5/7\\]\n\\[R_1(1,1) = 3/7 \\quad R_1(0,1) = 4/7\\]\n\\[R_1(1,0) = 3/7 \\quad R_1(0,0) = 4/7\\]\n\\[R_1(1,1) = 5/7 \\quad R_1(0,1) = 2/7\\]\n\\[R_1(1,0) = 5/7 \\quad R_1(0,0) = 2/7\\]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_017.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 5.2.19**\nAnd so, when it comes time to make a prediction, the score for answer 0 is no longer 0. We think it\u2019s possible, but unlikely, that this example is negative. So we still predict class 1.\n\n**Prediction with Correction**\n\\[ \\begin{align*}\nR_{1,1}(0,1)&= 5/7 R_{0,1}(0,1)=7/7 \\\\\nR_{1,0}(0,1)&=2/7 R_{0,0}(0,1)=1/7 \\\\\nR_{1,1}(1,0)&=0/7 R_{0,1}(1,0)=5/7 \\\\\nR_{1,0}(1,0)&=3/7 R_{0,0}(1,0)=4/7 \\\\\nR_{1,1}(0,1)&=5/7 R_{0,1}(0,1)=2/7 \\\\\nR_{1,1}(1,0)&=7/7 R_{0,1}(1,0)=1/7 \\\\\nR_{1,0}(1,1)&=0/7 R_{0,0}(0,1)=0/7 \\\\\n\\end{align*} \\]\nNew \\( x = \\langle 0,0,1 \\rangle \\):\n- \\( S(1) = R_{1}(0,1,0)*R_{1}(1,1)*R_{1}(1,1) = .156 \\)\n- \\( S(0) = R_{0}(1,0)*R_{0}(0,0)*R_{0}(1,0) = .017 \\)\n- \\( S(1) > S(0) \\), so predict class 1\n\n**Slide 5.2.20**\nWhat's the story of this algorithm in terms of hypothesis space? We\u2019ve fixed the spaces of hypotheses to have the form shown here. This is a very restricted form. But it is still a big (infinite, in fact) hypothesis space, because we have to pick the actual values of the coefficients alpha_j and beta_j for all j.\n\n**Hypothesis Space**\n- Output 1 if\n  \\[ \\prod_{j} \\alpha_j x_j + (1 - \\alpha_j)(1 - x_j)> \\prod_{j} \\beta_j x_j + (1- \\beta_j)(1 - x_j) \\]\n- Depends on parameters \\( \\alpha_1, \\alpha_2, \\ldots a_n, \\beta_1, \\ldots, \\beta_n \\)\n  (which we set to be the R_j values)\n\n**Slide 5.2.21**\nAll of our bias is in the form of the hypothesis. We\u2019ve restricted it significantly, so we would now like to choose the alpha\u2019s and beta\u2019s in such a way to minimize the error on the training set. For somewhat subtle technical reasons (ask me and I\u2019ll tell you), our choice of the R scores for the alpha\u2019s and beta\u2019s doesn\u2019t exactly minimize error on the training set. But it usually works pretty well.\n\nThe main reason we like this algorithm is that it\u2019s easy to train. One pass through the data and we can compute all the parameters. It\u2019s especially useful in things like text categorization, where there are huge numbers of attributes and we can\u2019t possibly look at them many times.\n\n**Slide 5.2.22**\nOne possible concern about this algorithm is that it\u2019s hard to interpret the hypotheses you get back. With DNF or decision trees, it\u2019s easy for a human to understand what features are playing an important role, for example.\n\nIn naive Bayes, all the features are playing some role in the categorization. You can think of each one as casting a weighted vote in favor of an answer of 1 versus 0. The weight of each feature\u2019s vote is this expression. The absolute value of this weight is a good indication of how important a feature is, and its sign tells us whether that feature is indicative of output 1 (when it\u2019s positive) or output 0 (when it\u2019s negative).\n\n**Hypothesis Space**\n- Output 1 if\n  \\[ \\prod_{j} \\alpha_j x_j + (1 - \\alpha_j)(1 - x_j)> \\prod_{j} \\beta_j x_j + (1- \\beta_j)(1 - x_j) \\]\n- Depends on parameters \\( \\alpha_1, \\alpha_2, \\ldots, a_n, \\beta_1, \\ldots, \\beta_n \\)\n  (which we set to be the R_j values)\n- Our method of computing parameters doesn\u2019t minimize training set error, but it\u2019s fast!\n- Weight of feature \\( x_j's \\) \"vote\" in favor of output 1:\n  \\[ \\log \\frac{\\alpha_j}{1 - \\alpha_j} - \\log \\frac{\\beta_j}{1 - \\beta_j} \\]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_018.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.2.23\nThis algorithm makes a fundamental assumption that we can characterize the influence of each feature on the class independently, and then combine them through multiplication. This assumption isn't always justified. We'll illustrate this using our old nemesis, exclusive or. Here's the data set we used before.\n\n[Table]\n  f1   f2   f3   y\n0  0   0    0    0\n0  0   1    0    1\n1  1   0    0    1\n1  1   1    0    0\n1  0   0    1    0\n1  0   1    1    1\n\nSlide 5.2.24\nHere are the R values obtained via counting and the Laplace correction. They're all equal to 1/2, because no feature individually gives information about whether the example is positive or negative.\n\nExclusive Or\nR(f1,1)=2/4 R(f1,0)=2/4  \nR(f1,1)=2/4 R(f1,0)=2/4  \n\nR(f2,0)=3/6 R(f2,1)=3/6  \n\nR(f3,0)=3/6 R(f3,1)=3/6  \n\nR(f1,1)=2/4 R(f1,0)=2/4  \nR(f1,1)=2/4 R(f1,0)=2/4\nR(f3,0)=3/6 R(f3,1)=3/6  \n\nSlide 5.2.25\nSure enough, when we compute the scores for any new example, we get the same result, giving us no basis at all for predicting the output.\n\nExclusive Or\n[Table]\n  f1  f2  f3  y\n0  0   0   0  0\n0  0   1   0  1\n1  1   0   0  1\n1  1   1   0  0\n1  0   0   1  0\n1  0   1   1  1\n\n- For any new x\n- S(1) = .5 * .5 * .5 * .5 = .0625\n- S(0) = .5 * .5 * .5 * .5 = .0625\n- We're indifferent between classes\n\nSlide 5.2.26\nCongessional Voting\nNow we show the results of applying naive Bayes to the congressional voting domain. In this case, we might expect the independence assumption to be reasonably well satisfied (a congressperson probably doesn\u2019t decide on groups of votes together, unless there are deals being made)."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_019.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.2.27\nUsing cross-validation, we determined that the accuracy of hypotheses generated by na\u00efve Bayes was approximately 0.91. This is not as good as that of decision trees, which had an accuracy of about 0.95. This result is not too surprising: decision trees can express more complex hypotheses that consider combinations of attributes.\n\nCongressional Voting\n- Accuracy on the congressional voting domain is about 0.91\n- Somewhat worse than decision trees (0.95)\n- Decision trees can express more complex hypotheses over combinations of attributes\n\nSlide 5.2.28\nThis domain is small enough that the efficiency of na\u00efve Bayes doesn\u2019t really matter. So we would prefer to use trees or DNF on this problem.\n\nCongression Voting\n- Accuracy on the congressional voting domain is about 0.91\n- Somewhat worse than decision trees (0.95)\n- Decision trees can express more complex hypotheses over combinations of attributes\n- Domain is small enough so that speed is not an issue\n- So, prefer trees or DNF in this domain\n\nSlide 5.2.29\nIt's interesting to look at the weights found for the various attributes. Here, I've sorted the attributes according to the magnitude of the weight assigned to them by na\u00efve Bayes. The positive ones (colored black) vote in favor of the output being 1 (republican); the negative ones (colored red) vote against the the output being 1 (and therefore in favor of democrat).\n\nThe results are consistent with the answers we've gotten from the other algorithms. The most diagnostic single issue seems to be voting on whether physician fees should be frozen; that is a strong indicator of being a democrat. The strongest indicators of being republican are accepting the budget, aid to the contras, and support of the mx missile.\n\nCongressional Voting\n-6.82   physician-fee-freeze\n-4.01   el-salvador-aid\n-3.64   crime\n-3.36   education-spending\n 3.35   adoption-of-the-budget-resolution\n 2.91   aid-to-nicaraguan-contras\n 3.07   mx-missile\n 2.36   superfund-right-to-sue\n 2.24   duty-free-exports\n-2.14   anti-satellite-test-ban\n-1.99   religious-groups-in-schools\n 1.87   export-administration-act-south-africa\n-1.26   synfuels-corporation-cutback\n-1.63   handicap-leaf\n 1.67   immigration\n-1.19   water-project-cost-sharing\n\nSlide 5.2.30\nNow we'll look briefly at the probabilistic justification for the algorithm. If you don\u2019t follow it, don\u2019t worry. But if you\u2019ve studied probability before, this ought to provide some useful intuition.\n\nProbabilistic Inference"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_020.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.2.31\nOne way to think about the problem of deciding what class a new item belongs to is to think of the features and the output as random variables. If we knew Pr(Y = 1 | f1, ..., fn), then when we got a new example, we could compute the probability that it had a Y value of 1, and generate the answer 1 if the probability was over 0.5. So, we\u2019re going to concentrate on coming up with a way to estimate this probability.\n\n[Title: Probabilistic Inference]\n- Think of features and output as random variables\n- Learn Pr(Y = 1 | f1,..., fn)\n- Given new example, compute probability it has value 1\n  - Generate answer 1 if that value is > 0.5, else 0\n- Concentrate on estimating this distribution from data\n\nSlide 5.2.32\n[Title: Bayes' Rule]\nBayes\u2019 rule gives us a way to take the conditional probability Pr(A|B) and express it in terms of Pr(B|A) and the marginals Pr(A) and Pr(B).\n\nGenerically:\nPr(A | B) = Pr(B | A) Pr(A) / Pr(B)\n\nSlide 5.2.33\nApplying it to our problem, we get Pr(Y = 1 | f1, ..., fn) = Pr(f1, ..., fn | Y = 1) Pr(Y = 1) / Pr(f1, ..., fn)\n\nSlide 5.2.34\nSince the denominator is independent of Y, we can ignore it in figuring out whether Pr(Y = 1 | f1, ..., fn) is greater than Pr(Y = 0 | f1, ..., fn).\n\n[Title: Bayes' Rule]\nGenerically:\nPr(A | B) = Pr(B | A) Pr(A) / Pr(B)\n\nSpecifically:\nPr(Y = 1 | f1, ..., fn) = Pr(f1, ..., fn | Y = 1) Pr(Y = 1) / Pr(f1, ..., fn)\n\n[independent of Y]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_021.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.2.35 \nThe term Pr(Y = 1) is often called the prior. It\u2019s a way to build into our decision-making a previous belief about the proportion of things that are positive. For now, we\u2019ll just assume that it\u2019s 0.5 for both positive and negative classes, and ignore it.\n\nSlide 5.2.36\nBayes\u2019 Rule \n\u2022 Generically:\n  $$Pr(A | B) = \\frac{Pr(B | A) Pr(A)}{Pr(B)}$$\n\n\u2022 Specifically:\n  $$Pr(Y = 1 | f_1,\u2026f_n) = \\frac{Pr(f_1,\u2026f_n | Y = 1) Pr(Y = 1)}{Pr(f_1,\u2026f_n)}$$\n\n  [independent of y]\n  [prior]\n\nThis will allow us to concentrate on Pr(f_1, ... f_n | Y = 1).\n\nSlide 5.2.37\nThe algorithm is called naive Bayes because it makes a big assumption, which is that it can be\nbroken down into a product like this. A probabilist would say that we are assuming that the features are conditionally independent given the class.\n\nSo, we\u2019re assuming that Pr(f_1, \u2026 f_n | Y = 1) is the product of all the individual conditional probabilities, Pr(f_j | Y = 1).\n\nSlide 5.2.38 \nWhy is Bayes Na\u00efve?\n\u2022 Make a big independence assumption\n\n  $$Pr(f_1,\u2026 f_n | Y = 1) = \\prod_j Pr(f_j | Y = 1)$$\n\n\nSlide 5.2.39 \nLearning Algorithm\n\u2022 Estimate from the data, for all j:\n  $$R(f_j = 1| Y = 1) = \\frac{#(x^j = 1 \u2227 y = 1)}{#(y = 1)}$$\n\n  $$R(f_j = 0 | Y = 1) = 1 \u2013 R(f_j = 1| Y = 1)$$\n\n  $$R(f_j = 1| Y = 0) = \\frac{#(x^j = 1 \u2227 y = 0)}{#(y = 0)}$$\n\n  $$R(f_j = 0 | Y = 0) = 1 \u2013 R(f_j = 1| Y = 0)$$\n\nSlide 5.2.38 \nHere is our same learning algorithm (without the Laplace correction, for simplicity), expressed in probabilistic terms.\n\nWe can think of the R values as estimates of the underlying conditional probabilities, based on the training set as a statistical sample drawn from those distributions."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch5_mach1\\page_022.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 5.2.39\nAnd here\u2019s the prediction algorithm, just written out using probabilistic notation. The S is also an\nessential probability, so we predict output 1 just when we think it\u2019s more likely that our new x\nwould have came from class 1 than from class 0.\n\nNow, we'll move on to considering the situation in which the inputs and outputs of the learning\nprocess can be real-valued.\n\n[Image]\nPrediction Algorithm\n\n\u2022 Given a new x\u2081,\n\n     S(x\u2081, ..., x_n | Y = 1) = \u03a0_{i} \\left \\{  \\begin{array} {cc}\nR(x_i = 1 | Y = 1) & \\text{if } x_i = 1 \\\\\nR(x_i = 0 | Y = 1) & \\text{otherwise}\\end{array} \\right \n\n\n     S(x\u2081, ..., x_n | Y = 0) = \u03a0_{i} \\left \\{ \\begin{array} {cc}\nR(x_i = 1 | Y = 0) & \\text{if } x_i = 1 \\\\\nR(x_i = 0 | Y = 0) & \\text{otherwise}\\end{array} \\right\n\n\n\u2022 Output 1 if\n   \n     S(x\u2081, ..., x_n | Y = 1) > S(x\u2081, ..., x_n | Y = 0)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 10.1\n\n---\n\nSlide 10.1.1\nSo far, we've only talked about binary features. But real problems are typically characterized by much more complex features.\n\n[Slide Text]\nFeature Spaces\n\u2022 Features can be much more complex\n\n---\n\nSlide 10.1.2\nSome features can take on values in a discrete set that has more than two elements. Examples might be the make of a car, or the age of a person.\n\n[Slide Text]\nFeature Spaces\n\u2022 Features can be much more complex\n\u2022 Drawn from bigger discrete set\n\n---\n\nSlide 10.1.3\nWhen the set doesn't have a natural order (actually, when it doesn't have a natural distance between the elements), then the easiest way to deal with it is to convert it into a bunch of binary attributes.\n\nYour first thought might be to convert it using binary numbers, so that if you have four elements, you can encode them as 00, 01, 10, and 11. Although that could work, it makes hard work for the learning algorithm, which, in order to select out a particular value in the set will have to do some hard work to decode the bits in these features.\n\nInstead, we typically make it easier on our algorithms by encoding such sets in unary, with one bit per element in the set. Then, for each value, we turn on one bit and set the rest to zero. So, we could encode a four-item set as 1000, 0100, 0010, 0001.\n\n[Slide Text]\nFeature Spaces\n\u2022 Features can be much more complex\n\u2022 Drawn from bigger discrete set  \n  \u2022 If set is unordered (4 different makes of cars, for example), use binary attributes to encode the values (1000, 0100, 0010, 0001)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.1.4  \nOn the other hand, when the set has a natural order, like someone's age, or the number of bedrooms in a house, it can usually be treated as if it were a real-valued attribute using methods we're about to explore.\n\nFeature Spaces  \n- Features can be much more complex  \n- Drawn from bigger discrete set  \n     \u2022 If set is unordered (4 different makes of cars, for example), use binary attributes to encode the values (1000, 0100, 0010, 0001)  \n     \u2022 If set is ordered, treat as real-valued\n\nSlide 10.1.5  \nWe'll spend this segment and the next looking at methods for dealing with real-valued attributes. The main goal will be to take advantage of the notion of distance between values that the reals affords us in order to build in a very deep bias that inputs whose features have \"nearby\" values ought, in general, to have \"nearby\" outputs.\n\nFeature Spaces\n- Features can be much more complex\n  - Drawn from bigger discrete set\n    \u2022 If set is unordered (4 different makes of cars, for example), use binary attributes to encode the values (1000, 0100, 0010, 0001)  \n    \u2022 If set is ordered, treat as real-valued\n\nSlide 10.1.6  \nPredicting Bankruptcy  \nWe'll use the example of predicting whether someone is going to go bankrupt. It only has two features, to make it easy to visualize.\n\nOne feature, L, is the number of late payments they have made on their credit card this year. This is a discrete value that we're treating as a real.\n\nThe other feature, R, is the ratio of their expenses to their income. The higher it is, the more likely you'd think the person would be to go bankrupt.\n\nWe have a set of examples of people who did, in fact go bankrupt, and a set who did not. We can plot the points in a two-dimensional space, with a dimension for each attribute. We've colored the \"positive\" (bankrupt) points blue and the negative points red.\n\nL: late payments / year\nR: expenses / income\n\n\nSlide 10.1.7  \nLove thy Nearest Neighbor  \n\u2022 Remember all your data\n\u2022 When someone asks a question,  \n\u2014find the nearest old data point  \n\u2014return the answer associated with it  \n\nWe took a brief look at the nearest neighbor algorithm in the first segment on learning. The idea is that you remember all the data points you've ever seen and, when you're given a query point, you find the old point that's nearest to the query point and predict its y value as your output."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.1.8\nWhat do we mean by \"Nearest\"?\n- Need a distance function on inputs\n- Typically use Euclidean distance (length of a straight line between the points) \n  \n  \\[ D(x^i, x^k) = \\sqrt{ \\sum (x_j^i - x_j^k)^2 } \\] \n\nIn order to say what point is nearest, we have to define what we mean by \"near\". Typically, we use \nEuclidean distance between two points, which is just the square root of the sum of the squared \ndifferences between corresponding feature values.\n\nSlide 10.1.9\nIn other machine learning applications, the inputs can be something other than fixed-length vectors \nof numbers. We can often still use nearest neighbor, with creative use of distance metrics. The \ndistance between two DNA strings, for example, might be the number of single-character edits \nrequired to turn one into the other.\n\nSlide 10.1.10\nScaling\n- What if we're trying to predict a car's gas mileage?\n  \\(f_1 = \\text{weight in pounds}\\)\n  \\(f_2 = \\text{number of cylinders}\\)\n\nThe naive Euclidean distance isn't always appropriate, though.\n\nConsider the case where we have two features describing a car. One is its weight in pounds and the \nother is the number of cylinders. The first will tend to have values in the thousands, whereas the \nsecond will have values between 4 and 8.\n\nSlide 10.1.11\nIf we just use Euclidean distance in this space, the number of cylinders will have essentially no \ninfluence on nearness. A difference of 4 pounds in a car's weight will swamp a difference between 4 \nand 8 cylinders.\n\nScaling\n- What if we're trying to predict a car's gas mileage?\n  \\(f_1 = \\text{weight in pounds}\\)\n  \\(f_2 = \\text{number of cylinders}\\)\n- Any effect of \\(f_2\\) will be completely lost because of the relative scales"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nScaling\nWhat if we\u2019re trying to predict a car\u2019s gas mileage?\n  \u2022 f\u2081 = weight in pounds\n  \u2022 f\u2082 = number of cylinders\nAny effect of f\u2081 will be completely lost because of the relative scales\nSo, re-scale the inputs\n(Slide 10.1.12)\nOne standard method for addressing this problem is to re-scale the features.\nIn the simplest case, you might, for each feature, compute its range (the difference between its maximum and minimum values). Then scale the feature by subtracting the minimum value and dividing by the range. All features values would be between 0 and 1.\n\n---\n\n(Slide 10.1.13)\nA somewhat more robust method (in case you have a crazy measurement, perhaps due to a noise in a sensor, that would make the range huge) is to scale the inputs to have 0 mean and standard deviation 1. If you haven\u2019t seen this before, it means to compute the average value of the feature, x-bar, and subtract it from each feature value, which will give you features all centered at 0. Then, to deal with the range, you compute the standard deviation (which is the square root of the variance, which we\u2019ll talk about in detail in the segment on regression) and divide each value by that. This transformation, called normalization, puts all of the features on about equal footing.\n\n---\n\nScaling\nWhat if we\u2019re trying to predict a car\u2019s gas mileage?\n  \u2022 f\u2081 = weight in pounds\n  \u2022 f\u2082 = number of cylinders\nAny effect of f\u2081 will be completely lost because of the relative scales\nSo, re-scale the inputs to have mean 0 and variance 1:\n  x - x-bar\n  ---------\n   sigma_x\n\n---\n\n(Slide 10.1.14)\nOf course, you may not want to have all your features on equal footing. It may be that you happen to know, based on the nature of the domain, that some features are more important than others. In such cases, you might want to multiply them by a weight that will increase their influence in the distance calculation.\n\n---\n\n(Slide 10.1.15)\nAnother popular, but somewhat advanced, technique is to use cross validation and gradient descent to choose weightings of the features that generate the best performance on the particular data set.\n\nScaling\nWhat if we\u2019re trying to predict a car\u2019s gas mileage?\n  \u2022 f\u2081 = weight in pounds\n  \u2022 f\u2082 = number of cylinders\nAny effect of f\u2081 will be completely lost because of the relative scales\nSo, re-scale the inputs to have mean 0 and variance 1:\n  x - x-bar\n  ---------\n   sigma_x\nOr, build knowledge in by scaling features differently\nOr use cross-validation to choose scales."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nPredicting Bankruptcy\n\nSlide 10.1.16\nOkay, Let\u2019s see how nearest neighbor works on our bankruptcy example. Let\u2019s say we\u2019ve thought about the domain and decided that the R feature (ratio between expenses and income) needs to be scaled up by 5 in order to be appropriately balanced against the L feature (number of late payments).\n\nSo we\u2019ll use Euclidian distance, but with the R values multiplied by 5 first. We\u2019ve scaled the axes on the slide so that the two dimensions are graphically equal. This means that locus of points at a particular distance d from a point on our graph will appear as a circle.\n\n [Graph: Plot showing No (blue dot) and Yes (red x) data points with R on the X-axis and L on the Y-axis.]\n\nD(x^x) = \\[(L - L*)^2 + (5R - 5R*)^2\n\n---\n\nSlide 10.1.17\nNow, let\u2019s say we have a new person with R equal 0.3 and L equal to 2. What y value should we predict?\n\n---\n\nSlide 10.1.18\nWe look for the nearest point, which is the red point at the edge of the yellow circle. The fact that there are no old points in the circle means that this red point is indeed the nearest neighbor of our query point.\n\n[Graph showing yellow circle around the query point at (0.3, 2) with one red point on the edge.]\n\n---\n\nSlide 10.1.19\nAnd so our answer would be \u201cno\u201d.  \n\n---\n\nPredicting Bankruptcy\n\n[Graph similar to the previous with the distance formula repeated:  \nD(x^x) = \\[(L - L*)^2 + (5R - 5R*)^2 ]\n\n---"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_006.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.1.20\nSimilarly, for another query point,\n\nSlide 10.1.21\nwe find the nearest neighbor, which has output \"yes\"\n\nSlide 10.1.22\nand generate \"yes\" as our prediction.\n\nSlide 10.1.23\nSo, what is the hypothesis of the nearest neighbor algorithm? It's sort of different from our other algorithms, in that it isn't explicitly constructing a description of a hypothesis based on the data it sees.\n\nGiven a set of points and a distance metric, you can divide the space up into regions, one for each point, which represent the set of points in space that are nearer to this designated point than to any of the others. In this figure, I've drawn a (somewhat inaccurate) picture of the decomposition of the space into such regions. It's called a \"Voronoi partition\" of the space.\n\nPredicting Bankruptcy\n\n[D(x^i, x^i') = \\sqrt{\\sum_{j} ((L^i - L^{i'})^2 + (5R^i - 5R^{i'})^2)}]\n\nThe graph on all slides:\n- Displays a scatter plot with axis labeled L (0 to 8) and R (0 to 2.5) \n- Contains data points where 'No' is represented by a red cross and 'Yes' is represented by a blue dot. \n\n*Note: In each step of analyzing the nearest neighbors, the specific point examined is accentuated using a circular highlight on the graph.*\n\nHypothesis\n\n[D(x^i, x^i') = \\sqrt{\\sum_{j} ((L^i - L^{i'})^2 + (5R^i - 5R^{i'})^2)}]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_007.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.1.24\nNow, we can think of our hypothesis as being represented by the edges in the Voronoi partition that separate a region associated with a positive point from a region associated with a negative one. In our example, that generates this bold boundary.\n\nIt's important to note that we never explicitly compute this boundary; it just arises out of the \"nearest neighbor\" query process.\n\nHypothesis\n\nD ( x ^( 1 ), x ^( 2 ) ) = \\sqrt { ( ( L ^( 1 ) - L ^( 2 ) )^2 + (5R - 5R)^2 ) }\n\nSlide 10.1.25\nIt's useful to spend a little bit of time thinking about how complex this algorithm is. Learning is very fast. All you have to do is remember all the data you've seen!\n\nTime and Space\n\u2022 Learning is fast\n\nSlide 10.1.26\nWhat takes longer is answering a query. Na\u00efvely, you have to, for each point in your training set (and there are m of them) compute the distance to the query point (which takes about n computations, since there are n features to compare). So, overall, this takes about m * n time.\n\nTime and Space\n\u2022 Learning is fast\n\u2022 Lookup takes about m*n computations\n\nSlide 10.1.27\nIt's possible to organize your data into a clever data structure (one such structure is called a K-D tree). It will allow you to find the nearest neighbor to a query point in time that's, on average, proportional to the log of m, which is a huge savings.\n\nTime and Space\n\u2022 Learning is fast\n\u2022 Lookup takes about m*n computations\n\u2022 storing data in a clever data structure (KD-tree) reduces this, on average, to log(m)*n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_008.jpeg",
    "text": "Slide 10.1.28\n---\nTime and Space\n- Learning is fast\n- Lookup takes about m*n computations\n  * storing data in a clever data structure (KD-tree) reduces this, on average, to log(m)*n\n- Memory can fill up with all that data\n\nAnother issue is memory. If you gather data over time, you might worry about your memory filling up, since you have to remember it all.\n\nSlide 10.1.29\n---\nThere are a number of variations on nearest neighbor that allow you to forget some of the data points; typically the ones that are most forgettable are those that are far from the current boundary between positive and negative.\n\nTime and Space\n- Learning is fast\n- Lookup takes about m*n computations\n  * storing data in a clever data structure (KD-tree) reduces this, on average, to log(m)*n\n- Memory can fill up with all that data\n * delete points that are far away from the boundary\n\nSlide 10.1.30\n---\nNoise\n\n[Graph Image Placeholder]\n\nIn our example so far, there has not been much (apparent) noise; the boundary between positives and negatives is clean and simple. Let's now consider the case where there's a blue point down among the reds. Someone with an apparently healthy financial record goes bankrupt.\n\nThere are, of course, two ways to deal with this data point. One is to assume that it is not noise; that is, that there is some regularity that makes people like this one go bankrupt in general. The other is to say that this example is an \"outlier.\" It represents an unusual case that we would prefer largely to ignore, and not to incorporate it into our hypothesis.\n\nSlide 10.1.31\n---\nSo, what happens in nearest neighbor if we get a query point next to this point?\n\n[Graph Image Placeholder]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.1.32\nWe find the nearest neighbor, which is a \u201cyes\u201d point, and predict the answer \u201cyes\u201d. This outcome is consistent with the first view; that is, that this blue point represents some important property of the problem.\n\nSlide 10.1.33\nBut if we think there might be noise in the data, we can change the algorithm a bit to try to ignore it. We'll move to the k-nearest neighbor algorithm. It's just like the old algorithm \n except that when we get a query, we'll search for the k closest points to the query points. And we'll generate, as output, the output associated with the majority of the k closest elements.\n\nSlide 10.1.34\nIn this case, we've chosen k to be 3. The three closest points consist of two \u201cno\u201ds and a \u201cyes\u201d, so our answer would be \u201cno\u201d.\n\nSlide 10.1.35\nIt's not entirely obvious how to choose k. The smaller the k, the more noise-sensitive your hypothesis is. The larger the k, the more 'smeared out' it is. In the limit of large k, you would always just predict the output value that's associated with the majority of your training points. So, k functions kind of like a complexity-control parameter, exactly analogous to epsilon in DNF and min-leaf-size in decision trees. With smaller k, you have high variance and risk overfitting; with large k, you have high bias and risk not being able to express the hypotheses you need.\n\nIt's common to choose k using cross-validation.\n\n\"k-Nearest Neighbor.  Find the k nearest points.  Predict output according to the majority.  Choose k using cross-validation.\" \n\nImage Descriptions:\nSlide 10.1.32: A graph with two types of points, \"No\" (red) and \"Yes\" (blue). A target point is within the circle of a \"Yes\" point.\n\nSlide 10.1.33: Another graph extending the previous model, showing a k-nearest approach.\n\nSlide 10.1.34: Graph showing k=3 with two \"No\" points and one \"Yes\" point within a selected circle.\n\nSlide 10.1.35: Graphic reiterating the explanation of k-nearest neighbor, illustrating the concepts of variance and bias."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Curse of Dimensionality**\n- Nearest neighbor is great in low dimensions (up to about 6).\n- As n increases, things get weird:\n\n---\n\n**Slide 10.1.36**\nNearest neighbor works very well (and is often the method of choice) for problems in relatively low-dimensional real-valued spaces.\n\nBut as the dimensionality of a space increases, its geometry gets weird. Here are some surprising (to me, at least) facts about high-dimensional spaces.\n\n---\n\n**Slide 10.1.37**\nIn high dimensions, almost all points are far away from one another.\n\nIf you make a cube or sphere in high dimensions, then almost all the points within that cube or sphere are near the boundaries.\n\n---\n\n**Curse of Dimensionality**\n- Nearest neighbor is great in low dimensions (up to about 6)\n- As n increases, things get weird:\n  - In high dimensions, almost all points are far away from one another\n  - They're almost all near the boundaries\n\n---\n\n**Slide 10.1.38**\nImagine sprinkling data points uniformly within a 10-dimensional unit cube (cube whose sides are of length 1).\n\nTo capture 10% of the points, you'd need a cube with sides of length .63!\n\n---\n\n**Slide 10.1.39**\nAll this means that the notions of nearness providing a good generalization principle, which are very effective in low-dimensional spaces, become fairly ineffective in high-dimensional spaces. There are two ways to handle this problem. One is to do \"feature selection\", and try to reduce the problem back down to a lower-dimensional one. The other is to fit hypotheses from a much smaller hypothesis class, such as linear separations, which we will see in the next chapter.\n\n**Curse of Dimensionality**\n- Nearest neighbor is great in low dimensions (up to about 6)\n- As n increases, things get weird:\n  - In high dimensions, almost all points are far away from one another\n  - They're almost all near the boundaries\n  - Imagine sprinkling data points uniformly within a 10-dimensional unit cube\n  - To capture 10% of the points, you'd need a cube with sides of length .63!\n  - Cure: feature selection or more global models"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nTest Domains\n\nSlide 10.1.40\nWe'll look at how nearest neighbor performs on two different test domains.\n\n---\n\nSlide 10.1.41\nThe first domain is predicting whether a person has heart disease, represented by a significant narrowing of the arteries, based on the results of a variety of tests. This domain has 297 different data points, each of which is characterized by 26 features. A lot of these features are actually boolean, which means that although the dimensionality is high, the curse of dimensionality, which really only bites us badly in the case of real-valued features, doesn't cause too much problem.\n\nTest Domains\n- Heart Disease: predict whether a person has significant narrowing of the arteries, based on tests\n  - 26 features\n  - 297 data points\n\n---\n\nSlide 10.1.42\nIn the second domain, we're trying to predict whether a car gets more than 22 miles-per-gallon fuel efficiency. We have 385 data points, characterized by 12 features. Again, a number of the features are binary.\n\nTest Domains\n- Heart Disease: predict whether a person has significant narrowing of the arteries, based on tests\n  - 26 features\n  - 297 data points\n- Auto MPG: predict whether a car gets more than 22 miles per gallon, based on attributes of car\n  - 12 features\n  - 385 data points\n\n---\n\nSlide 10.1.43\nHere's a graph of the cross-validation accuracy of nearest neighbor on the heart disease data, shown as a function of k. Looking at the data, we can see that the performance is relatively insensitive to the choice of k, though it seems like maybe it's useful to have k be greater than about 5.\n\n[Graph Image]\nHeart Disease\n- Relatively insensitive to k\n\n[Graph shows a plot with axis k from 0 to 40 and accuracy]\n[Line marked as \"Raw\"]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_012.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 10.1.44\nHeart Disease\n- Relatively insensitive to k\n- Normalization matters!\n\nThe red curve is the performance of nearest neighbor using the features directly as they are measured, without any scaling. We then normalized all of the features to have mean 0 and standard deviation 1, and re-ran the algorithm. You can see here that it makes a noticeable increase in performance.\n\n[Graph with x-axis labeled 'k' and y-axis with two lines labeled 'Raw' and 'Normalized']\n\n---\n\nSlide 10.1.45\nWe ran nearest neighbor with both normalized and un-normalized inputs on the auto-MPG data. It seems to perform pretty well in all cases. It is still relatively insensitive to k, and normalization only seems to help a tiny amount.\n\nAuto MPG\n- Relatively insensitive to k\n- Normalization doesn\u2019t matter much\n\n[Graph with x-axis labeled 'k' and y-axis with two lines labeled 'Raw' and 'Normalized']\n\n---\n\nSlide 10.1.46\nAuto MPG\n- Now normalization matters a lot!\n- Watch the scales on your graphs\n\nWatch out for tricky graphing! It\u2019s always possible to make your algorithm look much better than the other leading brand (as long as it\u2019s a little bit better), by changing the scale on your graphs. The previous graph had a scale of 0 to 1. This graph has a scale of 0.85 to 0.95. Now the normalized version looks much better! Be careful of such tactics when you read other peoples\u2019 papers, and certainly don\u2019t practice them in yours.\n\n[Graph with x-axis labeled 'k' and y-axis with two lines labeled 'Raw' and 'Normalized']"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 10.2\n\nSlide 10.2.1\nNow, let\u2019s go back to decision trees, and see if we can apply them to problems where the inputs are numeric.\n\n[Illustration: \"Remember Decision Trees\" chart showing decision branches for clothing and activity based on precipitation, temperature, and weekend status]\n\nSlide 10.2.2\nWhen we have features with numeric values, we have to expand our hypothesis space to include different tests on the leaves. We will allow tests on the leaves of a decision tree to be comparisons of the form xj > c, where c is a constant.\n\nNumerical Attributes\n* Tests in nodes can be of the form xj > constant\n\nSlide 10.2.3\nThis class of splits allows us to divide our feature-space into a set of exhaustive and mutually exclusive hyper-rectangles (that is, rectangles of potentially high dimension), with one rectangle for each leaf of the tree. So, each rectangle will have an output value (1 or 0) associated with it. The set of rectangles and their output values constitutes our hypothesis.\n\nNumerical Attributes\n* Tests in nodes can be of the form xj > constant\n* Divides the space into axis-aligned rectangles\n\nSlide 10.2.4\nSo, in this example, at the top level, we split the space into two parts, according to whether feature 1 has a value greater than 2. If not, then the output is 1.\n\n[Illustration: Graph showing division of space with feature 1 greater than 2 marked as \"yes\" leading to output 0, otherwise output is 1]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 10.2.5\nIf I is greater than 2, then we have another split, this time on whether I2 is greater than 4. If it is, the answer is 0, otherwise, it is 1. You can see the corresponding rectangles in the two-dimensional feature space.\n\nSlide 10.2.6\nNumerical Attributes\n  \u2022 Tests in nodes can be of the form xj > constant\n  \u2022 Divides the space into axis-aligned rectangles\n\nThis class of hypotheses is fairly rich, but it can be hard to express some concepts.\n\nThere are fancier versions of numeric decision trees that allow splits to be arbitrary hyperplanes (allowing us, for example, to make a split along a diagonal line in the 2D case), but we won't pursue them in this class.\n\nSlide 10.2.7\nThe only thing we really need to do differently in our algorithm is to consider splitting between each data point in each dimension.\n\nSlide 10.2.8\nConsidering Splits\n  \u2022 Consider a split between each point in each dimension\n\nSo, in our bankruptcy domain, we'd consider 9 different splits in the R dimension (in general, you'd expect to consider m-1 splits, if you have m data points; but in our dataset we have some examples with equal R values).\n\n---\n\nFigures included:\n\n1. A diagram showing the split on f1 > 2 and f2 > 4, dividing into regions labeled 0 and 1.\n2. A diagram showing axis-aligned rectangles for a test involving f1 and f2.\n3. A scatter plot with points labeled 'No' and 'Yes', showing potential splits between points in the dimension R."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n* Slide 10.2.9\nAnd there are another 6 possible splits in the L dimension (because L is an integer, really, there are lots of duplicate L values).\n\n* Slide 10.2.10\nAll together, this is a lot of possible splits! As before, when building a tree, we'll choose the split that minimizes the average entropy of the resulting child nodes.\n\n* Slide 10.2.11\nLet's see what actually happens with this algorithm in our bankruptcy domain.\n\nWe consider all the possible splits in each dimension, and compute their average entropies.\n\n* Slide 10.2.12\nSplitting in the L dimension at 1.5 will do the best job of reducing entropy, so we pick that split.\n\nFigures:\n- Considering Splits\n  - Consider a split between each point in each dimension\n    - Dots: No (blue), Yes (red)\n\n- Choose split that minimizes average entropy of child nodes\n\n- Bankruptcy Example\n  - Table listing entropy calculations:\n    - L value, NL, NR, Ave (etc.)\n    - Example row: 1.5 | 3 | 3 | 0.65 \n\n- AIE 1.00 | 0.69 | 0.69 | 0.62\n- Rcx 0.25 | 0.40 | 0.48 | 0.58"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_016.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.2.13  \nAnd we see that, conveniently, all the points with L not greater than 1.5 are of class 0, so we can \nmake a leaf there.\n\nImage Description: \nBankruptcy Example\n A table with columns: L<=, NL, NR, AE, Passen. L with values: 13, 5.0, 3.0, 2.5, 2.5, 2.0, 1.5, 1.0, 1.0, 0.9 \nA chart with different colored lines and points, representing some data values and criteria.\n\nSlide 10.2.14\nNow, we consider all the splits of the remaining part of the space. Note that we have to recalculate all the average entropies again, because the points that fall into the leaf node are taken out of consideration.\n\nSlide 10.2.15  \nNow the best split is at R > 0.9. And we see that all the points for which that's true are positive, so \nwe can make another leaf.\n\nBankruptcy Example\n\nImage Description: \nA diagram with axes possibly representing a coordinate plane, labeled points and labeled decision points: \nL > 1.5 (Yes or No), R > 0.9, and two outcomes with respective leaf.\n\nSlide 10.2.16\nAgain we consider all possible splits of the points that fall down the other branch of the tree."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_017.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 10.2.17**  \nAnd we find that splitting on L > 5.0 gives us two homogenous leaves.\n\n**Slide 10.2.18**  \nSo, we finish with this tree, which happens to have zero error on our data set.\n\nOf course, all of the issues that we talked about before with boolean attributes apply here: in general, you\u2019ll want to stop growing the tree (or post-prune it) in order to avoid overfitting.\n\n\n**Slide 10.2.19**  \nWe ran this decision-tree algorithm on the heart-disease data set. This graph shows the cross-validation accuracy of the hypotheses generated by the decision-tree algorithm as a function of the min-leaf-size parameter, which stops splitting when the number of examples in a leaf gets below the specified size.\n\nThe best performance of this algorithm is about .77, which is slightly worse than the performance of nearest neighbor.\n\n**Slide 10.2.20**  \nBut performance isn\u2019t everything. One of the nice things about the decision tree algorithm is that we can interpret the hypothesis we get out. Here is an example decision tree resulting from the learning algorithm.\n\nI\u2019m not a doctor (and I don\u2019t even play one on TV), but the tree at least kind of makes sense. The top-level split is on whether a certain kind of stress test, called \u201cthal\u201d comes out normal.\n\n**Figure Descriptions:**\n\n**Bankruptcy Example (Slide 10.2.17 & 10.2.18)**\n- A tree diagram with decision nodes labeled \"L > 1.5\" and \"R > 0.9\" leading to a question \"L > 5.0\"\n- Leaves labeled with 0 or 1\n- Plot with points and decision boundary lines\n\n**Heart Disease (Slide 10.2.19 & 10.2.20)**\n- Graph of CV accuracy versus min leaf size, showing that best performance is .77 while nearest neighbor is .81\n- Decision tree with a split on \"thal = 1\"\n\nNote: \"thal = 1\" indicates normal exercise thallium scintigraphy test."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_018.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.2.21  \nIf thal is not normal, then we look at the results of the \u201cca\u201d test. This test has as results numbers 0 through 3, indicating how many blood vessels were shown to be blocked in a different test. We chose to code this feature with 4 binary attributes.\n\n[Diagram]\nHeart Disease\n\nthal = 1\nca = 0\n\nSlide 10.2.22\nSo \u201cca = 0\u201d is false if 1 or more blood vessels appeared to be blocked. If that\u2019s the case, we assert that the patient has heart disease.\n\nSlide 10.2.23\nNow, if no blood vessels appeared to be blocked, we ask whether the patient is having exercise-induced angina (chest pain) or not. If not, we say they don\u2019t have heart disease; if so, we say they do.\n\n[Diagram]\nHeart Disease\nthal = 1: normal exercise thallium scintigraphy test\nca = 0: no vessels colored by fluoroscopy\n\nca = 0\nexang\n1\n0\n\nSlide 10.2.24\nNow, over on the other side of the tree, where the first test was normal, we also look at the results of the ca test.\n\n[Diagram]\nHeart Disease\n\nthal = 1: normal exercise thallium scintigraphy test\nca = 0: no vessels colored by fluoroscopy\nexang: exercise induced angina\n\nca = 0\n\n1\n  \n0  1"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_019.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Slide 10.2.25**\nIf it doesn't have value 0 (that is one or more vessels appear blocked), then we ask whether they have chest pain (presumably this is resting, not exercise-induced chest pain), and that determines the output.\n\n**Slide 10.2.26**\nIf no blood vessels appear to be blocked, we consider the person's age. If they're less than 57.5, then we declare them to be heart-disease free. Whew!\n\n**Slide 10.2.27**\nIf they're older than 57.5, then we examine some technical feature of the cardiogram, and let that determine the output.\n\nHypotheses like this are very important in real domains. A hospital would be much more likely to base or change their policy for admitting emergency-room patients who seem to be having heart problems based on a hypothesis that they can see and interpret rather than based on the sort of numerical gobbledigook that comes out of nearest neighbor or naive Bayes.\n\n[Image: Heart Disease Decision Tree]\nthall = 1\ntests chest pain\n- ca = 0: X (Yes, heart disease present)\n- - exang = 0: 1\n- - exang = 1: 0\n- ca = 1\n- - chest pain > 57.5: 1\n- - chest pain <= 57.5\n- - - age > oldpkg = 0.3.2\n[Description of Test: thall = 1: normal exercise thallium scintigraphy test | ca = 0: no vessels colored by fluoroscopy | exang: exercise induced angina | oldpkg: feature of cardiogram]\n\n**Slide 10.2.28**\nWe also ran the decision-tree algorithm on the Auto MPG data. We got essentially the same performance as nearest neighbor, and a strong insensitivity to leaf size.\n\n[Graph: Auto MPG Performance]\nY-axis: % accuracy ranging from 85 to 95\nX-axis: min leaf size ranging from 2 to 60\nCurve shows performance (.91) essentially the same as nearest neighbor."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_020.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 10.2.9\nHere\u2019s a sample resulting decision tree. It seems pretty reasonable. If the engine is big, then we\u2019re \nunlikely to have good gas mileage. Otherwise, if the weight is low, then we probably do have good \ngas mileage. For a low-displacement, heavy car, we consider the model-year. If it\u2019s newer than \n1978, (this is an old data set!) then we predict it will have good gas mileage. And if it\u2019s older, then \nwe make a final split based on whether or not it\u2019s really heavy.\n\nIt\u2019s also possible to apply naive bayes to problems with numeric attributes, but it\u2019s hard to justify \nwithout recourse to probability, so we\u2019ll skip it. %to %c Qu-# add a slide showing how one non-\nisohetetic split would do the job, %but it requires a lot of rectangles.\n\nDiagram:\nMore than 22 MPG?\ndisplacement > 189.5\n weight > 2224.5 \n year > 78.5\n weight > 2775\n  1         0\n                 1         0\n  1         0\n\n---\n\n6.034 Notes: Section 10.3\n\n---\n\nSlide 10.3.1\nSo far, we\u2019ve spent all of our time looking at classification problems, in which the y values are \neither 0 or 1. Now we\u2019ll briefly consider the case where the y\u2019s are numeric values. We\u2019ll see how \nto extend nearest neighbor and decision trees to solve regression problems.\n\nRegression\n- Output is a continuous numeric value\n- Locally-weighted averaging\n- Regression trees\n\n---\n\nLocal Averaging\n* Remember all your data\n\nDiagram: Graph with points scattered along the curve of X versus Y\n \n\n---\n\nSlide 10.3.2\nThe simplest method for doing regression is based on nearest neighbor. As in nearest neighbor, you \nremember all your data."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_021.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.3.3\nWhen you get a new query point x, you find the K nearest points.\n\nImage description: A graph showing data points and a query point x. A circle highlights the K nearest data points to x.\n\nSlide 10.3.4\nLocal Averaging\n* Remember all your data\n* When someone asks a question,\n  \u2013 find the K nearest old data points\n  \u2013 return the average of the answers associated with them\n\nEquation: y = (1/K) \u03a3 y^k\n\nImage description: A similar graph to Slide 10.3.3, illustrating local averaging with K nearest points highlighted.\n\nThen average their y values and return that as your answer.\n\nOf course, I'm showing this picture with a one-dimensional x, but the idea applies for higher-\ndimensional x, with the caveat that as the dimensionality of x increases, the curse of dimensionality is likely to be upon us.\n\nSlide 10.3.5\nWhen K = 1, this is like fitting a piecewise constant function to your data. It will track your data very\nclosely, but, as in nearest neighbor, have high variance and be prone to overfitting.\n\nK = 1\n* Tracks data very closely\n* Prone to overfitting\n\nImage description: A graph with a line that makes sharp turns at each data point, illustrating high variance and overfitting.\n\nSlide 10.3.6\nBigger K\n* Smoothes out variations in data\n* May introduce too much bias\n\nWhen K is larger, variations in the data will be smoothed out, but then there may be too much bias,\nmaking it hard to model the real variations in the function.\n\nImage description: A graph showing a smoother curve line fitting through data points, indicating increased bias with larger K."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_022.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 10.3.7\n\nOne problem with plain local averaging, especially as k gets large, is that we are letting all k neighbors have equal influence on the predicting the output of the query point. In locally weighted averaging, we still average the y values of multiple neighbors, but we weight them according to how close they are to the target point. That way, we let nearby points have a larger influence than farther ones.\n\n---\n\nLocally Weighted Averaging\n\n6.034 - Spring 04\n\n---\n\nLocally Weighted Averaging\n\n\u2022 Find all points within distance \u03bb from target point\n\u2022 Average the outputs, weighted according to how far away they are from the target point\n\n6.034 - Spring 04\n\n---\n\nSlide 10.3.8\n\nThe simplest way to describe locally weighted averaging involves finding all points that are within a distance lambda from the target point, rather than finding the k nearest points. We'll describe it this way, but it's not too hard to go back and reformulate it to depend on the k nearest.\n\n---\n\nSlide 10.3.9\n\nRather than committing to the details of the weighting function right now, let's just assume that we have a \u201ckernel\u201d function K, which takes the query point and a training point, and returns a weight, which indicates how much influence the y value of the training point should have on the predicted y value of the query point.\n\nThen, to compute the predicted y value, we just add up all of the y values of the points used in the prediction, multiplied by their weights, and divide by the sum of the weights.\n\nLocally Weighted Averaging\n\n\u2022 Find all points within distance \u03bb from target point\n\u2022 Average the outputs, weighted according to how far away they are from the target point\n\u2022 Given a target x, with k ranging over neighbors,\n\n![equation]\n\n y = \\frac{\\sum K(x,x^i) y^i}{\\sum K(x,x^i) }\n\n6.034 - Spring 04\n\n---\n\nSlide 10.3.10\n\nEpanechnikov Kernel\n\n\u2022 D is Euclidean distance\n\nK(x,x^i) = max \\left( 3\\left(1 - \\frac{D(x,x^i)^2}{r^2} \\right) , 0\\right)\n\nx = <5,5>\n\nr = 4\n\n\u2022 Many other possible choices of kernel K\n\n---\n\nSlide 10.3.10\n\nHere is one popular kernel, which is called the Epanechnikov kernel (I like to say that word!). You don't have to care too much about it; but see that it gives high weight to points that are near the query point (<5,5> in this graph) and decreasing weights out to distance lambda.\n\nThere are lots of other kernels which have various plusses and minuses, but the differences are too subtle for us to bother with at the moment.\n\n6.034 - Spring 04\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_023.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 10.3.11\nAs usual, we have the same issue with lambda here as we have had with epsilon, min-leaf-size, and k. If it's too small, we\u2019ll have high variance; if it's too big, we\u2019ll have high bias. We can use cross-validation to choose.\n\nIn general, it\u2019s better to convert the algorithm to use k instead of lambda (it just requires making the lambda parameter in the kernel be the distance to the farthest of the k nearest neighbors). This means that we\u2019re always averaging the same number of points; so in regions where we have a lot of data, we\u2019ll look more locally, but in regions where the training data is sparse, we\u2019ll cast a wider net.\n\n---\n\nSlide 10.3.12\nSmoooth\n\u2022 How should we choose \u03bb?\n  - If small, then we aren\u2019t averaging many points\n    - Worse at averaging out noise\n    - Better at modeling discontinuities\n  - If big, we are averaging a lot of points\n    - Good at averaging out noise\n    - Smears out discontinuities\n\u2022 Can use cross-validation to choose \u03bb\n\u2022 May be better to let it vary according to local density of points\n\nNow we'll take a quick look at regression trees, which are like decision trees, but which have numeric constants at the leaves rather than booleans.\n\n---\n\nSlide 10.3.13\nHere's an example regression tree. It has the same kinds of splits as a regular tree (in this case, with numeric features), but what's different are the labels of the leaves.\n\nDiagram:\nX > 2\n       no             yes\n     3.2             \n                          Y < 4\n                             no               yes\n                           -1.9                2.4\n\nRegression Trees\n\u2022 Like decision trees, but with real-valued constant outputs at the leaves\n\n---\n\nSlide 10.3.14\nLeaf Values\n\u2022 Assign a leaf node the average of the y values of the data points that fall there.\n\nLet's start by thinking about how to assign a value to a leaf, assuming that multiple training points are in the leaf and we have decided, for whatever reason, to stop splitting.\n\nIn the boolean case, we used the majority output value as the value for the leaf. In the numeric case, we'll use the average output value. It makes sense, and besides there's a hairy statistical argument in favor of it, as well."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_024.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 10.3.15\n\nSo, if we're going to use the average value at a leaf as its output, we'd like to split up the data so that the leaf averages are not too far away from the actual items in the leaf.\n\n---\n\nLeaf Values\n\n- Assign a leaf node the average of the y values of the data points that fall there.\n- We'd like to have groups of points in a leaf that have similar y values (because then the average is a good representative)\n\n---\n\nSlide 10.3.16\n\nLucky for us, the statistics folks have a good measure of how spread out a set of numbers is (and, therefore, how different the individuals are from the average); it's called the variance of a set.\n\n---\n\nVariance\n\n- Measure of how much a set of numbers is spread out\n\n---\n\nSlide 10.3.17\n\nFirst we need to know the mean, which is traditionally called mu. It\u2019s just the average of the values. That is, the sum of the values divided by how many there are (which we call m, here).\n\n- Mean of m values, z1 through zm :\n\n       \u03bc = (1/m) \u03a3 zk\n           k=1\n\n---\n\nSlide 10.3.18\n\nThen the variance is essentially the average of the squared distance between the individual values and the mean. If it's the average, then you might wonder why we're dividing by m-1 instead of m. I could tell you, but then I'd have to shoot you. Let's just say that dividing by m-1 makes it an unbiased estimator, which is a good thing.\n\nVariance\n\n- Measure of how much a set of numbers is spread out\n- Mean of m values, z1 through zm :\n\n       \u03bc = (1/m) \u03a3 zk\n           k=1\n\n- Variance: average squared difference between z\u2019s and the mean:\n\n       \u03c3\u00b2 = (1/(m-1)) \u03a3 (zk - \u03bc)\u00b2\n               k=1\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_025.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.3.19\nWe\u2019re going to use the average variance of the children to evaluate the quality of splitting on a particular feature. Here we have a data set, for which I've just indicated the y values. It currently has a variance of 40.5.\n\nSlide 10.3.20\nWe're considering two splits. One gives us variances of 3.7 and 1.67; the other gives us variances of 48.7 and 40.67.\n\nSlide 10.3.21\nJust as we did in the binary case, we can compute a weighted average variance, depending on the relative sizes of the two sides of the split.\n\nSlide 10.3.22\nDoing so, we can see that the average variance of splitting on feature 3 is much lower than of splitting on f7, and so we'd choose to split on f3.\n\nJust looking at the data in the leaves, f3 seems to have done a much better job of dividing the values into similar groups.\n\nDiagrams and Figures:\n\nSlide 10.3.19 (Diagram):\n- D: -2, 9, 1, 12, -4, 0, 11, 10, -1\n- \u03c3\u00b2 = 40.5\n\nSlide 10.3.20 (Diagrams):\n- Split on f3:\n  - Left: -2, -4, 0, -1 (\u03c3\u00b2 = 3.7)\n  - Right: 9, 12, 1, 10, 11 (\u03c3\u00b2 = 1.67)\n  - Combined \u03c3\u00b2 = 40.5\n- Split on f7:\n  - Left: 9, 12, 1, 10, 11 (\u03c3\u00b2 = 48.7)\n  - Right: -2, -4, 0, -1 (\u03c3\u00b2 = 40.67)\n\nSlide 10.3.22 (Diagram):\n- f3 split:\n  - AW = (5/9)*3.7 + (4/9)*1.67 = 2.8\n- f7 split:\n  - AW = (5/9)*48.7 + (4/9)*40.67 = 45.07\n\nFormulas:\n- AV(f) = p\u2080\u03c3\u00b2(D\u2080) + (1 - p\u2080)\u03c3\u00b2(D\u2081)\n- % of D with f=1\n- Subset of D with f=1"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch6_mach2\\page_026.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 10.3.23\nWe can stop growing the tree based on criteria that are similar to those we used in the binary case.\nOne reasonable criterion is to stop when the variance at a leaf is lower than some threshold.\n\nSlide 10.3.24\nOr we can use our old min-leaf-size criterion.\n\nStopping\n\u2022 Stop when variance at a leaf is small enough\n\nSlide 10.3.25\nOnce we do decide to stop, we assign each leaf the average of the values of the points in it.\n\nStopping\n\u2022 Stop when variance at a leaf is small enough\n\u2022 Or when you have fewer than min-leaf elements at a leaf\n\u2022 Set y at a leaf to be the mean of the y values of the elements\n\n      f_5\n    0          \n-1.2       10.5\n-2, 1, 4, 0, 1        9, 12, 11, 10\n              0"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_001.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 7.1\n\nSlide 7.1.1\nWe have been using this simulated bankruptcy data set to illustrate the different learning algorithms that operate on continuous data. Recall that R is supposed to be the ratio of earnings to expenses while L is supposed to be the number of late payments on credit cards over the past year. We will continue using it in this section where we look at a new hypothesis class, linear separators.\n\nOne key observation is that each hypothesis class leads to a distinctive way of defining the decision boundary between the two classes. The decision boundary is where the class prediction changes from one class to another. Let\u2019s look at this in more detail.\n\n---\n\nBankruptcy Example\n\nSlide 7.1.2\n1-Nearest Neighbor Hypothesis\nWe mentioned that a hypothesis for the 1-nearest neighbor algorithm can be understood in terms of a Voronoi partition of the feature space. The cells illustrated in this figure represent the feature space points that are closest to one of the training points. Any query in that cell will have that training point as its nearest neighbor and the prediction will be the class of that training point. The decision boundary will be the boundary between cells defined by points of different classes, as illustrated by the bold line shown here.\n\n---\n\nSlide 7.1.3\nSimilarly, a decision tree also defines a decision boundary in the feature space. Note that although both 1-NN and decision trees agree on all the training points, they disagree on the precise decision boundary and so will classify some query points differently. This is the essential difference between different learning algorithms.\n\n\nDecision Tree Hypothesis\n\n\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t1 /;13"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_002.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 7.1.4\n\nIn this section we will be exploring linear separators which are characterized by a single linear decision boundary in the space. The bankruptcy data can be successfully separated in that manner. But, notice that in contrast to 1-NN and decision trees, there is no guarantee that a single linear separator will successfully classify any set of training data. The linear separator is a very simple hypothesis class, not nearly as powerful as either 1-NN or decision trees. However, as simple as this class is, in general, there will be many possible linear separators to choose from.\n\nAlso, note that, once again, this decision boundary disagrees with that drawn by the previous algorithms. So, there will be some data sets where a linear separator is ideally suited to the data. For example, it turns out that if the data points are generated by two Gaussian distributions with different means but the same standard deviation, then the linear separator is optimal.\n\n---\n\nSlide 7.1.5\n\nA data set that can be successfully split by a linear separator is called, not surprisingly, linearly separable.\n\n---\n\nSlide 7.1.6\n\nAs we\u2019ve mentioned, not all data sets are linearly separable. Here\u2019s one for example. Another classic non-linearly-separable data set is our old nemesis XOR.\n\nIt turns out, although it\u2019s not obvious, that the higher the dimensionality of the feature space, the more likely that a linear separator exists. This will turn out to be important later on, so let\u2019s just file that fact away.\n\n---\n\nSlide 7.1.7\n\nWhen faced with a non-linearly-separable data set, we have two options. One is to use a more complex hypothesis class, such as shown here.\n\n---\n\n[Figures:]\n1. **Linear Hypothesis:**\n   - A graph with two features (R1, R2) showing a linear decision boundary separating two categories, \"Yes\" and \"No.\"\n   - Graph: \"Linear Hypothesis.\"\n   \n2. **Linearly Separable:**\n   - A scatter plot of data points that can be separated by a linear line.\n   - Caption: \"Linearly Separable.\"\n   \n3. **Not Linearly Separable:**\n   - A scatter plot where data points are difficult to separate with a single linear line.\n   - Caption: \"Not Linearly Separable.\"\n   \n4. **Not Linearly Separable with Complex Boundary:**\n   - A graph showing a more complex (non-linear) boundary separating the data points.\n   - Caption: \"Not Linearly Separable.\""
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_003.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.1.8\nOr, keep the simple linear separator and accept some errors. This is the classic bias/variance tradeoff. Use a more complex hypothesis with greater variance or a simpler hypothesis with greater bias. Which is more appropriate depends on the underlying properties of the data, including the amount of noise. We can use our old friend cross-validation to make the choice if we don't have much understanding of the data.\n\nSlide 7.1.9\nSo, let's look at the details of linear classifiers. First, we need to understand how to represent a particular hypothesis, that is, the equation of a linear separator. We will be illustrating everything in two dimensions but all the equations hold for an arbitrary number of dimensions.\n\nThe equation of a linear separator in n-dimensional feature space is (surprise!) a linear equation which is determined by n+1 values, the components of an n-dimensional coefficient vector w and a scalar value b. These n+1 values are what will be learned from the data. The x will be some point in the feature space.\n\nWe will be using dot product notation for compactness and to highlight the geometric interpretation of this equation (more on this in a minute). Recall that the dot product is simply the sum of the componentwise products of the vector components, as shown here.\n\n\u2014\n\nLinear Hypothesis Class\n\u2022 Equation of a hyperplane in the feature space\nw \u22c5 x + b = 0\n\u03a3_{j=1}^{n} w_j x_j + b = 0\n\u2022 w, b are to be learned\n\n\u2014\n\nSlide 7.1.10\nIn two dimensions, we can see the geometric interpretation of w and b. The vector w is perpendicular to the linear separator; such a vector is known as the normal vector. Often we say \"the vector normal to the surface\". The scalar b, which we will call the offset, is proportional to the perpendicular distance from the origin to the linear separator. The constant of proportionality is the negative of the magnitude of the normal vector. We'll examine this in more detail soon.\n\nBy the way, the choice of the letter \u201cw\u201d is traditional and meant to suggest \"weights\", we'll see why when we look at neural nets. The choice of \u201cb\u201d is meant to suggest \u201cbias\u201d - which is the third different connotation of this word in machine learning (the bias of a hypothesis class, bias vs variance, bias of a separator). They are all fundamentally related; they all refer to a difference from a neutral value. To keep the confusion down to a dull roar, we won\u2019t call b a bias term but are telling you about this so you won\u2019t be surprised if you see it elsewhere. \n\n\u2014\n\nSlide 7.1.11\nSometimes we will use the following trick to simplify the equations. We\u2019ll treat the offset as the 0th component of the weight vector w and we\u2019ll augment the data vector x with a 0th component that will always be equal to 1. Then we can write a linear equation as a dot product. When we do this, we will indicate it by using an overbar over the vectors.\n\n\u2014\n\nLinear Hypothesis Class\n\u2022 Equation of a hyperplane in the feature space\nw \u22c5 x + b = 0\n\u03a3_{j=1}^{n} w_j x_j + b = 0\n\u2022 w, b are to be learned\n\n\u2022 A useful trick: let x\u2080=1 and w\u2080=b\n"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_004.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.1.12 \nFirst a word on terminology: the equations we will be writing apply to linear separators in n dimensions. In two dimensions, such a linear separator is referred to as a \u201cline\u201d. In three dimensions, it is called a \u201cplane\u201d. These are familiar words. What do we call it in higher dimensions? The usual terminology is hyperplane. I know that sounds like some type of fast aircraft, but that's the accepted name.\n\nLet\u2019s look at the geometry of a hyperplane a bit more closely. We saw earlier that the offset b in the linear separator equation is proportional to the perpendicular distance from the origin to the linear separator and that the constant of proportionality is the magnitude of the w vector (negated). Basically, we can multiply both sides of the equation by any number without affecting the equality. So, there are an infinite set of equations all of which represent the same separator.\n\nIf we divide the equation through by the magnitude of w we end up with the situation shown in the figure. The normal vector is now unit length (denoted by the hat on the w) and the offset b is now equal to the perpendicular distance from the origin (negated).\n\nSlide 7.1.13\nIt's crucial to understand that the quantity w-hat dot x plus b is the perpendicular distance of point x to the linear separator.\n\nIf you recall, the geometric interpretation of a dot product a . b is that it is a number which is the magnitude of a times the magnitude of b times the cosine of the angle between the vectors. If one of the vectors, say a, has unit magnitude then we have is precisely the magnitude of the projection of the b vector onto the direction defined by a. Thus w-hat dot x is the distance from x to the origin measured perpendicular to the hyperplane.\n\nLooking at the right triangle defined by the w-hat and the x vector, both emanating from the origin, we see that the projection of x onto w-hat is the length of the base of the triangle, where x is the hypotenuse and theta as being as theta.\n\nNow, if we subtract out the perpendicular distance to the origin we get the distance of x from the hyperplane (rather than from the origin). Note that when theta is 90 degrees (that is, w, x and x are perpendicular), the cosine is equal to 0 and the distance is precisely b as we've expected.\n\n7.1.13\n\n|\\ \n| cos 0 x \n| \\ \n| perp distance to \n| \\ hyperplane \n| \\ \n| \\ \n| \\\\\n| \\\n\nSlide 7.1.14\nThis distance measure from the hyperplane is signed. It is zero for points on the hyperplane, it is positive for points in the side of the space towards which the normal vector points, and negative for points on the other side. Notice that if you multiply the normal vector w and the offset b by -1, you get an equation for the same hyperplane but you switch which side of the hyperplane has positive distances.\n\nSlide 7.1.15 \nWe can now exploit the sign of this distance to define a linear classifier, one whose decision boundary is a hyperplane. Instead of using 0 and 1 as the class labels (which was an arbitrary choice anyway) we use the sign of the distance, either +1 or -1 as the labels (that is the values of the y). \n\nw x_1 + b\nw x_2 + b \nperp distance\nis positive\n\nHyperplane: Geometry\n\\\n\\ w^T x + b \n / \\\\\n / \\\\\n\nsigned perpendicular\ndistance of point x to\nhyperplane.\n\nperp\ndistance is\nnegative\n\nperp\ndistance is\nzero.\n\nrecall: a \u2022 b = |a||b| cos \u03b8\n\nLinear Classifier\nh(x) = sign(w \u2022 x + b) = sign(w^T x + b)\n\nx_1 w\n \noutputs +1 or -1"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_005.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nLinear Classifier\n\nMargin:\n\\(\\gamma = y \\cdot (w^T \\, x + b) = w^T \\, x + b)\\cdot y\\)\nproportional to perpendicular distance from hyperplane to point\n\\(\n\\begin{align*}\n\\gamma > 0 &: \\text{point is correctly classified (sign of distance = y)} \\\\\n\\gamma < 0 &: \\text{point is incorrectly classified (sign of distance \\neq y)}\n\\end{align*}\n\\)\n\nSlide 7.1.16\nA variant of the signed distance of a training point to a hyperplane is the **margin** of the point. The margin (gamma) is the product of the actual signed distance for the point and the desired sign of the distance, y. If they agree (the point is correctly classified), then the margin is positive; if they disagree (the classification is in error), then the margin is negative.\n\n---\n\n6.034 Notes: Section 7.2\n\nSlide 7.2.1\nSo far we've talked about how to represent a linear hypothesis but not how to find one. In this slide is the perceptron algorithm, developed by Rosenblatt in the mid 50's. This is not exactly the original form of the algorithm but it is equivalent and it will help us later to see it in this form.\n\nThis is a greedy, \"mistake driven\" algorithm not unlike the Boolean function learning algorithms we saw earlier. We will be using the extended form of the weight and data-point vectors in this algorithm. The extended weight vector is what we are trying to learn.\n\nThe first step is to start with an initial value of the weight vector, usually all zeros. Then we repeat the inner loop until all the points are correctly classified using the current weight vector. The inner loop is to consider each point. If the point's margin is positive then it is correctly classified and we do nothing. Otherwise, if it is negative or zero, we have a mistake and we want to change the weights so as to increase the margin (so that it ultimately becomes positive).\n\nThe trick is how to change the weights. It turns out that using a value proportional to \\(y x\\) is the right thing. We'll see why, formally, later. For now, let's convince ourselves that it makes sense.\n\nSlide 7.2.2\nConsider the case in which y is positive; the negative case is analogous. If the jth component of x is positive then we will increase the corresponding component of w. Note that the resulting effect on the margin is positive. If the jth component of x is negative then we will decrease the corresponding component of w, and the resulting effect on the margin is also positive.\n\n**Perceptron Algorithm**\n**Rosenblatt, 1956**\n  - Pick initial weight vector (including b), e.g.\n  \\([0 \\, 0]\\)\n  - Repeat until all points correctly classified\n    - Repeat for each point\n      - Calculate margin (\\(y \\, w^T x\\)) for point i\n      - If margin > 0, point is correctly classified\n      - Else change weights to increase margin;\n        change in weight proportional to \\(y x\\)\n        \n  - Note that, if y = 1\n    - If w \\(\\cdot\\) x \\(>0\\) then w, increased (increases margin)\n    - If x \\(<0\\) then w decreased (increases margin)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_006.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.2.3\nSo, each change of w increases the margin on a particular point. However, the changes for the\ndifferent points interfere with each other, that is, different points might change the weights in\nopposing directions. So, it will not be the case that one pass through the points will produce a\ncorrect weight vector. In general, we will have to go around multiple times.\n\nThe remarkable fact is that the algorithm is guaranteed to terminate with the weights for a separating\nhyperplane as long as the data is linearly separable. The proof of this fact is beyond our scope.\n\nNotice that if the data is not separable, then this algorithm is an infinite loop. It turns out that it is a\ngood idea to keep track of the best separator you've seen so far (the one that makes the fewest\nmistakes) and after you get tired of going around the loop, return that one. This algorithm even has a\nname (the pocket algorithm: see, it keeps the best answer in its pocket...).\n\nPerceptron Algorithm \nRosenblatt, 1956\n- Pick initial weight vector (including b), e.g.\n[0 -1 0]\n\n- Repeat for each point\n \u2713 classify point correctly classified\n \u2717 if point is correctly\nclassified increasing \n\nElse guess y* to incorrect margins change weights\n\nChange in weight perceptually classified\nfind separating point\nIf xj > 0 then n increased (increases margin)\n- Until entire otherwise, increments by x* Missclassified, it smallest number, j,\n0 in tsep y, same before zero corrects being in the loop if error occurs, separately find \n\nSlide 7.2.4\nThis shows a trace of the perceptron algorithm on the bankruptcy data. Here it took 49 iterations\nthrough the data (the outer loop) for the algorithm to stop. The hypothesis at the end of each loop is\nshown here. Recall that the first element of the weight vector is actually the offset. So, the normal\nweight to the separating hyperplane is [0.94 0.44] and the offset is -0.22 (recall that it is proportional to\nthe negative perpendicular distance from origin to the line).\n\nNote that the units in the horizontal and vertical directions in this graph are not equal (the tick marks\nalong the axes indicate unit distances). We did this since the range of the data on each axis is so\ndifferent.\n\nOne usually picks some small \u201crate\u201d constant to scale the change to w. It turns out that for this\nalgorithm the value of the rate constant does not matter. We have used .01 in our examples, but 1\nalso works well.\n\nSlide 7.2.5\nLet\u2019s revisit the issue of why we picked yx to increment w in the perceptron algorithm. It might have\nseemed arbitrary but it\u2019s actually an instance of a general strategy called gradient ascent for finding\nthe input(s) that maximize a function\u2019s output (or gradient descent when we are minimizing).\n\nThe strategy in one input dimension is shown here. We guess an initial value of the input. We\ncalculate the slope of the function at that input value and we take a step that is proportional to the\nslope. Note that the sign of the slope will tell us whether an increase of the input variable will\nincrease or decrease the value of the output. The magnitude of the slope will tell us how fast the\nfunction is changing at that input value. The slope is basically a linear approximation of the function\nwhich is valid \"near\" the chosen input value. Since the approximation is only valid locally, we want\nto take a small step (determined by the rate constant \uf068) and repeat.\n\nWe want to stop when the output change is zero (or very small). This should correspond to a point\nwhere the slope is zero, which should be a local extremum of the function. This strategy will not\nguarantee finding the global maximal value, only a local one.\n\nGradient Ascent/Descent\n- To maximize f(w)\n\u2022 Pick initial w\n\u2022 Change w to w + \uf068 df/dw (\uf068 > 0, small)\n\u2022 until f stops changing (df/dw = 0)\n\u2022 Finds local maximum if function is globally convex\n- To minimize f(w)\n\nSlide 7.2.6\nThe generalization of this strategy to multiple input variables is based on the generalization of the\nnotion of slope, which is the gradient of the function. The gradient is the vector of first (partial)\nderivatives of the function with respect to each of the input variables. The gradient vector points in\nthe direction of steepest increase of the function output. So, we take a small step in that direction,\nrecompute the gradient and repeat until the output stops changing. Once again, this will only find us\na local maximum of the function, in general. However, if the function is globally convex, then it will\nfind the global optimum."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_007.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.2.7\nIn general, the choice of the rate constant (eta), which determines the step size, is fairly critical. Unfortunately, no single value is appropriate for all functions. If one chooses a very conservative small rate, it can take a long time to find a minimum; if one takes too big steps there is no guarantee that the algorithm will even converge to a minimum; it can oscillate as shown in the figure here where the sign of the slope changes and causes a back-and-forth search.\n\nIn more sophisticated search algorithms one does a search along the specified direction looking for a value of the step size that guarantees an increase in the function value.\n\nGradient Ascent/Descent\n- To maximize f(w)\n  - Pick initial w\n  - Change w to w + n \u2207_w f   (n > 0, small) until f stops changing (\u2207_w f = 0)\n  - Finds local maximum; global maximum if function is globally convex\n- Rate (n) has to be chosen carefully.\n  - Too small \u2014 slow convergence\n  - Too big \u2014 oscillation\n\nSlide 7.2.8\nNow we can see that our choice of increment in the perceptron algorithm is related to the gradient of the sum of the margins for the misclassified points.\n\nPerceptron Training via Gradient Descent\n- Maximize sum of margins of misclassified points\n  f(w) = \u03a3_(misclassified) (w^T x)\n\nSlide 7.2.9\nIf we actually want to maximize this sum via gradient descent we should sum all the corrections for every misclassified point using a single w vector and then apply that correction to get a new weight vector. We can then repeat the process until convergence. This is normally called an off-line algorithm in that it assumes access to all the input points.\n\nWhat we actually did was a bit different, we modified w based on each point as we went through the inner loop. This is called an on-line algorithm because, in principle, if the points were arriving over a communication link, we would make our update to the weights based on each arrival and we could discard the points after using them, counting on more arriving later.\n\nAnother way of thinking about the relationship of these algorithms is that the on-line version is using a (randomized) approximation to the gradient at each point. It is randomized in the sense that rather than taking a step based on the true gradient, we take a step based on an estimate of the gradient based on a randomly drawn example point. In fact, the on-line version is sometimes called \u201cstochastic (randomized) gradient ascent\u201d for this reason. In some cases, this randomness is good because it can get us out of shallow local minima.\n\nPerceptron Training via Gradient Descent\n- Maximize sum of margins of misclassified points\n  f(w) = \u03a3_(all x)    \u03a3_(misclassified) (w^T x_i)\n- Off-line training: Compute gradient as sum over all training points.\n- On-line training: Approximate gradient by one of the terms in the sum: \u03a3_(all x) w^T x\n\nSlide 7.2.10\nHere\u2019s another look at the perceptron algorithm on the bankruptcy data with a different initial starting guess of the weights. You can see the different separator hypotheses that it goes through. Note that it converges to a different set of weights from our previous example. However, recall that one can scale these weights and get the same separator. In fact these numbers are approximately 0.8 of the ones we got before, but only approximately; this is a slightly different separator.\n\nThe perceptron algorithm can be described as a gradient ascent algorithm, but its error criterion is slightly unusual in that there are many separators that all have zero error.\n\n[Figure Caption]\nPerceptron Algorithm Bankruptcy Data\nRate = 0.1\nInitial Guess:\n  L = [ 1.7 0.8 1.3 ]\n  T = [ -1.7 0.8 0.3 ]\n\n[Graphs in figures]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_008.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.2.11\nRecall that the perceptron algorithm starts with an initial guess for the weights and then adds in scaled versions of the misclassified training points to get the final weights. In this particular set of 10 iterations, the points indicated on the left are misclassified some number of times each. For example, the leftmost negative point is misclassified in each iteration except the last one. If we sum up the coordinates of each of these points, scaled by how many times each is misclassified and by the rate constant we get the total change in the weight vector.\n\nSlide 7.2.12\nThis analysis leads us to a somewhat different view of the perceptron algorithm, usually called the dual form of the algorithm. Call the count of how many times point i is misclassified, alpha_i. Then, assuming the weight vector is initialized to 0s, we can write the final weight vector in terms of these counts and the input data (as well as the rate constant).\n\nSlide 7.2.13\nSince the rate constant does not change the separator we can simply assume that it is 1 and ignore it.\nNow, we can substitute this form of the weights in the classifier and we get the classifier at the bottom of the slide, which has the interesting property that the data points only appear in dot-products with other data points. This will turn out to be extremely important later; file this one away.\n\nSlide 7.2.14\nWe can now restate the perceptron algorithm in this interesting way. The separator is described as a weighted sum of the input points, with alpha_i the weight for point i. Initially, set all of the alphas to zero, so the separator has all zero's as coefficients.\n\nThen, for each point, compute its margin with respect to the current separator. If the margin is positive, the point is classified correctly, so we do nothing. If the margin is negative, add that point into the weights of the separator. We can do that simply by incrementing the associated alpha.\n\nFinally, when all of the points are classified correctly, we return the weighted sum of the inputs as the coefficients to the separator. Note that if the data is not linearly separable, then the algorithm will loop forever, the alphas growing without bound.\n\nYou should convince yourself that this dual form is equivalent to the original. Once again, you may be wondering\u2026so what? I'll say again; file this away. It has surprising consequences.\n\nPerceptron Algorithm\nBankruptcy Data\nrate \u03b7 = 0.1\n(\u22121.0 \u2013 (0.1))\n(\u2212 (0.1) + 7.1)\n3x (0.1) + 6x \u2013 1.0 + 0.6 = 1.7\n5x (0.1) + 0.6 + 1.1 = 0.4\n\nAssume initial weights are 0:  \nrate = \u03b7 \n\n5 x (0,1,0)      + 6 x (1,1,0) + 3 x (0,0,1)\n-1 x (0,1,-1) + CURRENT\n\u03b11 = count of\nmistakes on point i\nduring training\nfinal weights\n\u03b1i just scales\nanswer, set to 1\n\nDual Form\nAssume initial weights are 0\n\u03b11 = count of\nmistakes on point i\nduring training\n0 = i * sign(h(x) = 2\nIf \u03b1i = 0, i is not used\n\n5 x 0 (1,0,0)\n6 x (1,1,0)\n3 x 0 (0,0,1)\n1 x 0 (0,1,-1)\n4 x 0 (1,0,0)\n5 x (0,0, 0)\n3 x 0 (0,0,1)\n1 x 0\nh(x0) = sign(w x, x0) = sign(\u03b1i x0, yi * x0)\n\nPerceptron Training\nDual Form\n* \u03b1 = 0\n* Repeat until all points correctly classified\n    * Repeat for each point i\n        * Calculate margin \u2211j=1 to m yj * xj xi\n        * If margin > 0, point i is correctly classified\n        * Else increment \u03b1i\n* Return w = \u2211j=1 to m \u03b1j yj xj\n* If data is not linearly separable, the \u03b1 grow without bound\n\nFinal Answer...\n(1 - 0.81,0.3)\n0.6...\n0.4...\nInitial Guess:\n= j+1 (0.7) = j-1 (0.3)\n        ..."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_009.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 7.3\n\nSlide 7.3.1\nWe will now turn our attention to artificial neural nets, sometimes also called \"feedforward nets\".\n\nThe basic idea in neural nets is to define interconnected networks of simple units (let's call them \"artificial neurons\") in which each connection has a weight. Weight w_{ij} is the weight of the jth input into unit i. The networks have some inputs where the feature values are placed and they compute one or more output values. The learning takes place by adjusting the weights in the network so that the desired output is produced whenever a sample in the input data set is presented.\n\nSlide 7.3.2\nWe start by looking at a simpler kind of \"neural-like\" unit called a perceptron. This is where the perceptron algorithm that we saw earlier came from. Perceptrons antedate the modern neural nets. Examining them can help us understand how the more general units work.\n\nSlide 7.3.3\nA perceptron unit basically compares a weighted combination of its inputs against a threshold value and then outputs a 1 if the weighted inputs exceed the threshold. We use our trick here of treating the (arbitrary) threshold as if it were a weight (w0) on a constant input (x0) whose value is -1 (note the sign is different from what we saw in our previous treatment but the idea is the same). In this way, we can write the basic rule of operation as computing the weighted sum of all the inputs and comparing to 0.\n\nThe key observation is that the decision boundary for a single perceptron unit is a hyperplane in the feature space. That is, it is a linear equation that divides the space into two half-spaces. We can easily see this in two dimensions. The equation that tells us when the perceptron\u2019s total input goes to zero is the equation of a line whose normal is the weight vector [w1, w2]. On one side of this line, the value of the weighted input is negative and so the perceptron\u2019s output is 0, on the other side of the line the weighted input is positive and the output is 1.\n\nWe have seen that there\u2019s a simple gradient-descent algorithm for finding such a linear separator if one exists.\n\n[Diagram \u2013 Left: Single Perceptron Unit]\n[Diagram \u2013 Right: Linear Classifier Single Perceptron Unit]\n\nArtificial Neural Networks (Feedforward Nets)\n\n-1          W_{01}\n\nW_{11}  W_{21}\n\nW_{12}  W_{22}\n\nW_{02}\n\nX1\n\nX2\n\nX3\n\nW_{01}  \n\nW_{11}\n\nW_{21}\n\nX0\n\nW_{02}  \n\nW_{12}\n\nW_{22}\n\nX1\n\nX2\n\nSingle Perceptron Unit\n\nLinear Classifier Single Perceptron Unit\n\nh(x) = \u03b8(w \u00b7 x + b) = \u03b8(w \u00b7 x)\n\n\u03b8(z) = [\\begin{array}{cc} 1 & z \u2265 0 \\\\ 0 & \\text{else} \\end{array}]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_010.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.3.4\nSince a single perceptron unit can only define a single linear boundary, it is limited to solving linearly separable problems. A problem like that illustrated by the values of the XOR boolean function cannot be solved by a single perceptron unit.\n\nBeyond Linear Separability\n\nSlide 7.3.5\nWe have already seen in our treatment of SVMs how the \"kernel trick\" can be used to generalize a perceptron-like classifier to produce arbitrary boundaries, basically by mapping into a high-dimensional space of non-linear mappings of the input features.\n\nBeyond Linear Separability\nNot linearly separable\n\nSlide 7.3.6\nWe will now explore a different approach (although later we will also introduce non-linear mapping). What about if we consider more than one linear separator and combine their outputs; can we get a more powerful classifier?\n\nSlide 7.3.7\nThe answer is yes. Since a single perceptron unit is so limited, a network of these units will be less limited. In fact, the introduction of \"hidden\" (not connected directly to the output) units into these networks make them much more powerful; they are no longer limited to linearly separable problems.\n\nWhat these networks do is basically use the earlier layers (closer to the input) to transform the problem into more tractable problems for the latter layers.\n\nMulti-Layer Perceptron\nLower layers transform the input problem into more tractable (linearly separable) problems for subsequent layers.\n\nMore powerful than single layer."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_011.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.3.8:\n\nTo see how having hidden units can help, let us see how a two-layer perceptron network can solve the XOR problem that a single unit failed to solve.\n\nWe see that each hidden unit defines its own \u201cdecision boundary\u201d and the output from each of these units is fed to the output unit, which returns a solution to the whole problem. Let's look in detail at each of these boundaries and its effect.\n\nNote that each of the weights in the first layer, except for the offsets, has been set to 1. So, we know that the decision boundaries are going to have normal vectors equal to [1 1], that is, pointing up and to the right, as shown in the diagram. The values of the offsets show that the hidden unit labeled o1 has a larger offset (that is, distance from the origin) and the hidden unit labeled o2 has a smaller offset. The actual distances from the line to the origin are obtained by dividing the offsets by sqrt(2), the magnitude of the normal vectors.\n\nIf we focus on the first decision boundary we see only one of the training points (the one with feature values (1,1)) is in the half space that the normal points into. This is the only point with a positive distance and thus a one output from the perceptron unit. The other points have negative distance and produce a zero output. This is shown in the shaded column in the table.\n\nSlide 7.3.9:\n\nLooking at the second decision boundary we see that three of the training points (except for the one with feature values (0,0)) are in the half space that the normal points into. These points have a positive distance and thus a one output from the perceptron unit. The other point has negative distance and produces a zero output. This is shown in the shaded column in the table.\n\nSlide 7.3.10:\n\nOn the lower right, we see that the problem has been mapped into a linearly separable problem in the space of the outputs of the hidden units. We can now easily find a linear separator, for example, the one shown here. This mapping is where the power of the multi-layer perceptron comes from.\n\nSlide 7.3.11:\n\nIt turns out that a three-layer perceptron (with sufficiently many units) can separate any data set. In fact, even a two-layer perceptron (with lots of units) can separate almost any data set that one would see in practice.\n\nHowever, the presence of the discontinuous threshold in the operation means that there is no simple local search for a good set of weights; one is forced into trying possibilities in a combinatorial way.\n\nThe limitations of the single-layer perceptron and the lack of a good learning algorithm for multi-layer perceptrons essentially killed the field of statistical machine learning for quite a few years. The stake through the heart was a slim book entitled \u201cPerceptrons\u201d by Marvin Minsky and Seymour Papert of MIT.\n\nMulti-Layer Perceptron Learning\n\n\u2022 Any set of training points can be separated by a three-layer perceptron network.\n\u2022 \u201cAlmost any\u201d set of points separable by two-layer perceptron network.\n\u2022 But, no efficient learning rule is known."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_012.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nMulti-Layer Perceptron Learning  \n- Any set of training points can be separated by a three-layer perceptron network.  \n- **Almost any** set of points separable by two-layer perceptron network.  \n- But, no efficient learning rule is known.  \n- Could we use gradient ascent/descent?  \n  - We would need smoothness: small change in weights produces small change in output.  \n  - Threshold function is not smooth.\n\nSlide 7.3.12\nA natural question to ask is whether we could use gradient descent to train a multi-layer perceptron. The answer is that we can't as long as the output is discontinuous with respect to changes in the inputs and the weights. In a perceptron unit it doesn't matter how far a point is from the decision boundary, you will still get a 0 or a 1. We need a smooth output (as a function of changes in the network weighs) if we're to do gradient descent.\n\nSlide 7.3.13\nEventually people realized that if one \"softened\" the thresholds, one could get information as to whether a change in the weights was helping or hurting and define a local improvement procedure that way.\n\nMulti-Layer Perceptron Learning\n- Any set of training points can be separated by a three-layer perceptron network.\n- **Almost any** set of points separable by two-layer perceptron network.\n- But, no efficient learning rule is known.  \n- Could we use gradient ascent/descent?\n  - We would need smoothness: small change in weights produces small change in output.\n  - Threshold function is not smooth.\n  - Use a smooth threshold function!\n\nSlide 7.3.14\nThe classic \"soft threshold\" that is used in neural nets is referred to as a \"sigmoid\" (meaning S-like) and is shown here. The variable z is the \"total input\" or \"activation\" of a neuron, that is, the weighted sum of all of its inputs.\n\nNote that when the output (z) is 0, the sigmoid's value is 1/2. The sigmoid is applied to the weighted inputs (including the threshold value as before). There are actually many different types of sigmoids that can be (and are) used in neural networks. The sigmoid shown here is actually called the logistic function.\n\nSlide 7.3.15\nWe can think of a sigmoid unit as a \"soft\" perceptron. The line where the perceptron switches from a 0 output to a 1, is now the line along which the output of the sigmoid unit is 1/2. On one side of this line, the output tends to 0, on the other it tends to 1.\n\nSo, this \"logistic perceptron\" is still a linear separator in the input space. In fact, there\u2019s a well known technique in statistics, called logistic regression which uses this type of model to fit the probabilities of boolean-valued outputs, which are not properly handled by a linear regression. Note that since the output of the logistic function is between 0 and 1, the output can be interpreted as a probability.\n\nFigure (Sigmoid Unit Illustration)\n- Equation: z = \u03a3 w\u1d62 x\u1d62, s(z) = 1/(1 + e^(-z))\n- Diagram showing inputs x1, x2, ..., x_n feeding into the unit."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_013.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Slide 7.3.16**\n\nThe key property of the sigmoid is that it is differentiable. This means that we can use gradient-based methods of minimization for training. Let's see what that means.\n\nThe output of a multi-layer net of sigmoid units is a function of two vectors, the inputs (x) and the weights (w \\[k\\]). An example of what that function looks like for a simple net is shown along the bottom, where S(w, x) is whatever output function we are using, for example, the logistic function we saw in the last slide.\n\nThe output of this function (y) varies smoothly with changes in the input and, importantly, with changes in the weights. In fact, the weights and inputs both play similar roles in the function.\n\n[Diagram: Network and Formula]\n\n*Training*\ny(x, w)\nx is a vector of inputs\n\nz1 = s(tw2T(w1T x, w2, w3)) = w1,2s(w3*z) = W1*y2\n\n---\n\n**Slide 7.3.17**\n\nGiven a dataset of training points, each of which specifies the net inputs and the desired outputs, we can write an expression for the training error, usually defined as the sum of the squared differences between the actual output (given the weights) and the desired output. The goal of training is to find a weight vector that minimizes the training error.\n\nWe could also use the mean squared error (MSE), which simply divides the sum of the squared errors by the number of training points instead of just 2. Since the number of training points is a constant, the value of the minimum is not affected.\n\n[Diagram: Training Error Formula]\n\n*Training*\ny(x, w)\nx /% input vector\ny /% desired output\n\\[E = \\frac{1}{2} \\sum_{i} (y_i(x, w) - y_{i})^2 \\]\n\nOur goal is to find weight vector w that minimizes error E.\n\n---\n\n**Slide 7.3.18**\n\nWe've seen that the simplest method for minimizing a differentiable function is gradient descent (or ascent if you're maximizing).\n\nRecall that we are trying to find the weights that lead to a minimum value of training error. Here we see the gradient of the training error as a function of the weights. The descent rule is basically to change the weights by taking a small step (determined by the learning rate \u03b1) in the direction opposite this gradient.\n\nNote that the gradient of the error is simply the sum over all the training points of the error in the prediction for that point (given the current weights), which is the network output y minus the desired output y_i times the gradient of the network output for that input and weight combination.\n\n[Diagram: Gradient Descent Rule]\n\n\\[w' = w \u2212 \u03b7 \u25bd_w (E)\\]\n\n---\n\n**Slide 7.3.19**\n\nLet's look at a single sigmoid unit and see what the gradient descent rule would be in detail. We'll use the on-line version of gradient descent, that is, we will find the weight change to reduce the training error on a single training point. Thus, we will be neglecting the sum over the training points in the real gradient.\n\nAs we saw in the last slide, we will need the gradient of the unit's output with respect to the weights, that is, the vector of changes in the output due to a change in each of the weights.\n\nThe output (y) of a single sigmoid unit is simply the output of the sigmoid function for the current activation (that is, total weighted input) of the unit. So, this output depends both on the values of the input features and the current values of the weights.\n\nThe gradient of this output function with respect to any of the weights can be found by an application of the chain rule of differentiation. The derivative of y with respect to w can be written as the product of the derivative with respect to z (the total activation) times the derivative of z with respect to the weight. The first term is the slope of the sigmoid function for the given input and weights, which we can write as ds(z)/dz. In this simple situation the total activation is a linear function of the weights, each with a coefficient corresponding to a feature value, x_k, for weight w_p.\n\n[Diagram: Gradient Descent Single Unit]\n\n\\[\\frac{1}{2} \\sum (y_i(x, w) - y_i)^2 \\]\nError on Training Set\nSquared Error\nGradient Error\nGradient Descent\n\n\u25bd_w E\n[y'_w = y - \u03b7 \u25bd_w (E)]"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_014.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSo, the derivative of the activation with respect to the weight is just the input feature value, x\u2096.\n\n**Gradient Descent Single Unit**  \n\nSlide 7.3.20\n\nNow, we can substitute this result into the expression for the gradient descent we found before (for a single point).\n\nWe will define a new quantity called delta, which is defined to be the derivative of the error with respect to a change in the activation Z. We can think of this value as the \"sensitivity\" of the network output to a change in the activation of a unit.\n\nThe important result we get is that the change in the ith weight is proportional to delta times the ith input. This innocent looking equation has more names than you can shake a stick at: the delta rule, the LMS rule, the Widrow-Hoff rule, etc. Or you can simply call it the chain rule applied to the squared training error.\n\nSlide 7.3.21\n\nThe derivative of the sigmoid plays a prominent role in these gradients, not surprisingly. Here we see that this derivative has a very simple form when expressed in terms of the output of the sigmoid. Then, it is just the output times 1 minus the output. We will use this fact liberally later.\n\n**Derivative of the sigmoid**\n\ns(z) = \\( \\frac{1}{1 + e^{-z}} \\)\n\n\\( \\frac{ds}{dz} = \\frac{1}{(1 + e^{-z})^2} \\cdot e^{-z} \\)\n\n\\( = \\Bigg[ \\frac{1}{1 + e^{-z}} \\Bigg] \\Bigg[ 1 - \\frac{1}{1 + e^{-z}} \\Bigg] \\)\n\n= s(z)(1 - s(z))\n\nSlide 7.3.22\n\nNow, what happens if the input to our unit is not a direct input but the output of another unit and we\u2019re interested in the rate of change in y in response to a change to one of the weights in this second unit?\n\nSlide 7.3.23\n\nWe use the chain rule again but now the change in the activation due to a change in the weight is a more complex expression: it is the product of the weight on the input times the rate of change in the output of the lower unit with respect to the weight. Notice that this new term is exactly of the same form as the one we are computing.\n\nGradient of Unit Output\n\n\\( \\nabla_w y \\) \\[ \\frac{y}{\\partial w_i} \\]\n\nz = \\( \\sum w_ix_i \\)\n\ns(z) = \\( \\frac{1}{1 + e^{-z}} \\)\n\n\\( \\frac{y}{\\partial w_i} \\)  \n\nGradient of Unit Output\n\nz = \\( \\sum w_ix_i, \\ \\ \\ \\ \\  y = s(z) = \\frac{1}{1 + e^{-z}} \\) \\[ \\ \\ \\[ \\frac{ds}{dz} \\ \\ \\[ \\frac{dz}{\\partial w_i} \\ \\ \\]\n\ns(z)(1-s(z)) \\times x_i\n\n\\( \\frac{y}{\\partial w_i} \\ = \\ 1 \\)  \n\ns(z)(1-s(z))"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_015.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.3.24\nGradient of Unit Output\nWe've just set up a recursive computation for the dy/dw terms. Note that these terms will be products of the slopes of the output sigmoid for the units times the weight on the input times a term of similar form for units below the input, until we get to the input with the weight we are differentiating with respect to. In the base case, we simply have the input value on that line, which could be one of the x\u1d62 or one of the y\u1d62, since clearly the derivative of any unit with respect to w\u1d62 \"below\" the line with that weight will be zero.\n\n[Image of Base Case and Recursion diagram]\n\n\u03a3 w\u1d62y\u1d62\ndy/dw\u1d62 = \u2014\u2014\u2014\u2014 = z\n\u2211 w\u1d62y\u1d62\n\nBase Case\n\nSlide 7.3.25\nLet's see how this works out for the simple case we've looked at before. There are two types of weights, the ones on the output unit, of the form, w\u2098\u2083. And the weights on the two lower level units, w\u2081\u2082 and w\u2082\u2082. The form of dy/dw for each of these two weights will be different as we saw in the last slide.\n\n[Diagram of unit connections with weights]\n\nGradient of Error\n\nSlide 7.3.26\nRecall that in the derivative of the error (for a single instance) with respect to any of the weights, we get a term that measures the error at the output (y*-y') times the change in the output which is produced by the change in the weight (dy/dw).\n\nGradient of Error\n\nE = \u00bd\u03a3 (y*<sub>i</sub> ('x<sub>i</sub>',w) - y<sub>i<sub>)\u00b2\n= \u03a3 (y* - y') (dy) (dy)\n\nz = y ([s*(w<sub>11</sub>x<sub>1</sub> + w<sub>12</sub>x<sub>2</sub> + w<sub>13</sub>x<sub>3</sub>) + w<sub>23</sub> (w<sub>21</sub>x<sub>1</sub> + w<sub>22</sub>(x<sub>2</sub> - w<sub>13</sub>) = -w<sub>23</sub>)\n\nSlide 7.3.27\nLet's pick weight w<sub>13</sub>, that weights the output of unit 1 (y<sub>1</sub>) coming into the output unit (unit 3). What is the change in the output y<sub>3</sub> as a result of a small change in w<sub>13</sub>? Intuitively, we should expect it to depend on the value of y<sub>1</sub>, the \"signal\" on that wire since the change in the total activation when we change w<sub>13</sub> is scaled by the value of y<sub>1</sub>. If y<sub>1</sub> were 0 then changing the weight would have no effect on the unit's output.\n\nChanging the weight changes the activation, which changes the output. Therefore, the impact of the weight change on the output depends on the slope of the output (the sigmoid output) with respect to the activation. If the slope is zero, for example, then changing the weight causes no change in the output.\n\nWhen we evaluate the gradient (using the chain rule), we see exactly what we expect -- the product of the sigmoid slope (dy/dz) times the signal value y<sub>1</sub>.\n\n[Diagram of Gradient of Error]\n\n\u2202E\n\u2014 = \u2014 (y\u207a - y') \u2202y\u2081\n\u2202w  \u2202y\u2081 \u2202w\u1e8d\n\n      MIT 7.3.27 42"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_016.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nGradient of Error\n\n$E = \\frac{1}{2}\\sum_i ({y_i^{(k_{out})} - y_i})^2$\n\n$y_3 = st(W_{31}x_1 + W_{32}x_2 + W_{33}x_3)$\n\n$y_5 = st(W_{54}y_4 + W_{55}y_5 = W_{36}x_3)$\n\n$ (dy_i - y_i) \\frac{\\partial y_i}{\\partial W_{ij}}$\n\n$x_6 = x_3$\n\nSlide 7.3.28\n\nWhat happens when we pick a weight that's deeper in the net, say $w_{12}$? Since that weight affects $y_1$, we expect that the change in the final output will be affected by the value of $w_{13}$ and the slope of the sigmoid at unit 3 (as when we were changing $w_{13}$). In addition, the change in $y_1$, will depend on the value of the signal on the wire $(x_1)$ and the slope of the sigmoid at unit 1. Which is precisely what we see.\n\nNote that in computing the gradients deeper in the net we will use some of the gradient terms closer to the output. For example, the gradient for weights on the inputs to unit 1 change the output by changing one input to unit 3 and so the final gradient depends on the behavior of unit 3. It is the realization of this reuse of terms that leads to an efficient strategy for computing the error gradient.\n\nSlide 7.3.29\n\nThe cases we have seen so far are not completely general in that there has been only one path through the network for the change in a weight to affect the output. It is easy to see that in more general networks there will be multiple such paths, such as shown here.\n\nThis means that a weight can affect more than one of the inputs to a unit, and so we need to add up all the effects before multiplying by the slope of the sigmoid.\n\nGradient of Unit Output\n\n\\( \\frac{\\partial E}{\\partial y_n} = (y^o - y_n) \\)\n\n\\( \\frac{\\partial y_n}{\\partial Net_n) = \\frac{\\partial st(net_n)}{\\partial net_n} = sn \\)\n\n\\( \\frac{\\partial}{\\partial Net_n^t} = (y^o - y_n)s_n \\)\n\n\\( = s_n\\sum_{inputs}W (y^o - y_n) = \\partial E/\\partial y \\)\n\nSlide 7.3.30\n\nGeneralized Delta Rule\n\nIn general we will be looking at networks connected in the fashion shown on the left, where the output of every unit at one level is connected to an input of every unit at the next level. We have not shown the bias inputs for each of the units, but they are there!\n\nA word on notation. To avoid having to spell out the names of all the weights and signals in these networks, we will give each unit an index. The output of unit i is $y_i$. We will specify the weights on the inputs of unit i as $w_{ki}$ where is either the index of one of the inputs or another unit. Because of the feedforward connectivity we have adopted this terminology is unambiguous.\n\nSlide 7.3.31\n\nIn this type of network we can define a generalization of the delta rule that we saw for a single unit. We still want to define the sensitivity of the training error (for an input point) to a change in the total activation of a unit. This is a quantity associated with the unit, independent of any weight. We can express the desired change in a weight that feeds into unit k as (negative of) the product of the learning rate, delta, for unit k and the value of the input associated with that weight.\n\nThe tricky part is the definition of delta. From our investigation into the form of dy/dw, the form of delta in the pink box should be plausible: the product of the slope of the output sigmoid times the sum of the products of weights and other deltas. This is exactly the form of the dy/dw expressions we saw before.\n\nThe clever part here is that by computing the deltas starting with that of the output unit and moving backward through the network we can compute all the deltas for every unit in the network in one pass (once we've computed all the y's and z's during a forward pass). It is this property that has led to the name of this algorithm, namely backpropagation.\n\nIt is important to remember that this is still the chain rule being applied to computing the gradient of the error. However, the computations have been arranged in a clever way to make computing the"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_017.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.3.32\nBackpropagation\nAn efficient method of implementing gradient descent for neural\nSlide 7.3.32\nThus, the algorithm for computing the gradients we need to update the weights of a net, with maximal re-using of intermediate results is known as backpropagation.\n\nThe two simple formulas we need are the ones we have just seen. One tells us how to change a weight. This is a simple gradient descent formula, except that it says that the gradient of the error is of the form delta, times y; where y_i is the signal on the wire with this weight, so it is either one of the inputs to the net or an output of some unit.\n\nThe delta of one unit is defined to be the slope of the sigmoid of that unit (for the current value of z, the weighted input) times the weighted sum of the deltas for the units that this unit feeds into.\n\nSlide 7.3.33\nThe backprop algorithm starts off by assigning random, small values to all the weights. The reason we want to have small weights is that we want to be near the approximately linear part of the sigmoid function, which happens for activations near 0. We want to make sure that (at least initially) none of the units are saturated, that is, are stuck at 0 or 1 because the magnitude of the total input is too large (positive or negative). If we get saturation, the slope of the sigmoid is 0 and there will not be any meaningful information of which way to change the weight.\n\nSlide 7.3.34\nBackpropagation\nAn efficient method of implementing gradient descent for neural networks\n\n\\[ w'_ij = w_ij - \u03b7 \\frac{dz_i}{dw_ij} \\]\nDescent Rule \"backprop\" rule\n\\[ \\frac{dz_i}{dw_ij} = y_i \\frac{dE}{dz_i} \\] \\[ \\frac{dE}{dz_i} = y_j \\] y_j is for input layer\n\n1. Initialize weights to small random values\n2. Choose a random sample input feature vector\n\nSlide 7.3.34\nNow we pick a sample input feature vector. We will use this to define the gradients and therefore the weight updates. Note that by updating the weights based on one input, we are introducing some randomness into the gradient descent. Formally, gradient descent on an error function defined as the sum of the errors over all the input instances should be the sum of the gradients over all the instances. However, backprop is typically implemented as shown here, making the weight change based on each feature vector. We will have more to say on this later.\n\nSlide 7.3.35\nNow that we have weights and inputs, we can do a forward propagation, that is, we can compute the values of all the z's and y's, that is, the weighted inputs and the outputs for all the units. We will need these values, so let's remember them for each unit.\n\n1. Initialize weights to small random values\n2. Choose a random sample input feature vector\n3. Compute total input (z_i) and output (y_j) for each unit (forward prop)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_018.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nSlide 7.3.36\nBackpropagation\nAn efficient method of implementing gradient descent for neural networks:\nW_ij' = W_ij - \u03b7 * \u2202Z/\u2202W_ij Descent rule\n\u2202Z/\u2202W_ij = \u2202z_i/\u2202x_i Backprop\n(W sum_k W_kj = x_i for input layer)\n\nNow, we start the process of computing the deltas. First we do it for the output units, using the formula shown here, that is, the product of the gradient of the sigmoid at the output unit times the error for that unit.\n\nSlide 7.3.37\nThen we compute the deltas for the other units at the preceding layer using the backprop rule.\n\nSlide 7.3.38\nWith the deltas and the unit outputs in hand, we can update the weights using the descent rule.\n\nBackpropagation\nAn efficient method of implementing gradient descent for neural networks:\n\n   W_ij' = W_ij - \u03b7 * \u2202Z/\u2202W_ij Descent rule\n   \u2202Z/\u2202W_ij = \u2202z_i/\u2202x_i Backprop   (W sum_k W_kj = x_i for input layer)\n1. Initialize weights to small random values\n2. Choose a random sample input feature vector\n3. Compute total input (z_j) and output (y_j) for each unit (forward prop)\n4. Compute \u03b4_k for output layer (back prop)\n   \u03b4_k = -\u2202z_k/\u2202x_k * (y_k^(i) - y_k)\n5. Compute \u03b4_j for all preceding layers by backprop rule\n6. Compute weight change by descent rule (repeat for all weights)\n\nSlide 7.3.39\nWe can see what is involved in doing the simple three-unit example we saw earlier. Here we see the simple expressions for the deltas and the weight update. Note that each expression involves data local to a particular unit, you don\u2019t have to look around summing things over the whole network, the delta\u2019s capture the recursion that we observed earlier. It is for this reason, simplicity, locality and, therefore, efficiency that backpropagation has become the dominant paradigm for training neural nets.\n\nAs mentioned before, however, the difficult choice of the learning rate and relatively slow convergence to a minimum are substantial drawbacks. Thousands of variations of backprop exist, aimed at addressing these drawbacks. More sophisticated minimization strategies, for example, do a search along the gradient direction (or related directions) to find a step that achieves a reduction in the error. Nevertheless, for these methods one still needs to derive the gradient of the network and a backprop-like computation can be used to do that.\n\nBackpropagation Example\nFirst do forward propagation:\nCompute z and y_j given x_i and W_ij\n\n   \u03b4_j = y_i^(i)(1-y_j)(y_j - y_j^i)\n\n   w e.g., = (1-y_1)^x_kx_j\n\nCompare to the direct derivation earlier.\n\nNote that all computations are local!"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_019.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n6.034 Notes: Section 7.4\n\nSlide 7.4.1\nNow that we have looked at the basic mathematical techniques for minimizing the training error of a neural net, we should step back and look at the whole approach to training a neural net, keeping in mind the potential problem of overfitting.  \n\nWe need to worry about overfitting because of the generality of neural nets and the proliferation of parameters associated even with a relatively simple net. It is easy to construct a net that has more parameters than there are data points. Such nets, if trained so as to minimize the training error without any additional constraints, can very easily overfit the training data and generalize very poorly. Here we look at a methodology that attempts to minimize that danger.\n\nTraining Neural Nets\nwithout overfitting, hopefully\u2026\n\nGiven: Data set, desired outputs and a neural net with weights. \nFind a setting for the weights that will give good predictive \nperformance on new data. Estimate expected performance on new \ndata.\n\nSlide 7.4.2\nThe first step (in the ideal case) is to separate the data into three sets. A training set for choosing the weights (using backpropagation), a validation set for deciding when to stop the training and, if possible, a separate set for evaluating the final results.\n\nTraining Neural Nets\nwithout overfitting, hopefully\u2026\n\nGiven: Data set, desired outputs and a neural net with weights.\nFind a setting for new weights that will give good predictive\nperformance on new data. Estimate expected performance on new\ndata.\n\n1.  Split data set (randomly) into three subsets:\n    + Training set \u2013 used for picking weights\n    + Validation set \u2013 used to stop training\n    + Test set \u2013 used to evaluate performance\n2.  Pick random, small weights as initial values\n\nSlide 7.4.3\nThen we pick a set of random small weights as the initial values of the weights. As we explained earlier, this reduces the chance that we will saturate any of the units initially.\n\nTraining Neural Nets\nwithout overfitting, hopefully\u2026\n\nGiven: Data set, desired outputs and a neural net with weights.\nFind a setting for new weights that will give good predictive\nperformance on new data. Estimate expected performance on new\ndata.\n\n1.  Split data set (randomly) into three subsets:\n    + Training set \u2013 used for picking weights\n    + Validation set \u2013 used to stop training\n    + Test set \u2013 used to evaluate performance\n2.  Pick random, small weights as initial values"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_020.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\nSlide 7.4.4:\nThen we perform the minimization of the training error, for example, using backpropagation. This will generally involve going through the input data and making changes to the weights many times. A common term used in this context is the epoch, which indicates how many times the algorithm has gone through every point in the training data. So, for example, one can plot the training error as a function of the training epoch. We will see this later.\n\n---\n\nSlide 7.4.5:\nAn important point is that we do not want to simply keep going until we reduce the training error to its minimum value. This is likely to overfit the training data. Instead, we can use the performance on the validation set as a way of deciding when to stop; we want to stop when we get best performance on the validation set. This is likely to lead to better generalization. We will look at this in more detail momentarily.\n\nThis type of \u201cearly termination\u201d keeps the weights relatively small. Keeping the weights small is a strategy for reducing the size of the hypothesis space. It's informally related to the idea of maximizing the margin by minimizing the magnitude of the weight vector in an SVM. It also reduces the variance of the hypothesis since it limits the impact that any particular data point can have on the output.\n\n---\n\nSlide 7.4.6:\nIn neural nets we do not have the luxury that we had in SVMs of knowing that we have found the global optimum after we finished learning. In neural nets, there are many local optima and backprop (or any other minimization strategy) can only guarantee finding a local optimum (and even this guarantee depends on careful choice of learning rate). So, it is often useful to repeat the training several times to see if a better result can be found. However, even a single round of training can be very expensive so this may not be feasible.\n\n---\n\nSlide 7.4.7:\nOnce we have a final set of weights, we can use them once on a held out test set to estimate the expected behavior on new data. Note the emphasis on doing this once. If we change the weights to improve this behavior, then we no longer have a held out set.\n\n---\n\nTraining Neural Nets\nGiven: Data set, desired outputs and a neural net with weights.\nFind a setting for the weights that will give good predictive performance on new data. Estimate expected performance on new data.\n\n1. Split data set (randomly) into three subsets:\n   Training set \u2014 used for picking weights\n   Validation set \u2014 used to stop training\n   Test set \u2014 used to evaluate performance\n\n2. Pick random, small weights as initial values\n\n3. Perform iterative minimization of error over training set.\n\n4. Stop when error on validation set reaches a minimum (to avoid over fitting).\n\n5. Repeat training (from step 2) several times (avoid local minima).\n\n6. Use best weights to compute error on test set, which is performance on new data. Do not repeat training to improve this.\n\n---\n\nTraining Neural Nets\nWithout overfitting, hopefully...\n\n1. Split data set (randomly) into three subsets:\n   Training set \u2014 used for picking weights\n   Validation set \u2014 used to stop training\n   Test set \u2014 used to evaluate performance\n\n2. Pick random, small weights as initial values\n\n3. Perform iterative minimization of error over training set.\n\n4. Stop when error on validation set reaches a minimum (to avoid over fitting).\n\n5. Repeat training (from step 2) several times (avoid local minima)"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_021.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n**Training Neural Nets**\nThe net worth overcoming. Hopefully.\n\n**Given:** Data set, desired outputs, and a neural net design.\n**Goal:** Find a setting of the weights that will give good predictive performance [on unseen data]\n\n1. Split data set (randomly) into three subsets:\n\n    Training set -- used to train network\n    Test set -- used to evaluate performance\n    Validation set -- used to terminate training\n\n2. Perform iterative minimization on training set\n    a. Iterate, trying to find global minimum (to avoid local)\n    b. Perform termination test, using validation set\n    c. Stop training when error of validation set increases and repeat training several times\n\n3. Use best weights to compute error on test set, and the data to\n    compare different architectures (or different training)\n\nCross-validation if data set is too small to divide into three\n\n----------------------\n\n**Slide 7.4.8**\n\nIn many cases, one doesn't have the luxury of having these separate sets, due to scarcity of data, in which case cross-validation may be used as a substitute.\n\n----------------------\n\n**Slide 7.4.9**\n\nLet's look at the termination/overfitting issue via some examples.\n\nHere we see the behavior of a small neural net (two inputs, two hidden units and one output) when trained on the data shown in the picture on the right. The white and black points represent 50 instances from each of two classes (drawn from Gaussian distributions with different centers). An additional 25 instances each (drawn from the same distributions) have been reserved as a test set.\n\nAs you can see, the point distributions overlap and therefore the net cannot fully separate the data. The red region represents the area where the net's output is close to 1 and the blue region represents where the output is close to 0. Intermediate colors represent intermediate values.\n\nThe error on the training net drops quickly at the beginning and does not drop much after that. The error on the test set behaves very similarly except that it is a bit bigger than the error on the training set. This is to be expected since the detailed placement of points near the boundaries will be different in the test set.\n\nThe behavior we see here is a good one; we have not overfit the data.\n\n----------------------\n\n**Slide 7.4.10**\n\nHere we see the behavior of a larger neural net (with 10 hidden units) on the same data.\n\nYou can see that the training error continues to drop over a much longer set of training epochs. In fact, the error goes up slightly sometimes, then drops, then stagnates and drops again. This is typical behavior for backprop.\n\nNote, however, that during most of the time that the training error is dropping, the test error is increasing. This indicates that the net is overfitting the data. If you look at the net output at the end of training, you can see what is happening. The net has constructed a baroque decision boundary to capture precisely the placement of the different instances in the training set. However, the instances in the test set are (very) unlikely to fall into that particular random arrangement.\n\nSo, in fact, all that extra work in fitting the training set is wasted. Note that the test error with this net is much higher than with the simpler net. If we had used a validation set, we could have stopped training before it went too far astray.\n\n----------------------\n\n**Slide 7.4.11**\n\nNote that this type of overfitting is not unique to neural nets. In this slide you can see the behavior of 1-nearest-neighbor and decision trees on the same data. Both fit it perfectly and produce classifiers that are just as unlikely to generalize to new data.\n\nFor K-nearest-neighbors, on this type of data one would want to use a value of K greater than 1. For decision trees one would want to prune back the tree somewhat. These decisions could be based on the performance on a held out validation set.\n\nSimilarly, for neural nets, one would want to choose the number of units and the stopping point based on performance on validation data.\n\n**Overfitting is not unique to neural nets...**\n- 1-Nearest Neighbor\n- Decision Trees"
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_022.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n---\n\n**Overfitting in SVM**\n\nSlide 7.4.12\n\nEven SVMs, which have relatively few parameters and a well-deserved reputation for being resistant to overfitting can overfit. In the center panel we see the output for a linear SVM. This is, in fact, the optimal type of separator for this data. On the upper left we see a fairly severe overfitting stemming from the choice of a too-small sigma for a radial kernel. On the right is the result from a larger choice of sigma (and a relatively high G), where the points are dense, this actually approaches the linear boundary but then deviates wildly in regions to an outlier near (2, 0). This illustrates how the choice of kernel can affect the generalization ability of an SVM classifier.\n\n---\n\nSlide 7.4.13\n\nWe mentioned earlier that backpropagation is an on-line training algorithm, meaning that it changes the weights for each input instance. Although this is not a \u201ccorrect\u201d implementation of gradient descent for the total training error, it is often argued that the randomization effect is beneficial as it tends to push the system out of some shallow local minima. In any case, the alternative \u201coff-line\u201d approach of actually adding up the gradients for all the instances and changing the weights based on that complete gradient is also used and has some advantages for smaller systems. For larger nets and larger datasets, the on-line approach has substantial performance advantages.\n\n**On-line vs Off-line**\n- There are two approaches to performing the error minimization:\n  - On-line training = present X and Y (chosen randomly from the training set). Change the weights/reduce error on this instance. Repeat.\n  - Off-line training = change weights to reduce total error on training set (sum over all instances).\n\n- On-line approach might produce a worse estimate for the gradient because an instance is \u201cnoisy\u201d relative to the full gradient based on all instances. This can be beneficial in terms of pushing the system through shallow local minima.\n\n---\n\n**Momentum**\n\nSlide 7.4.14\n\nWe have mentioned the difficulty of picking a learning rate for backprop that balances, on the one hand, the desire to move speedily towards a minimum by using a large learning rate and, on the other hand, the need to avoid overstepping the minimum and possibly getting into oscillations because of a too-large learning rate. One approach to balancing these is to effectively adjust the learning rate based on history. One of the original approaches for this is to use a momentum term in backprop.\n\nHere is the standard backprop gradient descent rule, where the change to the weights is proportional to delta and y.\n\n\\[ \\Delta w_{i,j} = - \\eta \\delta_j y_i \\]\n\n*Standard backprop descent*\n\n*Rewrite to define change in weights at time t*\n\n\\[ \\Delta w_{i,j} = w_{i,j}^{(t)} - w_{i,j}^{(t-1)} \\]\n\n---\n\nSlide 7.4.15\n\nWe can keep around the most recent change to the weights (at time t-1) and add some fraction of that weight change to the current delta. The fraction, alpha, is the momentum weight.\n\nThe basic idea is that if the changes to the weights have had a consistent sign, then the effect is to have a larger step size in the weight. If the sign has been changing, then the net change may be smaller.\n\nNote that even if the delta times y term is zero (denoting a local minimum in the error), in the presence of a momentum term, the change in the weights will not necessarily be zero. So, this may cause the system to move through a shallow minimum, which may be good. However, it may also lead to undesirable oscillations in some circumstances.\n\nIn practice, choosing a good value of momentum for a problem can be nearly as hard as choosing a learning rate and it\u2019s one more parameter to twiddle with.\n\nMomentum is not that popular a technique anymore; people will tend to use more complex search strategies to ensure convergence to a local minimum.\n\n**Momentum**\n- \\[ w_{i,j}^{(t)} = w_{i,j}^{(t-1)} - \\eta \\delta_j y_i \\]\n- \\[ \\Delta w_{i,j} = w_{i,j}^{(t)} - w_{i,j}^{(t-1)} \\]\n\n   - *Standard backprop descent*\n   - *Rewrite to define change in weights at time t*\n\n- \\[ \\Delta w_{i,j}^{(t)} = \\alpha \\Delta w_{i,j}^{(t-1)} \\]\n- \\[ \\Delta w_{i,j}^{(t)} = -\\eta \\delta_j y_i + \\alpha\\Delta w_{i,j}^{(t-1)} \\]\n\n   - *Adding a momentum term, which adds in a fraction of the weight change at the previous iteration.*\n\n   - Momentum can gradually increase step size when gradient sign is unchanging.\n   - Can help step through shallow local minima.\n   - One more parameter to twiddle... not used much anymore."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_023.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n### Slide 7.4.16\n**Input Representation**\n- All the signals in a neural net are \\[0, 1\\]. Input values should also be scaled to this range (or approximately so) so as to speed training.\n\n**Slide 7.4.16**\nOne question that arises in neural nets (and many other machine learning approaches) is how to represent the input data. We have discussed some of these issues before.\n\nOne issue that is prominent in neural nets is the fact that the behavior of the nets are dependent on the scale of the input. In particular, one does not want to saturate the units, since at that point it becomes impossible to train them. Note that all \"signals\" in a sigmoid-unit neural net are in the range \\[0,1\\] because that is the output range of the sigmoids. It is best to keep the range of the inputs in that range as well. Simple normalization (subtract the mean and divide by the standard deviation) will almost do that and is adequate for most purposes.\n\n### Slide 7.4.17\nAnother issue has to do with the representation of discrete data (also known as \"categorical\" data). You could think of representing these as either unary or binary numbers. Binary numbers are generally a bad choice; unary is much preferable if the number of values is small since it decouples the values completely.\n\n---\n\n- All the signals in a neural net are \\[0, 1\\]. Input values should also be scaled to this range (or approximately so) so as to speed training.\n- For discrete input values (e.g., A, B, C, or D) they need to be coded in unary form.\n- One difficulty is that there may be ambiguous outputs (e.g., two outputs at 0.5 in unary encoding).\n- More sophisticated method is to use special **softmax** units, which force outputs to sum to 1.\n\n### Slide 7.4.18\n**Output Representation**\nA neural net with a single sigmoid output unit is aimed at binary classification. Class is 0 if y<0.5 and 1 otherwise.\n- For multi-class problems\n  - Can use one output per class (unary encoding)\n  - There may be confusing outputs (two outputs @ 0.5 in unary encoding)\n  - More sophisticated method is to use special softmax units, which force outputs to sum to 1.\n\n**Slide 7.4.18**\nSimilar questions arise at the output end. So far, we have focused on binary classification. There, the representation is clear - a single output and we treat an output of 0.5 as the dividing value between one class and the other.\n\nFor multi-class problems, one can have multiple output units, for example, each aimed at recognizing one class, sharing the hidden units with other classes.\n\nOne difficulty with this approach is that there may be ambiguous outputs, e.g., two values above the 0.5 threshold when using a unary encoding. How do we treat such a case? One reasonable approach is to choose the class with the largest output.\n\nA more sophisticated method is to introduce a new type of unit (called \"softmax\") that forces the sum of the unary outputs to add to 1. One can then interpret the network outputs as the probabilities that the input belongs to each of the classes.\n\n---\n\n**Slide 7.4.19**\nAnother detail to consider in training is what to use as the desired (target) value for the network outputs. We have been talking as if we would use 0 or 1 as the targets. The problem with this is that those are the asymptotic values of the sigmoid only achieved for infinite values of the weights. So, if we were to attempt to train a network until the weights stop changing, then we'd be in trouble. It is common to use values such as 0.1 and 0.9 instead of 0 and 1 during neural network training.\n\nIn practice, however, the usual termination for training a network is when the training or, preferably, validation error either achieves an acceptable level, reaches a minimum or stops changing significantly. These outcomes generally happen long before we run the risk of the weights becoming infinite.\n\n### Target Value\n- During training it is impossible for the outputs to reach 0 or 1 (with finite weights).\n- Customary to use 0.1 and 0.9 as targets\n- But, most termination criteria, e.g. small change in training or validation error will stop training before targets are reached."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_024.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\n[Top left slide]\n**Regression**\n- A sigmoid output unit is not suitable for regression, since sigmoids are designed to change quickly from 0 to 1.\n- For regression, we want a linear output unit, that is, remove the output non-linearity.\n- The rest of the net still retains the sigmoid units.\n\n[Top right slide]\n**Slide 7.4.20**\nNeural nets can also do regression, that is, produce an output which is a real number outside the range of 0 to 1, for example, predicting the age of death as a function of packs of cigarettes smoked. However, to do regression, it is important that one does not try to predict a continuous value using an output sigmoid unit. The sigmoid is basically a soft threshold with a limited dynamic range. When doing regression, we want the output unit to be linear, that is, simply remove the sigmoid non-linearity and have the unit returned a weighted sum of its inputs.\n\n[Bottom left slide]\n**Slide 7.4.21**\nOne very interesting application of neural networks is the ALVINN project from CMU. The project was the brainchild of Dean Pomerleau. ALVINN is an automatic steering system for a car based on input from a camera mounted on the vehicle. This system was successfully demonstrated on a variety of real roads in actual traffic. A successor to ALVINN, which unfortunately was not based on\n\nImage: ALVINN steers on highways\nhttp://www.ri.cmu.edu/projects/project_160.html\n\n[Bottom right slide]\n**Slide 7.4.22**\nThe ALVINN neural network is shown here. It has 960 inputs (a 30x32 array derived from the pixels of an image), four hidden units and 30 output units (each representing a steering command). On the right you can see a pattern of the brightnesses of the input pixels and right above that you can see the pattern of the outputs, representing a \"steer left\" command.\n\n**Slide 7.4.23**\nOne of the most interesting issues that came up in the ALVINN project was the problem of obtaining training data. It's not difficult to get images of somebody driving correctly down the middle of the road, but if that were all that ALVINN could do, then it would not be safe to let it on the road. What if an obstacle arose or there was a momentary glitch in the control system or a bump in the road got you off center? It is important that ALVINN be able to recover and steer the vehicle back to the center.\n\nThe researchers considered having the vehicle drive in a wobbly path during training, but that posed the danger of having the system learn to drive that way. They came up with a clever solution. Simulate what the sensory data would have looked like had ALVINN been off-center or headed in a slightly wrong direction and also, for each such input, simulate what the steering command should have been.\n\nNow, you don't want to generate simulated images from scratch, as in a video game, since they are insufficiently realistic. What they did, instead, is transform the real images and fill in the few missing pixels by a form of \"interpolation\" on the actual pixels. The results were amazingly good.\n\nHowever, it turned out that once one understood that this whole project was possible and one\n\nImage: ALVINN steers on highways\nhttp://www.ri.cmu.edu/projects/project_160.html\n\n- Problem: Getting enough diversity in training set\n- Answer: Transform sensor image and steering direction\n\nImage removed due to copyright restrictions."
  },
  {
    "file": "AI_Course\\Lecture_Notes\\Processed\\ch7_mach3\\page_025.jpeg",
    "text": "6.034 Artificial Intelligence. Copyright \u00a9 2004 by Massachusetts Institute of Technology.\n\nunderstood what ALVINN was learning, it became possible to build a special purpose system that was faster and more reliable and involved no explicit on-line learning. This is not an uncommon side-effect of a machine-learning project.\n\n---\n\nSlide 7.4.24\n\n**Some observations...**\n\n- **Although Neural Nets kicked off the current phase of interest in machine learning, they are extremely problematic...**\n\n  - Too many parameters (weights, learning rate, momentum, etc)\n  - Hard to choose the architecture\n  - Very slow to train\n  - Easy to get stuck in local minima\n\n- **Interest has shifted to other methods, such as support vector machines, which can be viewed as variants of perceptrons (with a twist or two).**\n\n---\n\nNeural nets are largely responsible for the current interest in statistical methods for machine learning. For a while, they were wildly popular in the research community and saw many applications. Over time, the enthusiasm has waned somewhat.\n\nThe big problem with neural nets is that they are very complex and have many many parameters that have to be chosen. Furthermore, training them is a bit of a nightmare. So, recent interest has shifted towards methods that are simpler to use and can be characterized better. For example, support vector machines, which at one level can be viewed as a variation of the same perceptrons that neural nets superseded, is the current darling of the machine learning research and application community."
  }
]