In this presentation, we'll take a look at the class of problems called Constraint Satisfaction Problems (CSPs). CSPs arise in many application areas: they can be used to formulate scheduling tasks, robot planning tasks, puzzles, molecular structures, sensory interpretation tasks, etc.
In particular, we'll look at the subclass of Binary CSPs. A binary CSP is described in term of a set of Variables (denoted V i ), a domain of Values for each of the variables (denoted D i ) and a set of constraints involving the combinations of values for two of the variables (hence the name "binary"). We'll also allow "unary" constraints (constraints on a single variable), but these can be seen simply as cutting down the domain of that variable.
We can illustrate the structure of a CSP in a diagram, such as this one, that we call a constraint graph for the problem.
The solution of a CSP involves finding a value for each variable (drawn from its domain) such that all the constraints are satisfied. Before we look at how this can be done, let's look at some examples of CSP.
A CSP that has served as a sort of benchmark problem for the field is the so-called N-Queens problem, which is that of placing N queens on an NxN chessboard so that no two queens can attack each other.
One possible formulation is that the variables are the chessboard positions and the values are either Queen or Blank. The constraints hold between any two variables representing positions that are on a line. The constraint is satisfied whenever the two values are not both Queen.
This formulation is actually very wasteful, since it has N 2 variables. A better formulation is to have variables correspond to the columns of the board and values to the index of the row where the Queen for that column is to be placed. Note that no two queens can share a column and that every column must have a Queen on it. This choice requires only N variables and also fewer constraints to be checked.
In general, we'll find that there are important choices in the formulation of a CSP.
The problem of labeling the lines in a line-drawing of blocks as being either convex, concave or boundary, is the problem that originally brought the whole area of CSPs into prominence. Waltz's approach to solving this problem by propagation of constraints (which we will discuss later) motivated much of the later work in this area.
In this problem, the variables are the junctions (that is, the vertices) and the values are a combination of labels (+, -, >) attached to the lines that make up the junction. Some combinations of these labels are physically realizable and others are not. The basic constraint is that junctions that share a line must agree on the label for that line.
Note that the more natural formulation that uses lines as the variables is not a BINARY CSP, since all the lines coming into a junction must be simultaneously constrained.
Scheduling actions that share resources is also a classic case of a CSP. The variables are the activities, the values are chunks of time and the constraints enforce exclusion on shared resources as well as proper ordering of the tasks.
Another classic CSP is that of coloring a graph given a small set of colors. Given a set of regions with defined neighbors, the problem is to assign a color to each region so that no two neighbors have the same color (so that you can tell where the boundary is). You might have heard of the famous Four Color Theorem that shows that four colors are sufficient for any planar map. This
theorem was a conjecture for more than a century and was not proven until 1976. The CSP is not proving the general theorem, just constructing a solution to a particular instance of the problem.
A very important class of CSPs is the class of boolean satisfiability problems. One is given a formula over boolean variables in conjunctive normal form (a set of ORs connected with ANDs). The objective is to find an assignment that makes the formula true, that is, a satisfying assignment.
SAT problems are easily transformed into the CSP framework. And, it turns out that many important problems (such as constructing a plan for a robot and many circuit design problems) can be turned into (huge) SAT problems. So, a way of solving SAT problems efficiently in practice would have great practical impact.
However, SAT is the problem that was originally used to show that some problems are NPcomplete, that is, as hard as any problem whose solution can be checked in polynomial time. It is generally believed that there is no polynomial time algorithm for NP-complete problems. That is, that any guaranteed algorithm has a worst-case running time that grows exponentially with the size of the problem. So, at best, we can only hope to find a heuristic approach to SAT problems. More on this later.
Model-based recognition is the problem of finding an instance of a known geometric model, described, for example, as a line-boundary in an image which has been pre-processed to identify and fit lines to the boundaries. The position and orientation of the instance, if any, is not known.
There are a number of constraints that need to be satisfied by edges in the image that correspond to edges in the model. Notably, the angles between pairs of edges must be preserved.
So, looking through these examples of CSPs we have some good news and bad news. The good news is that CSP is a very general class of problems containing many interesting practical problems. The bad news is that CSPs include many problem that are intractable in the worst case. So, we should not be surprised to find that we do not have efficient guaranteed solutions for CSP. At best, we can hope that our methods perform acceptably in the class of problems we are interested in. This will depend on the structure of the domain of applicability and will not follow directly from the algorithms.
Given 40 courses (8.01,8.02 6.840) & 10 terms (Fall 1 , Spring 1\_ Spring 5). Find a legal schedule .
Let us take a particular problem and look at the CSP formulation in detail. In particular, let's look at an example which should be very familiar to MIT EECS students.
The problem is to schedule approximately 40 courses into the 10 terms for an MEng. For simplicity, let's assume that the list of courses is given to us.
## CSP Example
The constraints we need to represent and enforce are as follows:
- ● The pre-requisites of a course were taken in an earlier term (we assume the list contains all the pre-requisites).
- ● Some courses are only offered in the Fall or the Spring term.
- ● We want to limit the schedule to a feasible load such as 4 courses a term.
- ● And, we want to avoid time conflicts where we cannot sign up for two courses offered at the same time.
Given 40 courses (8.01,8.02 6.840) & 10 terms (Fall 1, Spring 1 Spring 5)Find a legal schedule.
Constraints
Pre-requisites
Courses offered on limited terms Limited number of courses per term Avoid time conflicts
Note that all of these constraints are either satisfed or not. CSPs are not typically used to express preferences but rather to enforce hard and fast constraints.
One key question that we must answer for any CSP formulation is "What are the variables and what are the values?" For our class scheduling problem, a number of options come to mind. For example, we might pick the terms as the variables. In that case, the values are combinations of four courses that are consistent , meaning that they are offered in the same term and whose times don't conflict. The pre-requisite constraint would relate every pair of terms and would require that no course appear in a term before that of any of its pre-requisite course.
This perfectly valid formulation has the practical weakness that the domains for the variables are huge, which has a dramatic effect on the running time of the algorithms.
One way of avoiding the combinatorics of using 4-course schedules as the values of the variables is to break up each term into "term slots" and assign to each term-slot a single course. This formulation, like the previous one, has the limit on the number of courses per term represented directly in the graph, instead of stating an explicit constraint. With this representation, we will still need constraints to ensure that the courses in a given term do not conflict and the pre-requisite ordering is enforced. The availability of a course in a given term could be enforced by filtering the domains of the variables.
## Choice of variables & values
Another formulation turns things around and uses the courses themselves as the variables and then uses the terms (or more likely, term slots) as the values. Let's look at this formulation in greater detail.
## VARIABLES
## DOMAINS
Tems?
## B. Tem Slots?
subdivide terms into slots e.9 4 of them (Fall 1,1) (Fall 1,2) (Fall1,3) (Fall 1,4)
C. Courses?
Legal combinations of for example 4 courses (but this is huge set of values) .
Courses offered during that term
Terms or term slots (Term slots allow expressing constraint on limited number of of courses /term:)
One constraint that must be represented is that the pre-requisites of a class must be taken before the actual class. This is easy to represent in this formulation. We introduce types of constraints called "term before" and "term after" which check that the values assigned to the variables, for example, 6.034 and 6.001, satisfy the correct ordering.
Note that the undirected links shown in prior constraint graphs are now split into two directed links, each with complementary constraints.
The constraint that some courses are only offered in some terms simply filters illegal term values from the domains of the variables.
The limit on courses to be taken in a term argues for the use of term-slots as values rather than just terms. If we use term-slots, then the constraint is implicitly satisfied.
Avoiding time conflicts is also easily represented. If two courses occur at overlapping times then we place a constraint between those two courses. If they overlap in time every term that they are given, we can make sure that they are taken in different terms. If they overlap only on some terms, that can also be enforced by an appropriate constraint.
We now turn our attention to solving CSPs. We will see that the approaches to solving CSPs are some combination of constraint propagation and search. We will look at these in turn and then look at how they can be profitably combined.
## Constraint Propagation [aka Arc Consistency)
Arc consistency eliminates values from domain of variable that can never be of a consistent solution. part
VxeD HyeD; such that (x,y) is allowed by the constraint on the arc
The great success of Waltz's constraint propagation algorithm focused people's attention on CSPs. The basic idea in constraint propagation is to enforce what is known as "ARC CONSISTENCY", that is, if one looks at a directed arc in the constraint graph, say an arc from V i to V , we say that this j arc is consistent if for every value in the domain of V i , there exists some value in the domain of V j that will satisfy the constraint on the arc.
## Constraint Propaqation laka\_Arc Consistency
Suppose there are some values in the domain at the tail of the constraint arc (for V i ) that do not have any consistent partner in the domain at the head of the arc (for V j ). We achieve arc consistency by dropping those values from D i . Note, however, that if we change D i , we now have to check to make sure that any other constraint arcs that have D i at their head are still consistent. It is this phenomenon that accounts for the name "constraint propagation".
Arc consistency eliminates values from domain of variable that can never be of a consistent solution. part
Directed arc (Vi. V) is arc consistent if VxeD; HyeD; such that (x,y) is allowed by the constraint on the arc
We can achieve consistency on arc by deleting values form D; (domain of variable at tail of constraint arc) that fail this condition.
What is the cost of this operation? In what follows we will reckon cost in terms of "arc tests": the number of times we have to check (evaluate) the constraint on an arc for a pair of values in the variable domains of that arc. Assuming that domains have at most d elements and that there are at most e binary constraints (arcs), then a simple constraint propagation algorithm takes O( ed 3 ) arc tests in the worst case.
It is easy to see that checking for consistency of each arc for all the values in the corresponding domains takes O( d 2 ) arc tests, since we have to look at all pairs of values in two domains. Going through and checking each arc once requires O( ed 2 ) arc tests. But, we may have to go through and look at the arcs more than once as the deletions to a node's domain propagate. However, if we look at an arc only when one of its variable domains has changed (by deleting some entry), then no arc can require checking more than d times and we have the final cost of O( ed 3 ) arc tests in the worst case.
Let's look at a trivial example of graph coloring. We have three variables with the domains indicated. Each variable is constrained to have values different from its neighbors.
We start with the V 1 -V 2 arc. Note that for every value in the domain of V 1 (R, G and B) there is some value in the domain of V 2 that it is consistent with (that is, it is different from). So, for R in V 1 there is a G in V 2 , for G in V 1 there is an R in V 2 and for B in V 1 there is either R and G in V 2 . Similarly, for each entry in V 2 there is a valid counterpart in V 1 . So, the arc is consistent and no changes are made.
We will now simulate the process of constraint propagation. In the interest of space, we will deal in this example with undirected arcs, which are just a shorthand for the two directed arcs between the variables. Each step in the simulation involves examining one of these undirected arcs, seeing if the arc is consistent and, if not, deleting values from the domain of the appropriate variable.
Moving to V 2 -V 3 , we note similarly that the G in V 2 has no valid counterpart in V 3 and so we drop it from V 2 's domain. Although we have now looked at all the arcs once, we need to keep going since we have changed the domains for V 1 and V 2 .
We test V 1 -V 3 and it is consistent.
Looking at V 1 -V 2 again we note that R in V 1 no longer has a valid counterpart in V 2 (since we have deleted G from V 2 ) and so we need to drop R from V 1 .
We move to V 1 -V 3 . The situation here is different. While R and B in V 1 can co-exist with the G in V 3 , not so the G in V 1 . And, so, we remove the G from V 1 . Note that the arc in the other direction is consistent.
We test V 2 -V 3 and it is consistent.
We are done; the graph is arc consistent. In general, we will need to make one pass through any arc whose head variable has changed until no further changes are observed before we can stop. If at any point some variable has an empty domain, the graph has no consistent solution.
Note that whereas arc consistency is required for there to be a solution for a CSP, having an arcconsistent solution is not sufficient to guarantee a unique solution or even any solution at all. For example, this first graph is arc-consistent but there are NO solutions for it (we need at least three colors and have only two).
This next graph is also arc consistent but there are 2 distinct solutions: BRG and BGR.
This next graph is also arc consistent but it has a unique solution, by virtue of the special constraint between two of the variables.
In general, if there is more than one value in the domain of any of the variables, we do not know whether there is zero, one, or more than one answer that is globally consistent. We have to search for an answer to actually know for sure.
How does one search for solutions to a CSP problem? Any of the search methods we have studied is applicable. All we need to realize is that the space of assignments of values to variables can be viewed as a tree in which all the assignments of values to the first variable are descendants of the first node and all the assignments of values to the second variable form the descendants of those nodes and so forth.
The classic approach to searching such a tree is called "backtracking", which is just another name for depth-first search in this tree. Note, however, that we could use breadth-first search or any of the heuristic searches on this problem. The heuristic value could be used to either guide the search to termination or bias it to a desired solution based on preferences for certain assignments. UniformCost and A* would make sense also if there were a non-uniform cost associated with a particular assignment of a value to a variable (note that this is another (better but more expensive) way of incorporating preferences).
However, you should observe that these CSP problems are different from the graph search problems we looked at before, in that we don't really care about the path to some state but just the final state itself.
If we undertake a DFS in this tree, going left to right, we first explore assigning R to V 1 and then move to V 2 and consider assigning R to it. However, for any assignment, we need to check any constraints involving previous assignments in the tree. We note that V 2 =R is inconsistent with V 1 =R and so that assignment fails and we have to backup to find an alternative assignment for the most recently assigned variable.
So, we consider assigning V 2 =G, which is consistent with the value for V 1 . We then move to V 3 =R. Since we have a constraint between V 1 and V 3 , we have to check for consistency and find it is not consistent, and so we backup to consider another value for V 3 .
But V 3 =G is inconsistent with V 2 =G, and so we have to backup. But there are no more pending values for V 3 or for V 2 and so we fail back to the V 1 level.
The process continues in that fashion until we find a solution. If we continue past the first success, we can find all the solutions for the problem (two in this case).
We can use some form of backtracking search to solve CSP independent of any form of constraint propagation. However, it is natural to consider combining them. So, for example, during a backtracking search where we have a partial assignment, where a subset of all the variables each has unique values assigned, we could then propagate these assignments throughout the constraint graph to obtain reduced domains for the remaining variables. This is, in general, advantageous since it decreases the effective branching factor of the search tree.
But, how much propagation should we do? Is it worth doing the full arc-consistency propagation we described earlier?
Question:
The answer is USUALLY no. It is generally sufficient to only propagate to the immediate neighbors of variables that have unique values (the ones assigned earlier in the search). That is, we eliminate from consideration any values for future variables that are inconsistent with the values assigned to past variables. This process is known as forward checking (FC) because one checks values for future variables (forward in time), as opposed to standard backtracking which checks value of past variables (backwards in time, hence back-checking).
When the domains at either end of a constraint arc each have multiple legal values, odds are that the constraint is satisfied, and so checking the constraint is usually a waste of time. This conclusion suggests that forward checking is usually as much propagation as we want to do. This is, of course, only a rule of thumb.
Let's step through a search that uses a combination of backtracking with forward checking. We start by considering an assignment of V 1 =R.
We then propagate to the neighbors of V 1 in the constraint graph and eliminate any values that are inconsistent with that assignment, namely the value R. That leaves us with the value G in the domains of V 2 and V 3 . So, we make the assignment V 2 =G and propagate.
But, when we propagate to V 3 we see that there are no remaining valid values and so we have found an inconsistency. We fail and backup. Note that we have failed much earlier than with simple backtracking, thus saving a substantial amount of work.
That eliminates G from V 2 and V 3 .
The domain of V 3 is empty, so we fail and backup.
We now consider V 1 =G and propagate.
We now consider V 2 =R and propagate.
This propagation does not delete any values. We pick V 2 =R and propagate.
We pick V 3 = G and have a consistent assignment.
So, we move to consider V 1 =B and propagate.
This removes the R values in the domains of V 1 and V 3 .
We can continue the process to find the other consistent solution.
Note that when doing forward checking there is no need to check new assignments against previous assignments. Any potential inconsistencies have been removed by the propagation. BT-FC is usually preferable to plain BT because it eliminates from consideration inconsistent assignments once and for all rather than discovering the inconsistency over and over again in different parts of the tree. For example, in pure BT, an assignment for V 3 that is inconsistent with a value of V 1 would be "discovered" independently for every value of V 2 . Whereas FC would delete it from the domain of V 3 right away.
We have been assuming that the order of the variables is given by some arbitrary ordering. However, the order of the variables (and values) can have a substantial effect on the cost of finding the answer. Consider, for example, the course scheduling problem using courses given in the order that they should ultimately be taken and assume that the term values are ordered as well. Then a depth first search will tend to find the answer very quickly.
Of course, we generally don't know the answer to start off with, but there are more rational ways of ordering the variables than alphabetical or numerical order. For example, we could order the variables before starting by how many constraints they have. But, we can do even better by dynamically re-ordering variables based on information available during a search.
Traditional backracking uses fixed ordering of variables & values, e.9 random order or place variables with many constraints first.
You can usually do better by choosing an order dynamically as the search proceeds \_
## BLEC with dynamic ordering
Traditional backracking uses fixed ordering of variables & values , random order or place variables with many constraints first . e.9 ,
You can usually do better by choosing an order dynamically as the search proceeds .
## Most constrained variable
when doing forward-checking. pick variable with fewest legal values to assign next (minimizes branching factor)
Spnng02
For example, assume we are doing backtracking with forward checking. At any point, we know the size of the domain of each variable. We can order the variables below that point in the search tree so that the most constrained variable (smallest valid domain) is next. This will have the effect of reducing the average branching factor in the tree and also cause failures to happen sooner.
## BI EC with dynamic ordering
Furthermore, we can count for each value of the variable the impact on the domains of its neighbors, for example the minimum of the resulting domains after propagation. The value with the largest minimum resulting domain size (or average value or sum) would be one that least constrains the remaining choices and is least likely to lead to failure.
Of course, value ordering is only worth doing if we are looking for a single answer to the problem. If we want all answers, then all values will have to be tried eventually.
## BLEC with dynamic ordering
Traditional backracking uses fixed ordering of variables & values, e.9-, random order or place variables with many constraints first.
You can usually do better by choosing an order dynamically as the search proceeds \_
- Most constrained variable
when doing forward-checking. pick variable with fewest legal values to assign next (minimizes branching factor)
## Least constraining value
choose value that rules out the fewest values from neighboring domains
E.g. this combination improves feasible n-queens performance from about n = 30 with just FC to about n = 1000 with FC & ordering .
Spnng02
This example of the 4-color map-coloring problem illustrates a simple situation for variable and value ordering. Here, A is colored Green, B is colored Blue and C is colored Red. What country should we color next, D or E or F?
This combination of variable and value ordering can have dramatic impact on some problems.
Traditional backracking uses fixed ordering of variables & values,e.9-, random order or place variables with many constraints first .
You can usually do better by choosing an order dynamically as the search proceeds
## Most constrained variable
when doing fonward-checking. pick variable with fewest legal values to assign next (minimizes branching factor)
## Least constraining value
choose value that rules out the fewest values from neighboring domains
By picking RED, we keep open the most options for D and F, so we pick that.
Well, E is more constrained (has fewer) legal values so we should try it next. Which of E's values should we try next?
All of the methods for solving CSPs that we have discussed so far are systematic (guaranteed searches). More recently, researchers have had surprising success with methods that are not systematic (they are randomized) and do not involve backup.
The basic idea is to do incremental repair of a nearly correct assignment. Imagine we had some heuristic that could give us a "good" answer to any of the problems. By "good" we mean one with relatively few constraint violations. In fact, this could even be a randomly chosen solution.
Then, we could take the following approach. Identify a random variable involved in some conflict. Pick a new value for that variable that minimizes the number of resulting conflicts. Repeat.
This is a type of local "greedy" search algorithm.
There are variants of this strategy that use this heuristic to do value ordering within a backtracking search. Remarkably, this type of ordering (in connection with a good initial guess) leads to remarkable behavior for benchmark problems. Notably, the systematic versions of this strategy can solve the million-queen problem in minutes. After this, people decided N-queens was not interesting...
The pure "greedy" hill-climber can readily fail on any problem (by finding a local minimum where any change to a single variable causes the number of conflicts to increase). We'll look at this a bit in the problem set.
There are several ways of trying to deal with local minima. One is to introduce weights on the violated constraints. A simpler one is to re-start the search with another random initial state. This is the approach taken by GSAT, a randomized search process that solves SAT problems using a similar approach to the one described here.
GSAT's performance is nothing short of remarkable. It can solve SAT problems of mind-boggling complexity. It has forced a complete reconsideration of what it means when we say that a problem is "hard". It turns out that for SAT, almost any randomly chosen problem is "easy". There are really hard SAT problems but they are difficult to find. This is an area of active study.
## GSALas Heuristic Search
- State space: Space of all full assignments to variables
- Initial state: A random full assignment
- Goal state: A satisfying assignment
- Actions: Flip value of one variable in current assignment
- Heuristic: The number of satisfied clauses (constraints); we want to maximize this Alternatively , minimize the number of unsatisfied clauses (constraints).
Spang 02
Here we see the GSAT algorithm, which is very simple in sketch. The critical implementation challenge is that of finding quickly the variable whose flip maximizes the score. Note that there are two user-specified variables: the number of times the outer loop is executed (MAXTRIES) and the number of times the inner loop is executed (MAXFLIPS). These parameters guard against local minima in the search, simply by starting with a new, randomly chosen assignment and trying a different sequence of flips. As we have mentioned, this works surprisingly well.
## WALKSAIIE)
- For i=1 to Maxtries
- Select a complete random assignment A
- Score number of satisfied clauses
- For j=1 to Maxflips
- If (A satisfies all clauses in F) return A
- Else
- With probability p r GSAT *
- flip & variable that maximizes score
- Flip a randomly chosen variable if no variable flip increases the score
- With probability 1-p r Random Walk *
- Pick a random unsatisfied clause €
- Flip a randomly chosen variable in €
An even more effective strategy turns out to add even more randomness. WALKSAT basically performs the GSAT algorithm some percentage of the time and the rest of the time it does a random walk in the space of assignments by randomly flipping variables in unsatisfied clauses (constraints).
It's a bit depressing to think that such simple randomized strategies can be so much more effective than clever deterministic strategies. There are signs at present that some of the clever deterministic strategies are becoming competitive or superior to the randomized ones. The story is not over.
GSAT can be framed as a heuristic search strategy. Its state space is the space of all full assignments to the variables. The initial state is a random assignment, while the goal state is any assignment that satisfies the formula. The actions available to GSAT are simply to flip one variable in the assignment from true to false or vice-versa. The heuristic value used for the search, which GSAT tries to maximize, is the number of satisfied clauses (constraints). Note that this is equivalent to minimizing the number of conflicts, that is, violated constraints.
## Min-conflict heuristic
The pure hill climber (without backtracking) can get stuck in local minima Can add random moves to attempt getting out of minima generally quite effective. Can also use weights on violated constraints & increase weight it remains violated. cycle every
## GSAI
Randomized hill climber used to solve SAT problems One of the most effective methods ever found for this problem
Spnng02
- For i=1 to Maxtries
- Select a complete random assignment A
- Score number of satisfied clauses
- For j=1 to Maxflips
- ~If (A satisfies all clauses in F) return A
- Else flip a variable that maximizes score
- ~Flip a randomly chosen variable if no variable flip increases the score