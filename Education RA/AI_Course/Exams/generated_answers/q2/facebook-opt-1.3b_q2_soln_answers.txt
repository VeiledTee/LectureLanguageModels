QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: > 2. What is the probability that the data points are positive? > 3. What is the probability that the data points are negative? > 4. What is the probability that the data points are positive? > 5. What is the probability that the data points are negative? > 6. What is the probability that the data points are positive? > 7. What is the probability that the data points are negative? > 8. What is the probability that the data points are positive? > 9. What is the probability that the data points are negative? > 10. What is the probability that the data points are positive? > 11. What is the probability that the data points are negative? > 12. What is the probability that the data points are positive? > 13. What is the probability that the data points are negative? > 14. What is the probability that the data points are positive? > 15. What is the probability that the data points are negative? > 16. What is the probability that the data points are positive? > 17. What is the probability that the data points are negative? > 18. What is the probability that the data points are positive? > 19. What is the probability that the data points are negative? > 20.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: > 3. Draw a decision tree on the graph. > 4. Draw a decision tree on the graph. > 5. Draw a decision tree on the graph. > 6. Draw a decision tree on the graph. > 7. Draw a decision tree on the graph. > 8. Draw a decision tree on the graph. > 9. Draw a decision tree on the graph. > 10. Draw a decision tree on the graph. > 11. Draw a decision tree on the graph. > 12. Draw a decision tree on the graph. > 13. Draw a decision tree on the graph. > 14. Draw a decision tree on the graph. > 15. Draw a decision tree on the graph. > 16. Draw a decision tree on the graph. > 17. Draw a decision tree on the graph. > 18. Draw a decision tree on the graph. > 19. Draw a decision tree on the graph. > 20. Draw a decision tree on the graph. > 21. Draw a decision tree on the graph. > 22. Draw a decision tree on the graph. > 23. Draw a decision tree on the graph. > 24. Draw a decision tree on the graph. > 25. Draw a decision tree on the graph. > 26.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: > 4. Explain how you chose the top-level test in the tree. > 5. Explain how you chose the top-level test in the tree. > 6. Explain how you chose the top-level test in the tree. > 7. Explain how you chose the top-level test in the tree. > 8. Explain how you chose the top-level test in the tree. > 9. Explain how you chose the top-level test in the tree. > 10. Explain how you chose the top-level test in the tree. > 11. Explain how you chose the top-level test in the tree. > 12. Explain how you chose the top-level test in the tree. > 13. Explain how you chose the top-level test in the tree. > 14. Explain how you chose the top-level test in the tree. > 15. Explain how you chose the top-level test in the tree. > 16. Explain how you chose the top-level test in the tree. > 17. Explain how you chose the top-level test in the tree. > 18. Explain how you chose the top-level test in the tree. > 19. Explain how you chose the top-level test in the tree.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: > 4. What class does the tree predict for the point (2, -2.01)? > 4. What class does the tree predict for the point (3, -3.01)? > 4. What class does the tree predict for the point (4, -4.01)? > 4. What class does the tree predict for the point (5, -5.01)? > 4. What class does the tree predict for the point (6, -6.01)? > 4. What class does the tree predict for the point (7, -7.01)? > 4. What class does the tree predict for the point (8, -8.01)? > 4. What class does the tree predict for the point (9, -9.01)? > 4. What class does the tree predict for the point (10, -10.01)? > 4. What class does the tree predict for the point (11, -11.01)? > 4. What class does the tree predict for the point (12, -12.01)? > 4. What class does the tree predict for the point (13, -13.01)? > 4.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries. (8 pts) > 1. Draw 1-NN decision boundaries.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

Answer in English.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: (1, -1.01) > 4. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 5. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 6. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 7. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 8. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 9. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 10. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 11. What class does 3-NN predict for (1, -1.01)? Explain. (1, -1.01) > 12.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).

Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).

Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).

Answer concisely in English without code: 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: > 2.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3. Describe the result if applied to the 5-point dataset from Problem 1. > 3.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: The sigmoid outputs are then used to compute the sigmoid function \(f(x)=x^2+x^3+x^4+x^5+x^6+x^7\).

Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). The sigmoid outputs are then used to compute the sigmoid function \(f(x)=x^2+x^3+x^4+x^5+x^6+x^7\).

Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: The answer is \(6.034\). The question is: What is the probability that \(\Delta w_2\) is equal to \(6.034\)?

Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). The answer is \(6.034\). The question is: What is the probability that \(\Delta w_2\) is equal to \(6.034\)?

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6      | | \(x_2=1\) | 0      | 0      | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=0)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.

Answer concisely in English without code: 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6      | 6      | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.

Answer concisely in English without code: 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: > 2. 1M training examples, 6D features, 100 test queries. > 3. 1M training examples, 6D features, 100 test queries. > 4. 1M training examples, 6D features, 100 test queries. > 5. 1M training examples, 6D features, 100 test queries. > 6. 1M training examples, 6D features, 100 test queries. > 7. 1M training examples, 6D features, 100 test queries. > 8. 1M training examples, 6D features, 100 test queries. > 9. 1M training examples, 6D features, 100 test queries. > 10. 1M training examples, 6D features, 100 test queries. > 11. 1M training examples, 6D features, 100 test queries. > 12. 1M training examples, 6D features, 100 test queries. > 13. 1M training examples, 6D features, 100 test queries. > 14. 1M training examples, 6D features, 100 test queries. > 15. 1M training examples, 6D features, 100 test queries. > 16. 1M training examples, 6D features, 100 test queries. > 17. 1M training examples, 6D features, 100 test queries.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: > 3. Classifier for kindergarten special education requiring justification. > 4. Classifier for kindergarten special education requiring justification. > 5. Classifier for kindergarten special education requiring justification. > 6. Classifier for kindergarten special education requiring justification. > 7. Classifier for kindergarten special education requiring justification. > 8. Classifier for kindergarten special education requiring justification. > 9. Classifier for kindergarten special education requiring justification. > 10. Classifier for kindergarten special education requiring justification. > 11. Classifier for kindergarten special education requiring justification. > 12. Classifier for kindergarten special education requiring justification. > 13. Classifier for kindergarten special education requiring justification. > 14. Classifier for kindergarten special education requiring justification. > 15. Classifier for kindergarten special education requiring justification. > 16. Classifier for kindergarten special education requiring justification. > 17. Classifier for kindergarten special education requiring justification. > 18. Classifier for kindergarten special education requiring justification. > 19. Classifier for kindergarten special education requiring justification. > 20. Classifier for kindergarten special education requiring justification. > 21. Classifier for kindergarten special education requiring justification. > 22. Classifier for kindergarten special education requiring justification. > 23. Classifier for kindergarten special education requiring justification. > 24.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: > 4. How to use the following algorithms to predict book preference? > 5. How to use the following algorithms to predict book preference? > 6. How to use the following algorithms to predict book preference? > 7. How to use the following algorithms to predict book preference? > 8. How to use the following algorithms to predict book preference? > 9. How to use the following algorithms to predict book preference? > 10. How to use the following algorithms to predict book preference? > 11. How to use the following algorithms to predict book preference? > 12. How to use the following algorithms to predict book preference? > 13. How to use the following algorithms to predict book preference? > 14. How to use the following algorithms to predict book preference? > 15. How to use the following algorithms to predict book preference? > 16. How to use the following algorithms to predict book preference? > 17. How to use the following algorithms to predict book preference? > 18. How to use the following algorithms to predict book preference? > 19. How to use the following algorithms to predict book preference? > 20. How to use the following algorithms to predict book preference? > 21. How to use the following algorithms to predict book preference? > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: > 3. The number of particles in a particle cloud. > 2. The number of particles in a cloud. > 1. The number of particles in a cloud. > 2. The number of particles in a cloud. > 3. The number of particles in a cloud. > 4. The number of particles in a cloud. > 5. The number of particles in a cloud. > 6. The number of particles in a cloud. > 7. The number of particles in a cloud. > 8. The number of particles in a cloud. > 9. The number of particles in a cloud. > 10. The number of particles in a cloud. > 11. The number of particles in a cloud. > 12. The number of particles in a cloud. > 13. The number of particles in a cloud. > 14. The number of particles in a cloud. > 15. The number of particles in a cloud. > 16. The number of particles in a cloud. > 17. The number of particles in a cloud. > 18. The number of particles in a cloud. > 19. The number of particles in a cloud. > 20. The number of particles in a cloud. > 21. The number of particles in a cloud.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: : > 2. Random Forest: > 3. Random Walk: > 4. Random Encoder: > 5. Random Encoder: > 6. Random Encoder: > 7. Random Encoder: > 8. Random Encoder: > 9. Random Encoder: > 10. Random Encoder: > 11. Random Encoder: > 12. Random Encoder: > 13. Random Encoder: > 14. Random Encoder: > 15. Random Encoder: > 16. Random Encoder: > 17. Random Encoder: > 18. Random Encoder: > 19. Random Encoder: > 20. Random Encoder: > 21. Random Encoder: > 22. Random Encoder: > 23. Random Encoder: > 24. Random Encoder: > 25. Random Encoder: > 26. Random Encoder: > 27. Random Encoder: > 28. Random Encoder: > 29. Random Encoder: > 30. Random Encoder: > 31. Random Encoder: > 32. Random Encoder: > 33. Random Encoder: > 34. Random Encoder: > 35. Random Encoder: > 36. Random Encoder: > 37. Random Encoder: > 38.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: : > 1. Linear SVM: > 2. Linear SVM: > 3. Linear SVM: > 4. Linear SVM: > 5. Linear SVM: > 6. Linear SVM: > 7. Linear SVM: > 8. Linear SVM: > 9. Linear SVM: > 10. Linear SVM: > 11. Linear SVM: > 12. Linear SVM: > 13. Linear SVM: > 14. Linear SVM: > 15. Linear SVM: > 16. Linear SVM: > 17. Linear SVM: > 18. Linear SVM: > 19. Linear SVM: > 20. Linear SVM: > 21. Linear SVM: > 22. Linear SVM: > 23. Linear SVM: > 24. Linear SVM: > 25. Linear SVM: > 26. Linear SVM: > 27. Linear SVM: > 28. Linear SVM: > 29. Linear SVM: > 30. Linear SVM: > 31. Linear SVM: > 32. Linear SVM: > 33. Linear SVM: > 34. Linear SVM: > 35. Linear SVM: > 36. Linear SVM: > 37.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 4. Decision Tree (variable depth) > 5. Decision Tree (variable depth) > 6. Decision Tree (variable depth) > 7. Decision Tree (variable depth) > 8. Decision Tree (variable depth) > 9. Decision Tree (variable depth) > 10. Decision Tree (variable depth) > 11. Decision Tree (variable depth) > 12. Decision Tree (variable depth) > 13. Decision Tree (variable depth) > 14. Decision Tree (variable depth) > 15. Decision Tree (variable depth) > 16. Decision Tree (variable depth) > 17. Decision Tree (variable depth) > 18. Decision Tree (variable depth) > 19. Decision Tree (variable depth) > 20. Decision Tree (variable depth) > 21. Decision Tree (variable depth) > 22. Decision Tree (variable depth) > 23. Decision Tree (variable depth) > 24. Decision Tree (variable depth) > 25. Decision Tree (variable depth) > 26. Decision Tree (variable depth) > 27. Decision Tree (variable depth) > 28. Decision Tree (variable depth) > 29. Decision Tree (variable depth) > 30. Decision Tree (variable depth) > 31. Decision Tree (variable depth) > 32.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 3. Random Forest (no regularization) > 2. Random Forest (no regularization) > 1. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0. Random Forest (no regularization) > 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 4. SVM (\(c < \infty\)) > 3. SVM (\(c < \infty\)) > 2. SVM (\(c < \infty\)) > 1. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 2. 3-NN (equally weighted) > 3. 4-NN (equally weighted) > 4. 5-NN (equally weighted) > 5. 6-NN (equally weighted) > 6. 7-NN (equally weighted) > 7. 8-NN (equally weighted) > 8. 9-NN (equally weighted) > 9. 10-NN (equally weighted) > 10. 11-NN (equally weighted) > 11. 12-NN (equally weighted) > 12. 13-NN (equally weighted) > 13. 14-NN (equally weighted) > 14. 15-NN (equally weighted) > 15. 16-NN (equally weighted) > 16. 17-NN (equally weighted) > 17. 18-NN (equally weighted) > 18. 19-NN (equally weighted) > 19. 20-NN (equally weighted) > 20. 21-NN (equally weighted) > 21. 22-NN (equally weighted) > 22. 23-NN (equally weighted) > 23. 24-NN (equally weighted) > 24. 25-NN (equally weighted) > 25.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 2. Regression Trees (leaf size=2) > 2. Regression Trees (leaf size=3) > 2. Regression Trees (leaf size=4) > 2. Regression Trees (leaf size=5) > 2. Regression Trees (leaf size=6) > 2. Regression Trees (leaf size=7) > 2. Regression Trees (leaf size=8) > 2. Regression Trees (leaf size=9) > 2. Regression Trees (leaf size=10) > 2. Regression Trees (leaf size=11) > 2. Regression Trees (leaf size=12) > 2. Regression Trees (leaf size=13) > 2. Regression Trees (leaf size=14) > 2. Regression Trees (leaf size=15) > 2. Regression Trees (leaf size=16) > 2. Regression Trees (leaf size=17) > 2. Regression Trees (leaf size=18) > 2. Regression Trees (leaf size=19) > 2. Regression Trees (leaf size=20) > 2. Regression Trees (leaf size=21) > 2. Regression Trees (leaf size=22) > 2.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: > 4. Random Forest > 5. Random Forest with Random Variables > 6. Random Forest with Random Variables with Random Variables > 7. Random Forest with Random Variables with Random Variables > 8. Random Forest with Random Variables with Random Variables > 9. Random Forest with Random Variables with Random Variables > 10. Random Forest with Random Variables with Random Variables > 11. Random Forest with Random Variables with Random Variables > 12. Random Forest with Random Variables with Random Variables > 13. Random Forest with Random Variables with Random Variables > 14. Random Forest with Random Variables with Random Variables > 15. Random Forest with Random Variables with Random Variables > 16. Random Forest with Random Variables with Random Variables > 17. Random Forest with Random Variables with Random Variables > 18. Random Forest with Random Variables with Random Variables > 19. Random Forest with Random Variables with Random Variables > 20. Random Forest with Random Variables with Random Variables > 21. Random Forest with Random Variables with Random Variables > 22. Random Forest with Random Variables with Random Variables > 23. Random Forest with Random Variables with Random Variables > 24.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (4 pts) > 3. Multi-layer Neural Network (3 pts) > 2. Multi-layer Neural Network (2 pts) > 1. Multi-layer Neural Network (1 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0. Multi-layer Neural Network (0 pts) > 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: > 1.

Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\) > 1.

Answer concisely in English without code: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 2.

