QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: > 2. What is the decision tree for the data? > 3. What is the decision tree for the data? > 4. What is the decision tree for the data? > 5. What is the decision tree for the data? > 6. What is the decision tree for the data? > 7. What is the decision tree for the data? > 8. What is the decision tree for the data? > 9. What is the decision tree for the data? > 10. What is the decision tree for the data? > 11. What is the decision tree for the data? > 12. What is the decision tree for the data? > 13. What is the decision tree for the data? > 14. What is the decision tree for the data? > 15. What is the decision tree for the data? > 16. What is the decision tree for the data? > 17. What is the decision tree for the data? > 18. What is the decision tree for the data? > 19. What is the decision tree for the data? > 20.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: > 3. What is the decision boundary equation? > 4. What is the decision boundary equation? > 5. What is the decision boundary equation? > 6. What is the decision boundary equation? > 7. What is the decision boundary equation? > 8. What is the decision boundary equation? > 9. What is the decision boundary equation? > 10. What is the decision boundary equation? > 11. What is the decision boundary equation? > 12. What is the decision boundary equation? > 13. What is the decision boundary equation? > 14. What is the decision boundary equation? > 15. What is the decision boundary equation? > 16. What is the decision boundary equation? > 17. What is the decision boundary equation? > 18. What is the decision boundary equation? > 19. What is the decision boundary equation? > 20. What is the decision boundary equation? > 21. What is the decision boundary equation? > 22. What is the decision boundary equation? > 23. What is the decision boundary equation? > 24.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: 1. The top-level test is the one that maximizes the information gain. 2. The top-level test is the one that minimizes the number of misclassified points. 3. The top-level test is the one that maximizes the number of misclassified points. 4. The top-level test is the one that minimizes the number of misclassified points. 5. The top-level test is the one that maximizes the information gain. 6. The top-level test is the one that minimizes the number of misclassified points. 7. The top-level test is the one that maximizes the information gain. 8. The top-level test is the one that minimizes the number of misclassified points. 9. The top-level test is the one that maximizes the information gain. 10. The top-level test is the one that minimizes the number of misclassified points. 11. The top-level test is the one that maximizes the information gain. 12. The top-level test is the one that minimizes the number of misclassified points. 13. The top-level test is the one that maximizes the information gain.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: Answer:

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: 2. Draw 3-NN decision boundaries. 3. Draw 5-NN decision boundaries. 4. Draw 10-NN decision boundaries. 5. Draw 20-NN decision boundaries. 6. Draw 50-NN decision boundaries. 7. Draw 100-NN decision boundaries. 8. Draw 200-NN decision boundaries. 9. Draw 500-NN decision boundaries. 10. Draw 1000-NN decision boundaries. 11. Draw 2000-NN decision boundaries. 12. Draw 5000-NN decision boundaries. 13. Draw 10000-NN decision boundaries. 14. Draw 20000-NN decision boundaries. 15. Draw 50000-NN decision boundaries. 16. Draw 100000-NN decision boundaries. 17. Draw 200000-NN decision boundaries. 18. Draw 500000-NN decision boundaries. 19.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: Answer: 1-NN predicts the class of (1, -1.01) to be the class of (1, -1.01) itself. This is because the distance metric used in 1-NN is the Euclidean distance, and in this case, the Euclidean distance between (1, -1.01) and (1, -1.01) is 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: Answer: 3-NN predicts the class -1.
Explanation: 3-NN uses the majority vote of the three nearest neighbors to make predictions. In this case, the three nearest neighbors are (1, -1.01), (1, -1.01), and (1, -1.01). Since all three neighbors belong to the class -1, 3-NN predicts the class -1 for (1, -1.01).

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: > 2. Suppose the perceptron algorithm is run on the data points above. What is the final weight vector? > 3. What is the decision boundary? > 4. What is the error rate of the perceptron algorithm? > 5. Suppose the data points are (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0), (1, 0). What is the decision boundary of the perceptron algorithm? > 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: > 1. Predict the class for (0.0, 1.0). > 0. Predict the class for (0.0, 0.0). > 4. Predict the class for (1.0, 1.0). > 5. Predict the class for (0.0, -1.0). > 6. Predict the class for (2.0, 1.0). > 7. Predict the class for (1.0, -1.0).

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: Answer:

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: | x | y | w0 | w1 | w2 |
|---|---|----|----|----|
| -1 | 0 | 0  | 1  | 1  |
| 2 | -2 | 0  | 1  | 1  |
| 1 | 0  | 0  | 1  | 1  |

| x | y | w0 | w1 | w2 |
|---|---|----|----|----|
| -1 | 0 | 0  | 1  | 1  |
| 2 | -2 | 0  | 1  | 1  |
| 1 | 0  | 0  | 1  | 1  |

| x | y | w0 | w1 | w2 |
|---|---|----|----|----|
| -1 | 0 | 0  | 1  | 1  |
| 2 | -2 | 0  | 1  | 1  |
| 1 | 0  |

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: > 3. Calculate \(\Delta w_3\) via backpropagation for input (1,1). > 4. Calculate \(\Delta w_1\) via backpropagation for input (1,-1). > 5. Calculate \(\Delta w_0\) via backpropagation for input (0,0). > 6. Update weights. > 7. Repeat steps 2-6 until convergence.

What is the value of \(\Delta w_0\) after 100 iterations?

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: 2.5.1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: > 2. 100 training examples, 6D features, 100 test queries. > 3. 1000 training examples, 6D features, 1000 test queries. > 4. 10000 training examples, 6D features, 10000 test queries. > 5. 100000 training examples, 6D features, 100000 test queries. > 6. 1000000 training examples, 6D features, 1000000 test queries. > 7. 10000000 training examples, 6D features, 10000000 test queries. > 8. 100000000 training examples, 6D features, 100000000 test queries. > 9. 1000000000 training examples, 6D features, 1000000000 test queries. > 10.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: > 3. Classifier for college admissions requiring justification. > 4. Classifier for credit card fraud requiring justification. > 5. Classifier for medical diagnosis requiring justification. > 6. Classifier for stock market prediction requiring justification. > 7. Classifier for movie recommendation requiring justification. > 8. Classifier for text classification requiring justification. > 9. Classifier for text summarization requiring justification. > 10. Classifier for web search ranking requiring justification. > 11. Classifier for image classification requiring justification. > 12. Classifier for image segmentation requiring justification. > 13. Classifier for recommender systems requiring justification. > 14. Classifier for recommender systems requiring justification. > 15. Classifier for recommender systems requiring justification. > 16. Classifier for recommender systems requiring justification. > 17. Classifier for recommender systems requiring justification. > 18. Classifier for recommender systems requiring justification. > 19. Classifier for recommender systems requiring justification. > 20. Classifier for recommender systems requiring justification. > 21. Classifier for recommender systems requiring justification.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: > 4. Recommendation for a user with 1000 items. > 5. Recommendation for a user with 1000 items, with a large number of items that are not relevant. > 6. Recommendation for a user with 1000 items, with a large number of items that are not relevant, and frequent updates. > 7. Recommendation for a user with 1000 items, with a large number of items that are not relevant, and frequent updates, and a large number of items that are irrelevant. > 8. Recommendation for a user with 1000 items, with a large number of items that are not relevant, and frequent updates, and a large number of items that are irrelevant, and a large number of items that are irrelevant. > 9. Recommendation for a user with 1000 items, with a large number of items that are not relevant, and frequent updates, and a large number of items that are irrelevant, and a large number of items that are irrelevant, and a large number of items that are irrelevant. > 10.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: > 5. Predicting the outcome of a political election. > 6. Predicting the outcome of a sporting event. > 7. Predicting the outcome of a medical diagnosis. > 8. Predicting the outcome of a financial transaction. > 9. Predicting the outcome of a stock market. > 10. Predicting the outcome of a movie. > 11. Predicting the outcome of a movie rating. > 12. Predicting the outcome of a movie recommendation. > 13. Predicting the outcome of a movie recommendation based on user ratings. > 14. Predicting the outcome of a movie recommendation based on user behavior. > 15. Predicting the outcome of a movie recommendation based on user preferences. > 16. Predicting the outcome of a movie recommendation based on user reviews. > 17. Predicting the outcome of a movie recommendation based on user demographics. > 18. Predicting the outcome of a movie recommendation based on user behavior and demographics. > 19. Predicting the outcome of a movie recommendation based on user behavior and preferences. > 20.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: error vs. complexity > 2. SVM error vs. complexity > 3. Decision Tree error vs. complexity > 4. Random Forest error vs. complexity > 5. KNN error vs. complexity > 6. Naive Bayes error vs. complexity > 7. Gradient Boosting error vs. complexity > 8. Neural Network error vs. complexity > 9. K-means error vs. complexity > 10. Hierarchical clustering error vs. complexity > 11. K-medoids error vs. complexity > 12. K-means vs. K-medoids error vs. complexity > 13. K-means vs. Hierarchical clustering error vs. complexity > 14. K-medoids vs. Hierarchical clustering error vs. complexity > 15. K-means vs. K-medoids vs. Hierarchical clustering error vs. complexity > 16. K-means vs. K-medoids vs. Neural Network error vs. complexity > 17. K-medoids vs. Neural Network error vs. complexity > 18. K-medoids vs. Neural Network error vs. complexity > 19.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: vs. Logistic Regression (10 pts) > 3. SVM vs. Logistic Regression (10 pts) > 4. SVM vs. Decision Trees (10 pts) > 5. SVM vs. Random Forest (10 pts) > 6. SVM vs. KNN (10 pts) > 7. SVM vs. Naive Bayes (10 pts) > 8. SVM vs. K-Means (10 pts) > 9. SVM vs. K-Means (10 pts) > 10. SVM vs. K-Means (10 pts) > 11. SVM vs. K-Means (10 pts) > 12. SVM vs. K-Means (10 pts) > 13. SVM vs. K-Means (10 pts) > 14. SVM vs. K-Means (10 pts) > 15. SVM vs. K-Means (10 pts) > 16. SVM vs. K-Means (10 pts) > 17. SVM vs.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 3. Decision Tree (variable depth) > 3. Random Forest > 3. Support Vector Machine (linear kernel) > 3. Support Vector Machine (RBF kernel) > 3. K-Nearest Neighbor (k=3) > 3. K-Nearest Neighbor (k=5) > 3. K-Nearest Neighbor (k=10) > 3. K-Nearest Neighbor (k=20) > 3. K-Nearest Neighbor (k=50) > 3. K-Nearest Neighbor (k=100) > 3. K-Nearest Neighbor (k=200) > 3. K-Nearest Neighbor (k=500) > 3. K-Nearest Neighbor (k=1000) > 3. K-Nearest Neighbor (k=2000) > 3. K-Nearest Neighbor (k=5000) > 3. K-Nearest Neighbor (k=10000) > 3.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 3. Neural Network (with L2 regularization) > 2. SVM (no regularization) > 1. SVM (with L2 regularization) > 0. No regularization > 0.5. No regularization, but with a small learning rate > 0.0. No regularization, and with a very small learning rate > 0.001. No regularization, and with a very small learning rate > 0.0001. No regularization, and with a very small learning rate > 0.00001. No regularization, and with a very small learning rate > 0.000001. No regularization, and with a very small learning rate > 0.0000001. No regularization, and with a very small learning rate > 0.00000001. No regularization, and with a very small learning rate > 0.000000001. No regularization, and with a very small learning rate > 0.0000000001. No regularization, and with a very small learning rate > 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 4. Kernel trick (\(c > \infty\)) > 3. Non-linear (\(c > \infty\)) > 2. Non-linear (\(c < \infty\)) > 1. Linear (\(c < \infty\)) > 0. No error (\(c = \infty\)) > 0. No complexity (\(c = 0\)) > 0. No error (\(c = \infty\)) and no complexity (\(c = 0\)) > 0. No error (\(c = \infty\)) and no complexity (\(c = 0\)) and no bias (\(b = 0\)) > 0. No error (\(c = \infty\)) and no complexity (\(c = 0\)) and no bias (\(b = 0\)) and no noise (\(n = 0\)) > 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 2. 2-NN (unequal weighted) > 3. 3-NN (equally weighted) > 4. 3-NN (unequal weighted) > 5. 3-NN (equally weighted) with a 3rd nearest neighbor removed > 6. 3-NN (unequal weighted) with a 3rd nearest neighbor removed > 7. 3-NN (unequal weighted) with a 2nd nearest neighbor removed > 8. 3-NN (unequal weighted) with a 2nd nearest neighbor removed and a 3rd nearest neighbor removed > 9. 3-NN (unequal weighted) with a 2nd nearest neighbor removed and a 3rd nearest neighbor removed and a 4th nearest neighbor removed > 10. 3-NN (unequal weighted) with a 2nd nearest neighbor removed and a 3rd nearest neighbor removed and a 4th nearest neighbor removed and a 5th nearest neighbor removed > 11.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 3. Regression Trees (leaf size=2) > 4. Regression Trees (leaf size=3) > 5. Regression Trees (leaf size=4) > 6. Regression Trees (leaf size=5) > 7. Regression Trees (leaf size=6) > 8. Regression Trees (leaf size=7) > 9. Regression Trees (leaf size=8) > 10. Regression Trees (leaf size=9) > 11. Regression Trees (leaf size=10) > 12. Regression Trees (leaf size=11) > 13. Regression Trees (leaf size=12) > 14. Regression Trees (leaf size=13) > 15. Regression Trees (leaf size=14) > 16. Regression Trees (leaf size=15) > 17. Regression Trees (leaf size=16) > 18. Regression Trees (leaf size=17) > 19. Regression Trees (leaf size=18) > 20. Regression Trees (leaf size=19) > 21. Regression Trees (leaf size=20) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: (10 pts) > 4. Logistic Regression (10 pts) > 5. SVM (10 pts) > 6. Decision Tree (10 pts) > 7. K-NN (10 pts) > 8. Random Forest (10 pts) > 9. Gradient Boosting (10 pts) > 10. Lasso Regression (10 pts) > 11. Elastic Net Regression (10 pts) > 12. Ridge Regression (10 pts) > 13. Elastic Net Regression (10 pts) > 14. Lasso Regression (10 pts) > 15. Elastic Net Regression (10 pts) > 16. Ridge Regression (10 pts) > 17. Elastic Net Regression (10 pts) > 18. Lasso Regression (10 pts) > 19. Elastic Net Regression (10 pts) > 20. Gradient Boosting (10 pts) > 21. Random Forest (10 pts) > 22. K-NN (10 pts) > 23.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (10 pts) Draw the output for a multi-layer neural network with 2 inputs, 3 hidden units, and 1 output. The hidden units are fully connected to the inputs and to each other. The output is a sigmoid function. > 5. Logistic Regression (10 pts) Draw the output for a logistic regression model with 2 inputs and 1 output. The output is a sigmoid function. > 3. Decision Tree (10 pts) Draw the output for a decision tree with 2 inputs and 1 output. The output is a sigmoid function. > 2. Random Forest (10 pts) Draw the output for a random forest with 2 inputs and 1 output. The output is a sigmoid function. > 1. Support Vector Machine (10 pts) Draw the output for a support vector machine with 2 inputs and 1 output. The output is a sigmoid function.

What is the output for a multi-layer neural network with 2 inputs, 3 hidden units, and 1 output, where the hidden units are fully connected to the inputs and to each other, and the output is a sigmoid function?

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: (e) \(x_1 + x_2 = 1\) (f) \(x_1 + 2x_2 = 1\) (g) \(x_1 + x_2 = -1\) (h) \(x_1 + 2x_2 = -1\) > 2. Find the maximum margin for the following data points: Negative: (-1, 0), (2, -2), (1, 0), (0, 1), (0, -1) Positive: (1, 0), (0, 1), (0, -1), (1, 0), (1, -1) > 3.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: > 3. Which of the following kernels can be used for a linear SVM? (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 4. Which of the following kernels can be used for a linear SVM? (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 5. Which of the following kernels can be used for a linear SVM? (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 6.

