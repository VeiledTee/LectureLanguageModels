QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: The tree should be a binary tree with 3 nodes. The root node should have a value of 0. The left and right subtrees should have values of 1 and 2, respectively. The tree should be balanced. The tree should be a binary tree with 3 nodes. The root node should have a value of 0. The left and right subtrees should have values of 1 and 2, respectively. The tree should be balanced. The tree should be a binary tree with 3 nodes. The root node should have a value of 0. The left and right subtrees should have values of 1 and 2, respectively. The tree should be balanced. The tree should be a binary tree with 3 nodes. The root node should have a value of 0. The left and right subtrees should have values of 1 and 2, respectively. The tree should be balanced. The tree should be a binary tree with 3 nodes. The root node should have a value of 0. The left and right subtrees should have values of 1 and 2, respectively. The tree should be balanced. The tree should be a binary tree with 3 nodes.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points. The decision boundary is the line that separates the positive and negative data points.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: **Question 13**

Write a Python function to convert a string of comma-separated numbers into a list of integers. The function should handle invalid input and raise an exception if the input is not a string.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: A) Negative B) Positive C) None of the above D) The tree predicts nothing.

Answer: C) None of the above

**Section 4: Data Preprocessing**

* Question 1: What is the difference between normalization and standardization?
* Question 2: Which of the following is a type of data preprocessing technique?
A) Data transformation
B) Data aggregation
C) Data filtering
D) Data transformation

Answer: C) Data transformation

**Section 5: Machine Learning**

* Question 1: What is the difference between supervised and unsupervised learning?
* Question 2: Which of the following is a type of machine learning algorithm?
A) Decision tree
B) Random forest
C) Neural network
D) Support vector machine

Answer: C) Neural network

**Section 6: Model Evaluation**

* Question 1: What is the purpose of model evaluation?

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: 2. What is the nearest neighbor of a point in the dataset? 3. What is the nearest neighbor of a point in the dataset? 4. What is the nearest neighbor of a point in the dataset? 5. What is the nearest neighbor of a point in the dataset? 6. What is the nearest neighbor of a point in the dataset? 7. What is the nearest neighbor of a point in the dataset? 8. What is the nearest neighbor of a point in the dataset? 9. What is the nearest neighbor of a point in the dataset? 10. What is the nearest neighbor of a point in the dataset? 11. What is the nearest neighbor of a point in the dataset? 12. What is the nearest neighbor of a point in the dataset? 13. What is the nearest neighbor of a point in the dataset? 14. What is the nearest neighbor of a point in the dataset? 15. What is the nearest neighbor of a point in the dataset? 16. What is the nearest neighbor of a point in the dataset? 17. What is the nearest neighbor of a point in the dataset? 18.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: **Answer:** 1-NN predicts the class with the highest probability (1) for the input (1, -1.01).

**Question 10:** What is the difference between a decision tree and a random forest?

**Answer:** A decision tree is a single tree that splits the data into subsets based on features, while a random forest is an ensemble of decision trees that combines multiple trees to improve accuracy and reduce overfitting.

**Question 11:** What is the difference between a support vector machine (SVM) and a k-nearest neighbors (KNN) algorithm?

**Answer:** A SVM is a supervised learning algorithm that finds the hyperplane that maximally separates classes, while a KNN is a supervised learning algorithm that predicts the class of a new instance based on the majority vote of its k nearest neighbors.

**Question 12:** What is the purpose of feature selection in machine learning?

**Answer:** Feature selection is the process of selecting a subset of the most relevant features from a large set of features to improve the performance of a machine learning model.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: (4 pts)

**Answer:** 3-NN predicts the class of the new point (1, -1.01) to be a 3-nearest neighbor, which is a class of the training data.

**Question 14:** What is the difference between a decision tree and a random forest? (2 pts)

**Answer:** A decision tree is a single tree that splits the data into subsets based on features, while a random forest is an ensemble of decision trees that combines multiple trees to improve accuracy.

**Question 15:** What is the difference between a classification model and a regression model? (2 pts)

**Answer:** A classification model predicts a categorical output (e.g., spam vs. not spam), while a regression model predicts a continuous output (e.g., the price of a house).

**Question 16:** What is the difference between a bias-variance tradeoff and a overfitting-underfitting tradeoff? (2 pts)

**Answer:** A bias-variance tradeoff refers to the tradeoff between the error introduced by simplifying a model (bias) and the error introduced by fitting the noise in the data (variance).

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: **Question 3**

Write a Python function to implement the perceptron algorithm. The function should take the following arguments:

- `X`: a 2D array of input data points
- `y`: a 1D array of output labels
- `w`: a 1D array of weights
- `b`: a scalar bias term

The function should return the following:

- `y_pred`: a 1D array of predicted output labels
- `w_pred`: a 1D array of predicted weights

The function should implement the perceptron algorithm as follows:

- Initialize the weights and bias term randomly
- For each input data point, compute the output label using the weights and bias term
- If the output label is positive, update the weights and bias term using the perceptron rule
- Repeat until convergence or a stopping criterion is reached

Your code should pass the following test case:
```python
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: **Answer concisely in English without code:** 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).

**Answer concisely in English without code:** 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).

**Answer concisely in English without code:** 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: Answer: The 3-point dataset is classified as positive by the 3-point dataset.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: 2. Compute output of hidden layer with weights \(w_3=0, w_4=1, w_5=1\). 3. Compute output of output layer with weights \(w_6=0, w_7=1, w_8=1\). 4. Compute output of neural net with weights \(w_9=0, w_10=1, w_11=1\). 5. Compute output of neural net with weights \(w_12=0, w_13=1, w_14=1\). 6. Compute output of neural net with weights \(w_15=0, w_16=1, w_17=1\). 7. Compute output of neural net with weights \(w_18=0, w_19=1, w_20=1\). 8. Compute output of neural net with weights \(w_21=0, w_22=1, w_23=1\). 9.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: **Answer:**

To calculate the change in weights for the second layer, we need to know the output of the first layer and the error between the predicted output and the actual output.

The output of the first layer is \(y_1 = 2\) and the error is \(e = 2 - 0 = 2\).

The output of the first layer is \(y_2 = 2\) and the error is \(e = 2 - 2 = 0\).

The output of the first layer is \(y_3 = 2\) and the error is \(e = 2 - 2 = 0\).

The output of the first layer is \(y_4 = 2\) and the error is \(e = 2 - 2 = 0\).

The output of the first layer is \(y_5 = 2\) and the error is \(e = 2 - 2 = 0\).

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.5 Naive Bayes (8 pts) | 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: **Answer:**

1. Gradient Boosting

**Reasoning:** Gradient Boosting is a popular algorithm for classification problems with large datasets and complex interactions between features. It's particularly effective when dealing with noisy or missing data, as it can handle these issues by iteratively refining the model. The 1M training examples and 6D features are sufficient to train a robust model, while the 100 test queries provide a good test set to evaluate the model's performance.

**Answer:**

1. 1M training examples, 6D features, 100 test queries.

**Reasoning:** The 1M training examples and 6D features are sufficient to train a robust model, while the 100 test queries provide a good test set to evaluate the model's performance. The 1M training examples are necessary to ensure the model is not overfitting to the training data. The 6D features are sufficient to capture the underlying patterns in the data.

**Answer:**

1. 1M training examples, 6D features, 100 test queries.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: **Scoring:**

* 6.034 Quiz 2, Spring 2005: 6 points
* 6 Learning Algorithms: 6 points
* Total: 12 points

**Note:** This is just a sample quiz and you should adjust the questions and scoring to fit your specific needs and the level of your students.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: **Question 3: Data Preprocessing**

Write a Python function to perform data preprocessing on a given dataset.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: **5. What is the difference between a hypothesis and a theory?**

A hypothesis is a educated guess that can be tested, while a theory is a well-substantiated explanation for a set of phenomena.

**6. What is the difference between a correlation and a causation?**

Correlation refers to a statistical relationship between two variables, while causation implies a direct cause-and-effect relationship.

**7. What is the difference between a primary and secondary source?**

Primary sources are original materials, such as diaries or photographs, while secondary sources are interpretations or analyses of primary sources.

**8. What is the difference between a hypothesis and a theory in science?**

A hypothesis is a tentative explanation for a phenomenon, while a theory is a well-substantiated explanation that has been extensively tested and supported by evidence.

**9. What is the difference between a correlation and a causation in statistics?**

Correlation refers to a statistical relationship between two variables, while causation implies a direct cause-and-effect relationship.

**10. What is the difference between a hypothesis and a theory in philosophy?

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: (10 points) > 2. Decision Tree (10 points) > 3. K-Nearest Neighbors (10 points) > 4. Support Vector Machine (10 points) > 5. Random Forest (10 points) > 6. Gradient Boosting (10 points) > 7. Neural Network (10 points) > 8. K-Means Clustering (10 points) > 9. Hierarchical Clustering (10 points) > 10. Principal Component Analysis (10 points) > 11. K-Means Clustering (10 points) > 12. Hierarchical Clustering (10 points) > 13. K-Means Clustering (10 points) > 14. Hierarchical Clustering (10 points) > 15. K-Means Clustering (10 points) > 16. K-Means Clustering (10 points) > 17. K-Means Clustering (10 points) > 18. K-Means Clustering (10 points) > 19.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: (15 pts) > 3. Decision Tree (15 pts) > 4. Random Forest (15 pts) > 5. Gradient Boosting (15 pts) > 6. K-Nearest Neighbors (15 pts) > 7. K-Means Clustering (15 pts) > 8. Hierarchical Clustering (15 pts) > 9. Principal Component Analysis (15 pts) > 10. Singular Value Decomposition (15 pts) > 11. K-Means Clustering (15 pts) > 12. Hierarchical Clustering (15 pts) > 13. K-Means Clustering (15 pts) > 14. K-Means Clustering (15 pts) > 15. K-Means Clustering (15 pts) > 16. K-Means Clustering (15 pts) > 17. K-Means Clustering (15 pts) > 18. K-Means Clustering (15 pts) > 19. K-Means Clustering (15 pts) > 20.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 4. Decision Tree (variable depth) > 5. Decision Tree (random depth) > 6. Decision Tree (random depth) > 7. Decision Tree (random depth) > 8. Decision Tree (random depth) > 9. Decision Tree (random depth) > 10.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 5. Decision Tree (no pruning) > 6. K-Nearest Neighbors (no distance metric) > 7. K-Means (no distance metric) > 8. SVM (no kernel) > 9. Random Forest (no feature selection) > 10. Gradient Boosting (no ensemble) > 11. Naive Bayes (no prior distribution) > 12. Support Vector Machine (no kernel) > 13. K-Nearest Neighbors (no distance metric) > 14. Decision Tree (no pruning) > 15. K-Means (no distance metric) > 16. K-Nearest Neighbors (no distance metric) > 17. Random Forest (no feature selection) > 18. Gradient Boosting (no ensemble) > 19. Naive Bayes (no prior distribution) > 20. Support Vector Machine (no kernel) > 21. K-Nearest Neighbors (no distance metric) > 22. Decision Tree (no pruning) > 23. K-Means (no distance metric) > 24.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 6. SVM (\(c > \infty\)) > 7. SVM (\(c = \infty\)) > 8. SVM (\(c < \infty\)) > 9. SVM (\(c > \infty\)) > 10. SVM (\(c = \infty\)) > 11. SVM (\(c < \infty\)) > 12. SVM (\(c > \infty\)) > 13. SVM (\(c = \infty\)) > 14. SVM (\(c < \infty\)) > 15. SVM (\(c > \infty\)) > 16. SVM (\(c = \infty\)) > 17. SVM (\(c < \infty\)) > 18. SVM (\(c > \infty\)) > 19. SVM (\(c = \infty\)) > 20. SVM (\(c < \infty\)) > 21.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 2-NN (weighted) > 3-NN (weighted) > 4-NN (weighted) > 5-NN (weighted) > 6-NN (weighted) > 7-NN (weighted) > 8-NN (weighted) > 9-NN (weighted) > 10-NN (weighted) > 11-NN (weighted) > 12-NN (weighted) > 13-NN (weighted) > 14-NN (weighted) > 15-NN (weighted) > 16-NN (weighted) > 17-NN (weighted) > 18-NN (weighted) > 19-NN (weighted) > 20-NN (weighted) > 21-NN (weighted) > 22-NN (weighted) > 23-NN (weighted) > 24-NN (weighted) > 25-NN (weighted) > 26-NN (weighted) > 27-NN (weighted) > 28-NN (weighted) > 29-NN (weighted) > 30-NN (weighted) > 3

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 3. Regression Trees (leaf size=2) > 4. Regression Trees (leaf size=3) > 5. Regression Trees (leaf size=4) > 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: (12 pts) Draw a simple neural network with 2 inputs, 2 hidden units, and 1 output. The network should have a single hidden layer with 2 neurons. The output layer should have 1 neuron with a linear activation function.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (12 pts) Draw a multi-layer neural network with 3 hidden layers and 1 output layer. The input layer has 10 neurons, the hidden layers have 5 neurons each, and the output layer has 1 neuron. The activation function for the hidden layers is ReLU, and the activation function for the output layer is sigmoid. The weights and biases are initialized randomly. The network is trained using stochastic gradient descent with a learning rate of 0.1. The loss function is the mean squared error. The network is trained for 1000 iterations and the loss function is updated after each iteration. The network is trained on the training set and evaluated on the test set. The accuracy is calculated as the number of correctly classified instances divided by the total number of instances. The accuracy is calculated for each iteration and the average accuracy is calculated. The accuracy is calculated for each epoch and the average accuracy is calculated. The accuracy is calculated for each iteration and the average accuracy is calculated. The accuracy is calculated for each epoch and the average accuracy is calculated. The accuracy is calculated for each iteration and the average accuracy is calculated. The accuracy is calculated for each epoch and the average accuracy is calculated.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: (e) \(x_1 + 4x_2 = 0\) (f) \(x_1 + 5x_2 = 0\) (g) \(x_1 + 6x_2 = 0\) (h) \(x_1 + 7x_2 = 0\) (i) \(x_1 + 8x_2 = 0\) (j) \(x_1 + 9x_2 = 0\) (k) \(x_1 + 10x_2 = 0\) (l) \(x_1 + 11x_2 = 0\) (m) \(x_1 + 12x_2 = 0\) (n) \(x_1 + 13x_2 = 0\) (o) \(x_1 + 14x_2 = 0\) (p) \(x_1 + 15x_2 = 0\) (q) \(x_1 + 16x_2 = 0\) (r

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: (e) RBF (\(\sigma=2.0\)) (f) RBF (\(\sigma=3.0\)) (g) RBF (\(\sigma=4.0\)) (h) RBF (\(\sigma=5.0\)) (i) RBF (\(\sigma=6.0\)) (j) RBF (\(\sigma=7.0\)) (k) RBF (\(\sigma=8.0\)) (l) RBF (\(\sigma=9.0\)) (m) RBF (\(\sigma=10.0\)) (n) RBF (\(\sigma=11.0\)) (o) RBF (\(\sigma=12.0\)) (p) RBF (\(\sigma=13.0\)) (q) RBF (\(\sigma=14.0\)) (r) RBF (\(\sigma=15.0\)) (s) RBF (\(\sigma=16.0\)) (t) RBF (\(\sigma=17.

