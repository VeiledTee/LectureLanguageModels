QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data. 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data. 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data. 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph. 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph. 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph. 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree. 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree. 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree. 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)? 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)? 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: 1. Draw 2-NN decision boundaries. 2. Draw 3-NN decision boundaries. 3. Draw 4-NN decision boundaries. 4. Draw 5-NN decision boundaries. 5. Draw 6-NN decision boundaries. 6. Draw 7-NN decision boundaries. 7. Draw 8-NN decision boundaries. 8. Draw 9-NN decision boundaries. 9. Draw 10-NN decision boundaries. 10. Draw 11-NN decision boundaries. 11. Draw 12-NN decision boundaries. 12. Draw 13-NN decision boundaries. 13. Draw 14-NN decision boundaries. 14. Draw 15-NN decision boundaries. 15. Draw 16-NN decision boundaries. 16. Draw 17-NN decision boundaries. 17. Draw 18-NN decision boundaries. 18. Draw 19-NN decision boundaries. 19. Draw 20-NN decision boundaries. 20. Draw 21-NN decision boundaries. 21. Draw 22-NN decision boundaries.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain. 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2). 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2). 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2). 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01). 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01). 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01). 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1. 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1. 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1. 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_3\) via backpropagation for input (2,2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_4\) via backpropagation for input (2,2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9pts) > 2. Calculate \(\Delta w_5\) via backpropagation for input (2,2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9pts) > 2. Calculate \(\Delta w_6\) via backpropagation for input (2,2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9pts) > 2. Calculate \(\Delta w_7\) via backpropagation for input (2,2). 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction. 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature. 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: > 2.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: > 3. Classifier for elementary school special education requiring justification. > 4. Classifier for middle school special education requiring justification. > 5. Classifier for high school special education requiring justification. > 6. Classifier for college special education requiring justification. > 7. Classifier for graduate special education requiring justification. > 8. Classifier for graduate special education requiring justification. > 9. Classifier for graduate special education requiring justification. > 10. Classifier for graduate special education requiring justification. > 11. Classifier for graduate special education requiring justification. > 12. Classifier for graduate special education requiring justification. > 13. Classifier for graduate special education requiring justification. > 14. Classifier for graduate special education requiring justification. > 15. Classifier for graduate special education requiring justification. > 16. Classifier for graduate special education requiring justification. > 17. Classifier for graduate special education requiring justification. > 18. Classifier for graduate special education requiring justification. > 19. Classifier for graduate special education requiring justification. > 20. Classifier for graduate special education requiring justification. > 21.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: > 4.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: > 5. Weather forecasting from satellite data. > 6. Weather forecasting from radar data. > 7. Weather forecasting from radar data. > 8. Weather forecasting from radar data. > 9. Weather forecasting from radar data. > 10. Weather forecasting from radar data. > 11. Weather forecasting from radar data. > 12. Weather forecasting from radar data. > 13. Weather forecasting from radar data. > 14. Weather forecasting from radar data. > 15. Weather forecasting from radar data. > 16. Weather forecasting from radar data. > 17. Weather forecasting from radar data. > 18. Weather forecasting from radar data. > 19. Weather forecasting from radar data. > 20. Weather forecasting from radar data. > 21. Weather forecasting from radar data. > 22. Weather forecasting from radar data. > 23. Weather forecasting from radar data. > 24. Weather forecasting from radar data. > 25. Weather forecasting from radar data. > 26. Weather forecasting from radar data. > 27. Weather forecasting from radar data. > 28.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: (10 points) > 2. Gradient Descent (10 points) > 3. Sigmoid (10 points) > 4. Linear Regression (10 points) > 5. Logistic Regression (10 points) > 6. Decision Trees (10 points) > 7. Random Forest (10 points) > 8. Gradient Boosting (10 points) > 9. Neural Networks (10 points) > 10. Support Vector Machines (10 points) > 11. K-Means Clustering (10 points) > 12. Clustering (10 points) > 13. Hierarchical Clustering (10 points) > 14. Principal Component Analysis (10 points) > 15. Isolation Forest (10 points) > 16. Random Forest (10 points) > 17. Gradient Boosting (10 points) > 18. Neural Networks (10 points) > 19. Decision Trees (10 points) > 20. Support Vector Machines (10 points) > 21. Clustering (10 points) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: (15 points) > 3. Decision Trees (15 points) > 4. Random Forest (15 points) > 5. Gradient Boosting (15 points) > 6. Naive Bayes (15 points) > 7. Support Vector Machines (15 points) > 8. K-Nearest Neighbors (15 points) > 9. Random Forest (15 points) > 10. Support Vector Machines (15 points) > 11. Decision Trees (15 points) > 12. Random Forest (15 points) > 13. Gradient Boosting (15 points) > 14. Naive Bayes (15 points) > 15. Support Vector Machines (15 points) > 16. K-Nearest Neighbors (15 points) > 17. Random Forest (15 points) > 18. Support Vector Machines (15 points) > 19. Decision Trees (15 points) > 20. Random Forest (15 points) > 21. Support Vector Machines (15 points) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 4. Random Forest (random depth) > 5. Gradient Boosting (random depth) > 6. Decision Tree (fixed depth) > 7. Random Forest (random depth) > 8. Gradient Boosting (random depth) > 9. Decision Tree (fixed depth) > 10. Random Forest (random depth) > 11. Gradient Boosting (random depth) > 12. Decision Tree (fixed depth) > 13. Random Forest (random depth) > 14. Gradient Boosting (random depth) > 15. Decision Tree (fixed depth) > 16. Random Forest (random depth) > 17. Gradient Boosting (random depth) > 18. Decision Tree (fixed depth) > 19. Random Forest (random depth) > 20. Gradient Boosting (random depth) > 21. Decision Tree (fixed depth) > 22. Random Forest (random depth) > 23. Gradient Boosting (random depth) > 24. Decision Tree (fixed depth) > 25. Random Forest (random depth) > 26.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 5. K-Nearest Neighbors (no regularization) > 6. Decision Tree (no regularization) > 7. Random Forest (no regularization) > 8. Gradient Boosting (no regularization) > 9. Support Vector Machine (no regularization) > 10. Naive Bayes (no regularization) > 11. Logistic Regression (no regularization) > 12. Decision Trees (no regularization) > 13. Random Forest (no regularization) > 14. Gradient Boosting (no regularization) > 15. Support Vector Machine (no regularization) > 16. Naive Bayes (no regularization) > 17. Logistic Regression (no regularization) > 18. Decision Trees (no regularization) > 19. Random Forest (no regularization) > 20. Gradient Boosting (no regularization) > 21. Support Vector Machine (no regularization) > 22. Naive Bayes (no regularization) > 23. Decision Trees (no regularization) > 24. Random Forest (no regularization) > 25. Gradient Boosting (no regularization) > 26.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 6. SVM (\(c < \infty\)) > 7. SVM (\(c < \infty\)) > 8. SVM (\(c < \infty\)) > 9. SVM (\(c < \infty\)) > 10. SVM (\(c < \infty\)) > 11. SVM (\(c < \infty\)) > 12. SVM (\(c < \infty\)) > 13. SVM (\(c < \infty\)) > 14. SVM (\(c < \infty\)) > 15. SVM (\(c < \infty\)) > 16. SVM (\(c < \infty\)) > 17. SVM (\(c < \infty\)) > 18. SVM (\(c < \infty\)) > 19. SVM (\(c < \infty\)) > 20. SVM (\(c < \infty\)) > 21.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 2. 3-NN (equally weighted) > 3. 4-NN (equally weighted) > 4. 5-NN (equally weighted) > 5. 6-NN (equally weighted) > 6. 7-NN (equally weighted) > 7. 8-NN (equally weighted) > 8. 9-NN (equally weighted) > 9. 10-NN (equally weighted) > 10. 11-NN (equally weighted) > 11. 12-NN (equally weighted) > 12. 13-NN (equally weighted) > 13. 14-NN (equally weighted) > 14. 15-NN (equally weighted) > 15. 16-NN (equally weighted) > 16. 17-NN (equally weighted) > 17. 18-NN (equally weighted) > 18. 19-NN (equally weighted) > 19.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 3. Regression Trees (leaf size=2) > 4. Regression Trees (leaf size=3) > 5. Regression Trees (leaf size=4) > 6. Regression Trees (leaf size=5) > 7. Regression Trees (leaf size=6) > 8. Regression Trees (leaf size=7) > 9. Regression Trees (leaf size=8) > 10. Regression Trees (leaf size=9) > 11. Regression Trees (leaf size=10) > 12. Regression Trees (leaf size=11) > 13. Regression Trees (leaf size=12) > 14. Regression Trees (leaf size=13) > 15. Regression Trees (leaf size=14) > 16. Regression Trees (leaf size=15) > 17. Regression Trees (leaf size=16) > 18. Regression Trees (leaf size=17) > 19. Regression Trees (leaf size=18) > 20. Regression Trees (leaf size=19) > 21. Regression Trees (leaf size=20) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: (10pts) > 4. Convolutional Neural Network (10pts) > 5. Recurrent Neural Network (10pts) > 6. Convolutional Autoencoder (10pts) > 7. Convolutional Neural Network (10pts) > 8. Convolutional Autoencoder (10pts) > 9. Convolutional Neural Network (10pts) > 10. Convolutional Neural Network (10pts) > 11. Convolutional Neural Network (10pts) > 12. Convolutional Neural Network (10pts) > 13. Convolutional Neural Network (10pts) > 14. Convolutional Neural Network (10pts) > 15. Convolutional Neural Network (10pts) > 16. Convolutional Neural Network (10pts) > 17. Convolutional Neural Network (10pts) > 18. Convolutional Neural Network (10pts) > 19. Convolutional Neural Network (10pts) > 20.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (MLN) > 5. Convolutional Neural Network (CNN) > 6. Recurrent Neural Network (RNN) > 7. Long Short-Term Memory (LSTM) > 8. Transformers (10pts) Write a Python function to compute the output of a neural network given a set of input features and a set of target labels.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: (e) \(x_1 + 3x_2 = 0\) (f) \(x_1 + 2x_2 = 0\) (g) \(x_1 + 3x_2 = 0\) (h) \(x_1 + 2x_2 = 0\) (i) \(x_1 + 3x_2 = 0\) (j) \(x_1 + 2x_2 = 0\) (k) \(x_1 + 3x_2 = 0\) (l) \(x_1 + 2x_2 = 0\) (m) \(x_1 + 3x_2 = 0\) (n) \(x_1 + 2x_2 = 0\) (o) \(x_1 + 3x_2 = 0\) (p) \(x_1 + 2x_2 = 0\) (q) \(x_1 + 3x_2 = 0\) (r) \(x_1 +

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: (e) RBF (\(\sigma=1.0\)) (f) RBF (\(\sigma=1.0\)) (g) RBF (\(\sigma=1.0\)) (h) RBF (\(\sigma=1.0\)) (i) RBF (\(\sigma=1.0\)) (j) RBF (\(\sigma=1.0\)) (k) RBF (\(\sigma=1.0\)) (l) RBF (\(\sigma=1.0\)) (m) RBF (\(\sigma=1.0\)) (n) RBF (\(\sigma=1.0\)) (o) RBF (\(\sigma=1.0\)) (p) RBF (\(\sigma=1.0\)) (q) RBF (\(\sigma=1.0\)) (r) RBF (\(\sigma=1.0\)) (s) RBF (\(\sigma=1.0\)) (t) RBF (\(\sigma=1.0\)) (u) RBF (\(\sigma=1.

