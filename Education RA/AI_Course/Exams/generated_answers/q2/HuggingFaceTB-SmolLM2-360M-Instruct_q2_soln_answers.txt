QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: A:

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: 7.034 Quiz 2, Spring 2005 > 2 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. What is the probability that a randomly selected point is positive? 8.034 Quiz 2, Spring 2005 > 3 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What is the probability that a randomly selected point is negative? 9.034 Quiz 2, Spring 2005 > 4 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 5. What is the probability that a randomly selected point is both positive and negative? 10.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: 4. What is the predicted probability of the positive class given the positive test? 5. What is the predicted probability of the negative class given the negative test? 6. What is the predicted probability of the negative class given the negative test? 7. What is the predicted probability of the positive class given the positive test? 8. What is the predicted probability of the negative class given the negative test? 9. What is the predicted probability of the negative class given the positive test? 10. What is the predicted probability of the positive class given the positive test? 11. What is the predicted probability of the negative class given the negative test? 12. What is the predicted probability of the negative class given the negative test? 13. What is the predicted probability of the positive class given the positive test? 14. What is the predicted probability of the negative class given the negative test? 15. What is the predicted probability of the negative class given the negative test? 16. What is the predicted probability of the positive class given the positive test? 17. What is the predicted probability of the negative class given the positive test? 18.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: A:

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: 2. Draw 2-NN decision boundaries. 3. Draw 3-NN decision boundaries. 4. Draw 4-NN decision boundaries. 5. Draw 5-NN decision boundaries. 6. Draw 6-NN decision boundaries. 7. Draw 7-NN decision boundaries. 8. Draw 8-NN decision boundaries. 9. Draw 9-NN decision boundaries. 10. Draw 10-NN decision boundaries. 11. Draw 11-NN decision boundaries. 12. Draw 12-NN decision boundaries. 13. Draw 13-NN decision boundaries. 14. Draw 14-NN decision boundaries. 15. Draw 15-NN decision boundaries. 16. Draw 16-NN decision boundaries. 17. Draw 17-NN decision boundaries. 18. Draw 18-NN decision boundaries. 19. Draw 19-NN decision boundaries. 20. Draw 20-NN decision boundaries. 21. Draw 21-NN decision boundaries. 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: A: 1-NN predicts the class of the nearest training example. In this case, the nearest training example is (1, -1.01), which is the second training example. Therefore, 1-NN predicts the class of the second training example, which is the class of the second training example.

Question: What class does 1-NN predict for (1, -1.01)?

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: A: 3. The class predicted by 3-NN for (1, -1.01) is "A".

What is the class predicted by 3-NN for (1, -1.01)?

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: A:
The perceptron algorithm produces the separator (0, 1, 2) by using the following formula:

y = 1 if x1 < x2 and y = 0 if x1 > x2

In this case, the formula is:

y = 1 if x1 < x2 and y = 0 if x1 > x2

This means that the perceptron will output 1 if the x1 value is less than x2 and 0 if the x1 value is greater than x2. The 0 in the formula is the "0" part of the separator, which means that the perceptron will output 0 if the x1 value is equal to x2.

For example, if the x1 value is -1, the perceptron will output 0 because -1 < -2 and 0 = 0. If the x1 value is 2, the perceptron will output 1 because 2 > -2 and 1 = 1.

So, the perceptron algorithm produces the separator (0, 1, 2) by using the "0" part of the formula.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: A:

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: A:
The perceptron algorithm is a type of feedforward neural network that is used for supervised learning. It is a linear model that can be used to classify data points into two categories.

The perceptron algorithm works by taking a set of inputs and comparing them to a threshold value. If the inputs are greater than the threshold, the output is 1, otherwise the output is 0.

In the case of the 5-point dataset from Problem 1, the perceptron algorithm would take the inputs (1, 0) and (2, -2) and compare them to the threshold value of 3. The perceptron algorithm would output 1 for the first input and 0 for the second input.

The perceptron algorithm is a linear model, which means that it can only learn linearly separable patterns. In other words, the perceptron algorithm can only learn to separate two classes of data points by a straight line.

If the dataset from Problem 1 were linearly separable, the perceptron algorithm would output the correct label for each data point.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: 2. Compute the output of the neural net for the point (1,1) with weights \(w_0=0, w_1=1, w_2=1\). 3. Compute the output of the neural net for the point (1,1) with weights \(w_0=0, w_1=1, w_2=0\). 4. Compute the output of the neural net for the point (1,1) with weights \(w_0=0, w_1=1, w_2=0\). 5. Compute the output of the neural net for the point (1,1) with weights \(w_0=0, w_1=1, w_2=0\). 6. Compute the output of the neural net for the point (1,1) with weights \(w_0=0, w_1=1, w_2=0\). 7. Compute the output of the neural net for the point (1,1) with weights \(w_0=0, w_1=1, w_2=0\). 8.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: 7.034 Quiz 2, Spring 2005 > 6 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction. 8.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: (1 pt)

A:
The most influential feature is \(x_2=1\) because it has the highest probability of \(y=1\) and the highest frequency of \(y=1\) among all the features.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: 2. 1M training examples, 6D features, 100 test queries. 3. 1M training examples, 6D features, 100 test queries. 4. 1M training examples, 6D features, 100 test queries. 5. 1M training examples, 6D features, 100 test queries. 6. 1M training examples, 6D features, 100 test queries. 7. 1M training examples, 6D features, 100 test queries. 8. 1M training examples, 6D features, 100 test queries. 9. 1M training examples, 6D features, 100 test queries. 10. 1M training examples, 6D features, 100 test queries. 11. 1M training examples, 6D features, 100 test queries. 12. 1M training examples, 6D features, 100 test queries. 13. 1M training examples, 6D features, 100 test queries.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification. 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: 4. 3D image reconstruction with 1M features and frequent updates. 5. 3D image reconstruction with 1M features and sparse updates. 6. 3D image reconstruction with 1M features and sparse updates. 7. 3D image reconstruction with 1M features and sparse updates. 8. 3D image reconstruction with 1M features and sparse updates. 9. 3D image reconstruction with 1M features and sparse updates. 10. 3D image reconstruction with 1M features and sparse updates. 11. 3D image reconstruction with 1M features and sparse updates. 12. 3D image reconstruction with 1M features and sparse updates. 13. 3D image reconstruction with 1M features and sparse updates. 14. 3D image reconstruction with 1M features and sparse updates. 15. 3D image reconstruction with 1M features and sparse updates. 16. 3D image reconstruction with 1M features and sparse updates. 17. 3D image reconstruction with 1M features and sparse updates. 18.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides. 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: vs. Perceptron (1 pt) > 2. Perceptron vs. Perceptron (1 pt) > 3. Perceptron vs. Perceptron (1 pt) > 4. Perceptron vs. Perceptron (1 pt) > 5. Perceptron vs. Perceptron (1 pt) > 6. Perceptron vs. Perceptron (1 pt) > 7. Perceptron vs. Perceptron (1 pt) > 8. Perceptron vs. Perceptron (1 pt) > 9. Perceptron vs. Perceptron (1 pt) > 10. Perceptron vs. Perceptron (1 pt) > 11. Perceptron vs. Perceptron (1 pt) > 12. Perceptron vs. Perceptron (1 pt) > 13. Perceptron vs. Perceptron (1 pt) > 14. Perceptron vs. Perceptron (1 pt) > 15. Perceptron vs. Perceptron (1 pt) > 16. Perceptron vs.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: > 3. SVM > 4. SVM > 5. SVM > 6. SVM > 7. SVM > 8. SVM > 9. SVM > 10. SVM > 11. SVM > 12. SVM > 13. SVM > 14. SVM > 15. SVM > 16. SVM > 17. SVM > 18. SVM > 19. SVM > 20. SVM > 21. SVM > 22. SVM > 23. SVM > 24. SVM > 25. SVM > 26. SVM > 27. SVM > 28. SVM > 29. SVM > 30. SVM > 31. SVM > 32. SVM > 33. SVM > 34. SVM > 35. SVM > 36. SVM > 37. SVM > 38. SVM > 39.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 4. Decision Tree (variable depth) > 5. Random Forest (fixed depth) > 6. Random Forest (variable depth) > 7. K-Means Clustering (fixed number of clusters) > 8. K-Means Clustering (variable number of clusters) > 9. K-Means Clustering (fixed number of clusters) > 10. K-Means Clustering (variable number of clusters) > 11. K-Means Clustering (fixed number of clusters) > 12. K-Means Clustering (variable number of clusters) > 13. K-Means Clustering (fixed number of clusters) > 14. K-Means Clustering (variable number of clusters) > 15. K-Means Clustering (fixed number of clusters) > 16. K-Means Clustering (variable number of clusters) > 17. K-Means Clustering (fixed number of clusters) > 18. K-Means Clustering (variable number of clusters) > 19. K-Means Clustering (fixed number of clusters) > 20. K-Means Clustering (variable number of clusters) > 21.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 5. Neural Network (with regularization) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Neural Network (with regularization) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Neural Network (with regularization) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Neural Network (with regularization) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Neural Network (with regularization) > 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\)) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\)) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\)) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\)) > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\)) > 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 2. 1-NN (equally weighted) > 3. 1-NN (uniform) > 4. 2-NN (uniform) > 5. 2-NN (uniform) > 6. 2-NN (uniform) > 7. 2-NN (uniform) > 8. 2-NN (uniform) > 9. 2-NN (uniform) > 10. 2-NN (uniform) > 11. 2-NN (uniform) > 12. 2-NN (uniform) > 13. 2-NN (uniform) > 14. 2-NN (uniform) > 15. 2-NN (uniform) > 16. 2-NN (uniform) > 17. 2-NN (uniform) > 18. 2-NN (uniform) > 19. 2-NN (uniform) > 20. 2-NN (uniform) > 21. 2-NN (uniform) > 22. 2-NN (uniform) > 23.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 3. Regression Trees (leaf size=2) > 4. Regression Trees (leaf size=3) > 5. Regression Trees (leaf size=4) > 6. Regression Trees (leaf size=5) > 7. Regression Trees (leaf size=6) > 8. Regression Trees (leaf size=7) > 9. Regression Trees (leaf size=8) > 10. Regression Trees (leaf size=9) > 11. Regression Trees (leaf size=10) > 12. Regression Trees (leaf size=11) > 13. Regression Trees (leaf size=12) > 14. Regression Trees (leaf size=13) > 15. Regression Trees (leaf size=14) > 16. Regression Trees (leaf size=15) > 17. Regression Trees (leaf size=16) > 18. Regression Trees (leaf size=17) > 19. Regression Trees (leaf size=18) > 20. Regression Trees (leaf size=19) > 21. Regression Trees (leaf size=20) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: (10 pts) Implement a linear neural network with 3 layers, 2 neurons in each layer, and 1 output neuron. Use the following weights: W1 = 0.05, W2 = 0.05, W3 = 0.05, W4 = 0.05, W5 = 0.05, W6 = 0.05, W7 = 0.05, W8 = 0.05, W9 = 0.05, W10 = 0.05, W11 = 0.05, W12 = 0.05, W13 = 0.05, W14 = 0.05, W15 = 0.05, W16 = 0.05, W17 = 0.05, W18 = 0.05, W19 = 0.05, W20 = 0.05, W21 = 0.05, W22 = 0.05, W23 = 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (10 pts) 1. Write a Python function to train a multi-layer neural network on a dataset. 2. Write a Python function to test the trained neural network on a dataset. 3. Write a Python function to evaluate the performance of the trained neural network on a dataset. 4. Write a Python function to predict the output of a dataset using the trained neural network. 5. Write a Python function to train a multi-layer neural network on a dataset. 6. Write a Python function to test the trained neural network on a dataset. 7. Write a Python function to evaluate the performance of the trained neural network on a dataset. 8. Write a Python function to predict the output of a dataset using the trained neural network. 9. Write a Python function to train a multi-layer neural network on a dataset. 10. Write a Python function to test the trained neural network on a dataset. 11. Write a Python function to evaluate the performance of the trained neural network on a dataset. 12. Write a Python function to predict the output of a dataset using the trained neural network. 13. Write a Python function to train a multi-layer neural network on a dataset.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: (e) \(x_1 + 2x_2 = 0\) (f) \(x_1 + 1.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: (e) RBF (\(\sigma=0.5\)) (f) RBF (\(\sigma=1.0\)) (g) RBF (\(\sigma=0.5\)) (h) RBF (\(\sigma=1.0\)) (i) RBF (\(\sigma=0.5\)) (j) RBF (\(\sigma=1.0\)) (k) RBF (\(\sigma=0.5\)) (l) RBF (\(\sigma=1.0\)) (m) RBF (\(\sigma=0.5\)) (n) RBF (\(\sigma=1.0\)) (o) RBF (\(\sigma=0.5\)) (p) RBF (\(\sigma=1.0\)) (q) RBF (\(\sigma=0.5\)) (r) RBF (\(\sigma=1.0\)) (s) RBF (\(\sigma=0.5\)) (t) RBF (\(\sigma=1.0\)) (u) RBF (\(\sigma=0.

