QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: > 2. Write a Python function to calculate the accuracy of a decision tree. > 3. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 4. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 5. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 6. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 7. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 8. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 9. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 10. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 11. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 12. Write a Python function to calculate the accuracy of a decision tree for a given dataset. > 13. Write a Python function to calculate the accuracy of a decision tree for a given dataset.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: > 3. How many decision trees can be constructed from this data? > 4. How many decision trees can be constructed from this data? > 5. How many decision trees can be constructed from this data? > 6. How many decision trees can be constructed from this data? > 7. How many decision trees can be constructed from this data? > 8. How many decision trees can be constructed from this data? > 9. How many decision trees can be constructed from this data? > 10. How many decision trees can be constructed from this data? > 11. How many decision trees can be constructed from this data? > 12. How many decision trees can be constructed from this data? > 13. How many decision trees can be constructed from this data? > 14. How many decision trees can be constructed from this data? > 15. How many decision trees can be constructed from this data? > 16. How many decision trees can be constructed from this data? > 17. How many decision trees can be constructed from this data? > 18. How many decision trees can be constructed from this data? > 19.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: > 4. What is the purpose of the "root" node in a decision tree? > 5. What is the purpose of the "leaf" node in a decision tree? > 6. What is the purpose of the "internal node" in a decision tree? > 7. What is the purpose of the "terminal node" in a decision tree? > 8. What is the purpose of the "branch" node in a decision tree? > 9. What is the purpose of the "branch" node in a decision tree? > 10. What is the purpose of the "branch" node in a decision tree? > 11. What is the purpose of the "branch" node in a decision tree? > 12. What is the purpose of the "branch" node in a decision tree? > 13. What is the purpose of the "branch" node in a decision tree? > 14. What is the purpose of the "branch" node in a decision tree? > 15. What is the purpose of the "branch" node in a decision tree? > 16. What is the purpose of the "branch" node in a decision tree?

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: 1. Positive 2. Negative 3. Neutral 4. Negative 5. Positive 6. Neutral 7. Positive 8. Negative 9. Positive 10. Negative 11. Positive 12. Negative 13. Positive 14. Negative 15. Positive 16. Negative 17. Positive 18. Negative 19. Positive 20. Negative 21. Positive 22. Negative 23. Positive 24. Negative 25. Positive 26. Negative 27. Positive 28. Negative 29. Positive 30. Negative 31. Positive 32. Negative 33. Positive 34. Negative 35. Positive 36. Negative 37. Positive 38. Negative 39. Positive 40. Negative 41. Positive 42. Negative 43. Positive 44. Negative 45. Positive 46. Negative 47. Positive 48. Negative 49. Positive 50. Negative 51. Positive 52. Negative 53.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: > 2. What is the difference between 1-NN and 2-NN? > 3. What is the difference between 1-NN and 2-NN? > 4. What is the difference between 1-NN and 2-NN? > 5. What is the difference between 1-NN and 2-NN? > 6. What is the difference between 1-NN and 2-NN? > 7. What is the difference between 1-NN and 2-NN? > 8. What is the difference between 1-NN and 2-NN? > 9. What is the difference between 1-NN and 2-NN? > 10. What is the difference between 1-NN and 2-NN? > 11. What is the difference between 1-NN and 2-NN? > 12. What is the difference between 1-NN and 2-NN? > 13. What is the difference between 1-NN and 2-NN? > 14. What is the difference between 1-NN and 2-NN?

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: **Section 3: Machine Learning**

**Concept 3: Machine Learning**

**Concept 3.1: Supervised Learning**

**Concept 3.2: Unsupervised Learning**

**Concept 3.3: Regression**

**Concept 3.4: Classification**

**Concept 3.5: Clustering**

**Concept 3.6: Dimensionality Reduction**

**Concept 3.7: Model Evaluation**

**Concept 3.8: Model Selection**

**Concept 3.9: Model Deployment**

**Concept 3.10: Model Interpretability**

**Concept 3.11: Model Ensemble**

**Concept 3.12: Model Selection**

**Concept 3.13: Model Deployment**

**Concept 3.14: Model Evaluation**

**Concept 3.15: Model Selection**

**Concept 3.16: Model Deployment**

**Concept 3.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: **Question 6.035 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 4. What class does 3-NN predict for (1, -1.01)? Explain.

**Question 6.036 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 5. What class does 3-NN predict for (1, -1.01)? Explain.

**Question 6.037 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 6. What class does 3-NN predict for (1, -1.01)? Explain.

**Question 6.038 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 7. What class does 3-NN predict for (1, -1.01)? Explain.

**Question 6.039 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 8.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: **Q1.2: Perceptron (7pts)**

In this question, you will be given a dataset and asked to classify it into one of three categories: positive, negative, or neutral. The dataset consists of 1000 points, each represented by a 2D vector.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: > 3. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes. The perceptron is a linear model that is used to classify data points into one of two classes.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: 2.2.

**Problem 1.2.2: 5-point dataset**

The 5-point dataset is a simple example of a dataset with 5 data points. The data points are represented by the following 5 points:

- (-1, 0)
- (2, -2)
- (1, 0)
- (0, 1)
- (1, 1)

The 5-point dataset is a simple example of a dataset with 5 data points. The data points are represented by the following 5 points:

- (-1, 0)
- (2, -2)
- (1, 0)
- (0, 1)
- (1, 1)

The 5-point dataset is a simple example of a dataset with 5 data points.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: 2. Compute the output of the neural network for the point (2,0) with weights \(w_0=0, w_1=1, w_2=1\). 3. Compute the output of the neural network for the point (0,0) with weights \(w_0=0, w_1=1, w_2=1\). 4. Compute the output of the neural network for the point (0,0) with weights \(w_0=0, w_1=1, w_2=1\). 5. Compute the output of the neural network for the point (0,0) with weights \(w_0=0, w_1=1, w_2=1\). 6. Compute the output of the neural network for the point (0,0) with weights \(w_0=0, w_1=1, w_2=1\). 7. Compute the output of the neural network for the point (0,0) with weights \(w_0=0, w_1=1, w_2=1\). 8.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: **Question 6.035 Quiz 2, Spring 2005 > 5 Neural Net (9 pts) > 3. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).

**Question 6.036 Quiz 2, Spring 2005 > 6 Neural Net (9 pts) > 4. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).

**Question 6.037 Quiz 2, Spring 2005 > 7 Neural Net (9 pts) > 5. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).

**Question 6.038 Quiz 2, Spring 2005 > 8 Neural Net (9 pts) > 6. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).

**Question 6.039 Quiz 2, Spring 2005 > 9 Neural Net (9 pts) > 7.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: **Question 1.2: Naive Bayes (8pts) **

**Answer:** 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.

**Question 1.3: Naive Bayes (8pts) **

**Answer:** 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: > 3. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 4. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 5. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 6. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 7. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 8. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 9. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 10. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 11. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 12. What is the probability of a person having a certain disease given that they have a certain genetic predisposition? > 13.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: > 2. 1M training examples, 1000 test queries. > 3. 1M training examples, 1000 test queries. > 4. 1M training examples, 1000 test queries. > 5. 1M training examples, 1000 test queries. > 6. 1M training examples, 1000 test queries. > 7. 1M training examples, 1000 test queries. > 8. 1M training examples, 1000 test queries. > 9. 1M training examples, 1000 test queries. > 10. 1M training examples, 1000 test queries. > 11. 1M training examples, 1000 test queries. > 12. 1M training examples, 1000 test queries. > 13. 1M training examples, 1000 test queries. > 14. 1M training examples, 1000 test queries. > 15.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: > 3. Algorithm for solving the traveling salesman problem. > 4. Algorithm for solving the knapsack problem. > 5. Algorithm for solving the knapsack problem with a constraint. > 6. Algorithm for solving the knapsack problem with a constraint. > 7. Algorithm for solving the knapsack problem with a constraint. > 8. Algorithm for solving the knapsack problem with a constraint. > 9. Algorithm for solving the knapsack problem with a constraint. > 10. Algorithm for solving the knapsack problem with a constraint. > 11. Algorithm for solving the knapsack problem with a constraint. > 12. Algorithm for solving the knapsack problem with a constraint. > 13. Algorithm for solving the knapsack problem with a constraint. > 14. Algorithm for solving the knapsack problem with a constraint. > 15. Algorithm for solving the knapsack problem with a constraint. > 16. Algorithm for solving the knapsack problem with a constraint. > 17. Algorithm for solving the knapsack problem with a constraint. > 18.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: > 2. The best way to handle a large dataset with a small number of features. > 4. The best way to handle a large dataset with a large number of features. > 5. The best way to handle a large dataset with a small number of features. > 6. The best way to handle a large dataset with a small number of features. > 7. The best way to handle a large dataset with a small number of features. > 8. The best way to handle a large dataset with a small number of features. > 9. The best way to handle a large dataset with a small number of features. > 10. The best way to handle a large dataset with a small number of features. > 11. The best way to handle a large dataset with a small number of features. > 12. The best way to handle a large dataset with a small number of features. > 13. The best way to handle a large dataset with a small number of features. > 14. The best way to handle a large dataset with a small number of features. > 15. The best way to handle a large dataset with a small number of features.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: > 5. The best way to solve a Rubik's Cube. > 6. The best way to find the shortest path between two points.

**Section 2: Problem-Solving Strategies**

* **Problem-Solving Strategies** (20 points)
	+ Identify the problem and its constraints.
	+ Break down the problem into smaller sub-problems.
	+ Develop a plan of attack.
	+ Solve each sub-problem.
	+ Evaluate the solution.
	+ Refine the solution.
	+ Repeat the process until the solution is found.
* **Problem-Solving Strategies** (20 points)
	+ Identify the problem and its constraints.
	+ Break down the problem into smaller sub-problems.
	+ Develop a plan of attack.
	+ Solve each sub-problem.
	+ Evaluate the solution.
	+ Refine the solution.
	+ Repeat the process until the solution is found.
* **Problem-Solving Strategies** (20 points)
	+ Identify the problem and its constraints.
	+ Break down the problem into smaller sub-problems.
	+ Develop a plan of attack.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: > 2. SVM > 3. Random Forest > 4. Gradient Boosting > 5. Neural Network > 6. Gradient Descent > 7. Gradient Trick > 8. Gradient Descent with momentum > 9. Gradient Descent with RMSProp > 10. Gradient Descent with momentum > 11. Gradient Descent with RMSProp > 12. Gradient Descent with momentum > 13. Gradient Descent with RMSProp > 14. Gradient Descent with momentum > 15. Gradient Descent with RMSProp > 16. Gradient Descent with momentum > 17. Gradient Descent with RMSProp > 18. Gradient Descent with momentum > 19. Gradient Descent with RMSProp > 20. Gradient Descent with momentum > 21. Gradient Descent with RMSProp > 22. Gradient Descent with momentum > 23. Gradient Descent with RMSProp > 24. Gradient Descent with momentum > 25. Gradient Descent with RMSProp > 26. Gradient Descent with momentum > 27. Gradient Descent with RMSProp > 28.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: > 3. K-Means > 4. K-Means++ > 5. K-Means++ > 6. K-Means++ > 7. K-Means++ > 8. K-Means++ > 9. K-Means++ > 10. K-Means++ > 11. K-Means++ > 12. K-Means++ > 13. K-Means++ > 14. K-Means++ > 15. K-Means++ > 16. K-Means++ > 17. K-Means++ > 18. K-Means++ > 19. K-Means++ > 20. K-Means++ > 21. K-Means++ > 22. K-Means++ > 23. K-Means++ > 24. K-Means++ > 25. K-Means++ > 26. K-Means++ > 27. K-Means++ > 28. K-Means++ > 29. K-Means++ > 30. K-Means++ > 31.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 2. Binary Search (fixed time) > 1. Binary Search (variable time) > 4. Binary Search (variable time) > 5. Binary Search (variable time) > 6. Binary Search (variable time) > 7. Binary Search (variable time) > 8. Binary Search (variable time) > 9. Binary Search (variable time) > 10. Binary Search (variable time) > 11. Binary Search (variable time) > 12. Binary Search (variable time) > 13. Binary Search (variable time) > 14. Binary Search (variable time) > 15. Binary Search (variable time) > 16. Binary Search (variable time) > 17. Binary Search (variable time) > 18. Binary Search (variable time) > 19. Binary Search (variable time) > 20. Binary Search (variable time) > 21. Binary Search (variable time) > 22. Binary Search (variable time) > 23. Binary Search (variable time) > 24. Binary Search (variable time) > 25.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 5. Gradient Descent (no regularization) > 6. Gradient Descent with Momentum (no regularization) > 7. Gradient Descent with Adagrad (no regularization) > 8. Gradient Descent with RMSProp (no regularization) > 9. Gradient Descent with Adam (no regularization) > 10. Gradient Descent with SGD (no regularization) > 11. Gradient Descent with Adagrad (no regularization) > 12. Gradient Descent with RMSProp (no regularization) > 13. Gradient Descent with Adam (no regularization) > 14. Gradient Descent with SGD with Momentum (no regularization) > 15. Gradient Descent with SGD with Adagrad (no regularization) > 16. Gradient Descent with SGD with RMSProp (no regularization) > 17. Gradient Descent with SGD with Adagrad (no regularization) > 18. Gradient Descent with SGD with Adam (no regularization) > 19. Gradient Descent with SGD with RMSProp (no regularization) > 20.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 4. K-Means > 3. K-Means++ > 2. K-Means++ > 1. K-Means++ > 0. K-Means++ (15 pts) 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\)) > 4. K-Means > 3. K-Means++ > 2. K-Means++ > 1. K-Means++ > 0. K-Means++ (15 pts) 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\)) > 4. K-Means > 3. K-Means++ > 2. K-Means++ > 1. K-Means++ > 0. K-Means++ (15 pts) 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 3-NN (equal weights) > 4-NN (equal weights and non-linear weights) > 5-NN (equal weights and non-linear weights) > 6-NN (equal weights and non-linear weights and non-linear weights) > 7-NN (equal weights and non-linear weights and non-linear weights) > 8-NN (equal weights and non-linear weights and non-linear weights and non-linear weights) > 9-NN (equal weights and non-linear weights and non-linear weights and non-linear weights) > 10-NN (equal weights and non-linear weights and non-linear weights and non-linear weights) > 11-NN (equal weights and non-linear weights and non-linear weights and non-linear weights) > 12-NN (equal weights and non-linear weights and non-linear weights and non-linear weights) > 13-NN (equal weights and non-linear weights and non-linear weights and non-linear weights) > 14-NN (equal weights and non-linear weights and non-linear weights and non-linear weights) > 15-NN (equal

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 3. Decision Trees (leaf size=1) > 4. Random Forest (leaf size=1) > 5. Gradient Boosting (leaf size=1) > 6. Random Forest (leaf size=1) > 7. Gradient Boosting (leaf size=1) > 8. Random Forest (leaf size=1) > 9. Gradient Boosting (leaf size=1) > 10. Random Forest (leaf size=1) > 11. Gradient Boosting (leaf size=1) > 12. Random Forest (leaf size=1) > 13. Gradient Boosting (leaf size=1) > 14. Random Forest (leaf size=1) > 15. Gradient Boosting (leaf size=1) > 16. Random Forest (leaf size=1) > 17. Gradient Boosting (leaf size=1) > 18. Random Forest (leaf size=1) > 19. Gradient Boosting (leaf size=1) > 20. Random Forest (leaf size=1) > 21. Gradient Boosting (leaf size=1) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: > 4. Logistic Regression > 5. Decision Tree > 6. Random Forest > 7. Support Vector Machine > 8. K-Nearest Neighbors (KNN) > 9. K-Means Clustering > 10. Hierarchical Clustering > 11. Principal Component Analysis (PCA) > 12. Factor Analysis > 13. Principal Component Regression (PCR) > 14. Partial Least Squares (PLS) > 15. Non-Parametric Regression > 16. Non-Parametric Estimation > 17. Non-Parametric Estimation (12 pts)

**Section 4: Advanced Topics**

* 4.1. Statistical Inference: Confidence Intervals, Hypothesis Testing, and Confidence Intervals for Means (12 pts)
* 4.2. Regression Analysis: Multiple Regression, Partial Regression, and Regression Diagnostics (12 pts)
* 4.3. Time Series Analysis: Autocorrelation, Stationarity, and Seasonal Decomposition (12 pts)
* 4.4.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (MLN) > 5. Gradient Descent (GD) > 6. Gradient Descent (GD) > 7. Gradient Descent (GD) > 8. Gradient Descent (GD) > 9. Gradient Descent (GD) > 10. Gradient Descent (GD) > 11. Gradient Descent (GD) > 12. Gradient Descent (GD) > 13. Gradient Descent (GD) > 14. Gradient Descent (GD) > 15. Gradient Descent (GD) > 16. Gradient Descent (GD) > 17. Gradient Descent (GD) > 18. Gradient Descent (GD) > 19. Gradient Descent (GD) > 20. Gradient Descent (GD) > 21. Gradient Descent (GD) > 22. Gradient Descent (GD) > 23. Gradient Descent (GD) > 24. Gradient Descent (GD) > 25. Gradient Descent (GD) > 26. Gradient Descent (GD) > 27.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: (e) \(x_1 + 2x_2 = 0\) (f) \(x_1 + 3x_2 = 0\) (g) \(x_1 + 3x_2 = 0\) (h) \(x_1 + 3x_2 = 0\) (i) \(x_1 + 3x_2 = 0\) (j) \(x_1 + 3x_2 = 0\) (k) \(x_1 + 3x_2 = 0\) (l) \(x_1 + 3x_2 = 0\) (m) \(x_1 + 3x_2 = 0\) (n) \(x_1 + 3x_2 = 0\) (o) \(x_1 + 3x_2 = 0\) (p) \(x_1 + 3x_2 = 0\) (q) \(x_1 + 3x_2 = 0\) (r) \(x_1 +

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: > 3. Match kernel to decision boundary: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 4. Match kernel to decision boundary: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 5. Match kernel to decision boundary: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 6. Match kernel to decision boundary: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\)) > 7.

