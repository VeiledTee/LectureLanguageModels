QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: 1. The data points are: 1.000, 0.999, 0.999, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.000, 1.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: 1. The decision boundary is the line that separates the two regions. 2. The decision boundary is the line that separates the two regions. 3. The decision boundary is the line that separates the two regions. 4. The decision boundary is the line that separates the two regions. 5. The decision boundary is the line that separates the two regions. 6. The decision boundary is the line that separates the two regions. 7. The decision boundary is the line that separates the two regions. 8. The decision boundary is the line that separates the two regions. 9. The decision boundary is the line that separates the two regions. 10. The decision boundary is the line that separates the two regions. 11. The decision boundary is the line that separates the two regions. 12. The decision boundary is the line that separates the two regions. 13. The decision boundary is the line that separates the two regions. 14. The decision boundary is the line that separates the two regions. 15. The decision boundary is the line that separates the two regions. 16. The decision boundary is the line that separates the two regions. 17.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: 

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: Answer:

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: > 2. Draw 1-NN decision boundaries. > 3. Draw 1-NN decision boundaries. > 4. Draw 1-NN decision boundaries. > 5. Draw 1-NN decision boundaries. > 6.034 Quiz 2, Spring 2005 > 3 Nearest Neighbors (8 pts) > 4. Draw 1-NN decision boundaries. > 5. Draw 1-NN decision boundaries. > 6.034 Quiz 2, Spring 2005 > 4 Nearest Neighbors (8 pts) > 5. Draw 1-NN decision boundaries. > 6.034 Quiz 2, Spring 2005 > 3 Nearest Neighbors (8 pts) > 4. Draw 1-NN decision boundaries. > 5. Draw 1-NN decision boundaries. > 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 5. Draw 1-NN decision boundaries. > 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: Answer:

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: A:

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: 

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: Question: What is the class of the data points (2.0, -1.01) in the data set?

Answer: 2.0

Question: What is the class of the data points (2.0, -1.01) in the data set?

Answer: 2.0

Question: What is the class of the data points (2.0, -1.01) in the data set?

Answer: 2.0

Question: What is the class of the data points (2.0, -1.01) in the data set?

Answer: 2.0

Question: What is the class of the data points (2.0, -1.01) in the data set?

Answer: 2.0

Question: What is the class of the data points (2.0, -1.01) in the data set?

Answer: 2.0

Question: What is the class of the data points (2.0, -1.01) in the data set?

Answer: 2.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: 

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: 2. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 3. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 4. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 5. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 6. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\). 7.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: 7. Calculate \(\Delta w_2\) via backpropagation for output (2,2). 8. Calculate \(\Delta w_2\) via backpropagation for output (2,2). 9. Calculate \(\Delta w_2\) via backpropagation for input (2,2). 10. Calculate \(\Delta w_2\) via backpropagation for output (2,2). 11. Calculate \(\Delta w_2\) via backpropagation for output (2,2). 12. Calculate \(\Delta w_2\) via backpropagation for input (2,2). 13. Calculate \(\Delta w_2\) via backpropagation for output (2,2). 14. Calculate \(\Delta w_2\) via backpropagation for input (2,2). 15. Calculate \(\Delta w_2\) via backpropagation for output (2,2). 16. Calculate \(\Delta w_2\) via backpropagation for output (2,2). 17.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: What is the expected number of correct answers for the following test case?

Test case: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: Answer: 6.034

Explanation: The most influential feature is the feature with the highest probability of being selected. The feature with the highest probability of being selected is the feature with the highest probability of being positive. Therefore, the most influential feature is the feature with the highest probability of being selected.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: > 2. 1M training examples, 6D features, 100 test queries. > 3. 1M training examples, 6D features, 100 test queries. > 4. 1M training examples, 6D features, 100 test queries. > 5. 1M training examples, 6D features, 100 test queries. > 6. 1M training examples, 6D features, 100 test queries. > 7. 1M training examples, 6D features, 100 test queries. > 8. 1M training examples, 6D features, 100 test queries. > 9. 1M training examples, 6D features, 100 test queries. > 10. 1M training examples, 6D features, 100 test queries. > 11. 1M training examples, 6D features, 100 test queries. > 12. 1M training examples, 6D features, 100 test queries. > 13.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: > 3. Classifier for kindergarten special education requiring justification. > 4. Classifier for kindergarten special education requiring justification. > 5. Classifier for kindergarten special education requiring justification. > 6. Classifier for kindergarten special education requiring justification. > 7. Classifier for kindergarten special education requiring justification. > 8. Classifier for kindergarten special education requiring justification. > 9. Classifier for kindergarten special education requiring justification. > 10. Classifier for kindergarten special education requiring justification. > 11. Classifier for kindergarten special education requiring justification. > 12. Classifier for kindergarten special education requiring justification. > 13. Classifier for kindergarten special education requiring justification. > 14. Classifier for kindergarten special education requiring justification. > 15. Classifier for kindergarten special education requiring justification. > 16. Classifier for kindergarten special education requiring justification. > 17. Classifier for kindergarten special education requiring justification. > 18. Classifier for kindergarten special education requiring justification. > 19. Classifier for kindergarten special education requiring justification. > 20. Classifier for kindergarten special education requiring justification. > 21.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: > 4. A user is given a list of 1000000 entries and wants to predict the next entry. > 5. A user is given a list of 1000000 entries and wants to predict the next entry. > 6. A user is given a list of 1000000 entries and wants to predict the next entry. > 7. A user is given a list of 1000000 entries and wants to predict the next entry. > 8. A user is given a list of 1000000 entries and wants to predict the next entry. > 9. A user is given a list of 1000000 entries and wants to predict the next entry. > 10. A user is given a list of 1000000 entries and wants to predict the next entry. > 11. A user is given a list of 1000000 entries and wants to predict the next entry. > 12. A user is given a list of 1000000 entries and wants to predict the next entry.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: > 5. The best way to get a good night's sleep. > 6. The best way to get a good night's sleep. > 7. The best way to get a good night's sleep. > 8. The best way to get a good night's sleep. > 9. The best way to get a good night's sleep. > 10. The best way to get a good night's sleep. > 11. The best way to get a good night's sleep. > 12. The best way to get a good night's sleep. > 13. The best way to get a good night's sleep. > 14. The best way to get a good night's sleep. > 15. The best way to get a good night's sleep. > 16. The best way to get a good night's sleep. > 17. The best way to get a good night's sleep. > 18. The best way to get a good night's sleep. > 19. The best way to get a good night's sleep. > 20. The best way to get a good night's sleep. > 21.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: : 1.000 2.000 3.000 4.000 5.000 6.000 7.000 8.000 9.000 10.000 11.000 12.000 13.000 14.000 15.000 16.000 17.000 18.000 19.000 20.000 21.000 22.000 23.000 24.000 25.000 26.000 27.000 28.000 29.000 30.000 31.000 32.000 33.000 34.000 35.000 36.000 37.000 38.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: (15 pts) > 3. Decision Trees (15 pts) > 4. Random Forest (15 pts) > 5. Gradient Boosting (15 pts) > 6. Support Vector Machines (15 pts) > 7. Neural Networks (15 pts) > 8. KNN (15 pts) > 9. Random Graphs (15 pts) > 10. KNN (15 pts) > 11. KNN (15 pts) > 12. KNN (15 pts) > 13. KNN (15 pts) > 14. KNN (15 pts) > 15. KNN (15 pts) > 16. Random Graphs (15 pts) > 17. Random Graphs (15 pts) > 18. Random Graphs (15 pts) > 19. Random Graphs (15 pts) > 20. Random Graphs (15 pts) > 21. Random Graphs (15 pts) > 22. Random Graphs (15 pts) > 23.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 4. Decision Tree (variable depth) > 5. Decision Tree (variable depth) > 6. Decision Tree (variable depth) > 7. Decision Tree (variable depth) > 8. Decision Tree (variable depth) > 9. Decision Tree (variable depth) > 10. Decision Tree (variable depth) > 11. Decision Tree (variable depth) > 12. Decision Tree (variable depth) > 13. Decision Tree (variable depth) > 14. Decision Tree (variable depth) > 15. Decision Tree (variable depth) > 16. Decision Tree (variable depth) > 17. Decision Tree (variable depth) > 18. Decision Tree (variable depth) > 19. Decision Tree (variable depth) > 20. Decision Tree (variable depth) > 21. Decision Tree (variable depth) > 22. Decision Tree (variable depth) > 23. Decision Tree (variable depth) > 24. Decision Tree (variable depth) > 25. Decision Tree (variable depth) > 26. Decision Tree (variable depth) > 27.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 5. Linear Regression > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Linear Regression > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Linear Regression > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Linear Regression > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5. Linear Regression > 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization) > 5.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 6.034 > 7 Error vs Complexity (15 pts) > 5. SVM (\(c < \infty\)) > 6.034 > 7 Error vs Complexity (15 pts) > 5. SVM (\(c < \infty\)) > 6.034 > 7 Error vs Complexity (15 pts) > 5. SVM (\(c < \infty\)) > 6.034 > 7 Error vs Complexity (15 pts) > 5. SVM (\(c < \infty\)) > 6.034 > 7 Error vs Complexity (15 pts) > 5. SVM (\(c < \infty\)) > 6.034 > 7 Error vs Complexity (15 pts) > 5. SVM (\(c < \infty\)) > 6.034 > 7 Error vs Complexity (15 pts) > 5. SVM (\(c < \infty\)) > 6.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 3-NN (weighted) > 4-NN (weighted) > 5-NN (weighted) > 6-NN (weighted) > 7-NN (weighted) > 8-NN (weighted) > 9-NN (weighted) > 10-NN (weighted) > 11-NN (weighted) > 12-NN (weighted) > 13-NN (weighted) > 14-NN (weighted) > 15-NN (weighted) > 16-NN (weighted) > 17-NN (weighted) > 18-NN (weighted) > 19-NN (weighted) > 20-NN (weighted) > 21-NN (weighted) > 22-NN (weighted) > 23-NN (weighted) > 24-NN (weighted) > 25-NN (weighted) > 26-NN (weighted) > 27-NN (weighted) > 28-NN (weighted) > 29-NN (weighted) > 30-NN (weighted) > 31-NN (weighted) >

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 3. Regression Trees (leaf size=2) > 4. Regression Trees (leaf size=3) > 5. Regression Trees (leaf size=4) > 6. Regression Trees (leaf size=5) > 7. Regression Trees (leaf size=6) > 8. Regression Trees (leaf size=7) > 9. Regression Trees (leaf size=8) > 10. Regression Trees (leaf size=9) > 11. Regression Trees (leaf size=10) > 12. Regression Trees (leaf size=11) > 13. Regression Trees (leaf size=12) > 14. Regression Trees (leaf size=13) > 15. Regression Trees (leaf size=14) > 16. Regression Trees (leaf size=15) > 17. Regression Trees (leaf size=16) > 18. Regression Trees (leaf size=17) > 19. Regression Trees (leaf size=18) > 20. Regression Trees (leaf size=19) > 21. Regression Trees (leaf size=20) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: (12 pts) > 4. Linear Regression (12 pts) > 5. Logistic Regression (12 pts) > 6.034 Regression (12 pts) > 7.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (10 pts) > 5.000 Regression (10 pts) > 6.000 Regression (10 pts) > 7.000 Regression (10 pts) > 8.000 Regression (10 pts) > 9.000 Regression (10 pts) > 10.000 Regression (10 pts) > 11.000 Regression (10 pts) > 12.000 Regression (10 pts) > 13.000 Regression (10 pts) > 14.000 Regression (10 pts) > 15.000 Regression (10 pts) > 16.000 Regression (10 pts) > 17.000 Regression (10 pts) > 18.000 Regression (10 pts) > 19.000 Regression (10 pts) > 20.000 Regression (10 pts) > 21.000 Regression (10 pts) > 22.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: (e) \(x_1 + 2x_2 = 0\) (f) \(x_1 + 3x_2 = 0\) (g) \(x_1 + 4x_2 = 0\) (h) \(x_1 + 5x_2 = 0\) (i) \(x_1 + 6x_2 = 0\) (j) \(x_1 + 7x_2 = 0\) (k) \(x_1 + 8x_2 = 0\) (l) \(x_1 + 9x_2 = 0\) (m) \(x_1 + 10x_2 = 0\) (n) \(x_1 + 11x_2 = 0\) (o) \(x_1 + 12x_2 = 0\) (p) \(x_1 + 13x_2 = 0\) (q) \(x_1 + 14x_2 = 0\) (r) \

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: (e) RBF (\(\sigma=2.0\)) (f) RBF (\(\sigma=3.0\)) (g) RBF (\(\sigma=4.0\)) (h) RBF (\(\sigma=5.0\)) (i) RBF (\(\sigma=6.0\)) (j) RBF (\(\sigma=7.0\)) (k) RBF (\(\sigma=8.0\)) (l) RBF (\(\sigma=9.0\)) (m) RBF (\(\sigma=10.0\)) (n) RBF (\(\sigma=11.0\)) (o) RBF (\(\sigma=12.0\)) (p) RBF (\(\sigma=13.0\)) (q) RBF (\(\sigma=14.0\)) (r) RBF (\(\sigma=15.0\)) (s) RBF (\(\sigma=16.0\)) (t) RBF (\(\sigma=17.

