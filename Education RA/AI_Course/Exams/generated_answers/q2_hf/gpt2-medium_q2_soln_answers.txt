QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 1. Construct a decision tree for the data.
//// ANSWER: > 2. Find the best decision tree for the data. > 3. Find the best decision tree for the data. > 4. Find the best decision tree for the data. > 5. Find the best decision tree for the data. > 6. Find the best decision tree for the data. > 7. Find the best decision tree for the data. > 8. Find the best decision tree for the data. > 9. Find the best decision tree for the data. > 10. Find the best decision tree for the data. > 11. Find the best decision tree for the data. > 12. Find the best decision tree for the data. > 13. Find the best decision tree for the data. > 14. Find the best decision tree for the data. > 15. Find the best decision tree for the data. > 16. Find the best decision tree for the data. > 17. Find the best decision tree for the data. > 18. Find the best decision tree for the data. > 19. Find the best decision tree for the data. > 20. Find the best decision tree for the data. > 21. Find the best decision tree for the data. > 22. Find the best decision tree for the data. > 23.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 2. Draw decision boundaries on the graph.
//// ANSWER: > 3. Draw decision boundaries on the graph. > 4. Draw decision boundaries on the graph. > 5. Draw decision boundaries on the graph. > 6. Draw decision boundaries on the graph. > 7. Draw decision boundaries on the graph. > 8. Draw decision boundaries on the graph. > 9. Draw decision boundaries on the graph. > 10. Draw decision boundaries on the graph. > 11. Draw decision boundaries on the graph. > 12. Draw decision boundaries on the graph. > 13. Draw decision boundaries on the graph. > 14. Draw decision boundaries on the graph. > 15. Draw decision boundaries on the graph. > 16. Draw decision boundaries on the graph. > 17. Draw decision boundaries on the graph. > 18. Draw decision boundaries on the graph. > 19. Draw decision boundaries on the graph. > 20. Draw decision boundaries on the graph. > 21. Draw decision boundaries on the graph. > 22. Draw decision boundaries on the graph. > 23. Draw decision boundaries on the graph. > 24. Draw decision boundaries on the graph. > 25. Draw decision boundaries on the graph. > 26. Draw decision boundaries on the graph. > 27. Draw decision boundaries on the graph. > 28.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 3. Explain how you chose the top-level test in the tree.
//// ANSWER: > 4. Explain how you chose the bottom-level test in the tree. > 5. Explain how you chose the intermediate test in the tree. > 6. Explain how you chose the final test in the tree. > 7. Explain how you chose the intermediate test in the tree. > 8. Explain how you chose the final test in the tree. > 9. Explain how you chose the intermediate test in the tree. > 10. Explain how you chose the final test in the tree. > 11. Explain how you chose the intermediate test in the tree. > 12. Explain how you chose the final test in the tree. > 13. Explain how you chose the intermediate test in the tree. > 14. Explain how you chose the final test in the tree. > 15. Explain how you chose the intermediate test in the tree. > 16. Explain how you chose the final test in the tree. > 17. Explain how you chose the intermediate test in the tree. > 18. Explain how you chose the final test in the tree. > 19. Explain how you chose the intermediate test in the tree. > 20. Explain how you chose the final test in the tree. > 21. Explain how you chose the intermediate test in the tree.

QUESTION: 6.034 Quiz 2, Spring 2005 > 1 Decision Trees (13 pts) Data points: Negative: (-1, 0), (2, 1), (2, -2) Positive: (0, 0), (1, 0) > 4. What class does the tree predict for the point (1, -1.01)?
//// ANSWER: > 5. What class does the tree predict for the point (1, -1.01)? > 6. What class does the tree predict for the point (1, -1.01)? > 7. What class does the tree predict for the point (1, -1.01)? > 8. What class does the tree predict for the point (1, -1.01)? > 9. What class does the tree predict for the point (1, -1.01)? > 10. What class does the tree predict for the point (1, -1.01)? > 11. What class does the tree predict for the point (1, -1.01)? > 12. What class does the tree predict for the point (1, -1.01)? > 13. What class does the tree predict for the point (1, -1.01)? > 14. What class does the tree predict for the point (1, -1.01)? > 15. What class does the tree predict for the point (1, -1.01)? > 16. What class does the tree predict for the point (1, -1.01)? > 17.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 1. Draw 1-NN decision boundaries.
//// ANSWER: > 2. Draw 1-NN decision boundaries. > 3. Draw 1-NN decision boundaries. > 4. Draw 1-NN decision boundaries. > 5. Draw 1-NN decision boundaries. > 6. Draw 1-NN decision boundaries. > 7. Draw 1-NN decision boundaries. > 8. Draw 1-NN decision boundaries. > 9. Draw 1-NN decision boundaries. > 10. Draw 1-NN decision boundaries. > 11. Draw 1-NN decision boundaries. > 12. Draw 1-NN decision boundaries. > 13. Draw 1-NN decision boundaries. > 14. Draw 1-NN decision boundaries. > 15. Draw 1-NN decision boundaries. > 16. Draw 1-NN decision boundaries. > 17. Draw 1-NN decision boundaries. > 18. Draw 1-NN decision boundaries. > 19. Draw 1-NN decision boundaries. > 20. Draw 1-NN decision boundaries. > 21. Draw 1-NN decision boundaries. > 22. Draw 1-NN decision boundaries. > 23. Draw 1-NN decision boundaries. > 24. Draw 1-NN decision boundaries. > 25. Draw 1-NN decision boundaries. > 26. Draw 1-NN decision boundaries. > 27.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 2. What class does 1-NN predict for (1, -1.01)? Explain.
//// ANSWER: > 2.

QUESTION: 6.034 Quiz 2, Spring 2005 > 2 Nearest Neighbors (8 pts) > 3. What class does 3-NN predict for (1, -1.01)? Explain.
//// ANSWER: > 3. What class does 3-NN predict for (1, -0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3. What class does 3-NN predict for (1, 0.01)? Explain. > 3.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Explain how the perceptron algorithm produces the separator (0, 1, 2).
//// ANSWER: > 2. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 3. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 4. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 5. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 6. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 7. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 8. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 9. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 10. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 11. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 12. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 13. Explain how the perceptron algorithm produces the separator (0, 1, 2). > 14.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Predict the class for (2.0, -1.01).
//// ANSWER: > 3. Predict the class for (2.0, -0.01). > 4. Predict the class for (2.0, 0.01). > 5. Predict the class for (2.0, 0.01). > 6. Predict the class for (2.0, 0.01). > 7. Predict the class for (2.0, 0.01). > 8. Predict the class for (2.0, 0.01). > 9. Predict the class for (2.0, 0.01). > 10. Predict the class for (2.0, 0.01). > 11. Predict the class for (2.0, 0.01). > 12. Predict the class for (2.0, 0.01). > 13. Predict the class for (2.0, 0.01). > 14. Predict the class for (2.0, 0.01). > 15. Predict the class for (2.0, 0.01). > 16. Predict the class for (2.0, 0.01). > 17. Predict the class for (2.0, 0.01). > 18. Predict the class for (2.0, 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 3 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3. Describe the result if applied to the 5-point dataset from Problem 1.
//// ANSWER: > 3.5 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3.6 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3.7 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3.8 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 3.9 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 4 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 4.1 Perceptron (7 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 4.

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 1. Compute sigmoid outputs for points (-1,0), (2,-2), (1,0) with weights \(w_0=0, w_1=1, w_2=1\).
//// ANSWER: > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >

QUESTION: 6.034 Quiz 2, Spring 2005 > 4 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).
//// ANSWER: > > 5 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). > > 6 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). > > 7 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). > > 8 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). > > 9 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). > > 10 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). > > 11 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2). > > 12 Neural Net (9 pts) > 2. Calculate \(\Delta w_2\) via backpropagation for input (2,-2).

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.
//// ANSWER: > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6        | 6      | | \(x_2=1\) | 0       | 0      | | \(x_3=1\) | 2       | 4      | > 1. Compute \(\Pr(x_1=1|y=0)\), \(\Pr(x_2=1|y=1)\), \(\Pr(x_3=0|y=0)\) with Laplacian correction.

QUESTION: 6.034 Quiz 2, Spring 2005 > 5 Naive Bayes (8 pts) Training data (12 examples: 6 positive, 6 negative): |  Feature  | \(y=0\) | \(y=1\) | |-----------|---------|---------| | \(x_1=1\) | 6       | 6       | | \(x_2=1\) | 0       | 0       | | \(x_3=1\) | 2       | 4       | > 2. Identify the most influential feature.
//// ANSWER: > 5.2.2.1.2.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19.20.21.22.23.24.25.26.27.28.29.30.31.32.33.34.35.36.37.38.39.40.41.42.43.44.45.46.47.48.49.50.51.52.53.54.55.56.57.58.59.60.61.62.63.64.65.66.67.68.69.70.71.72.73.74.75.76.77.78.79.80.81.82.83.84.85.86.87.88.89.90.91.92.93.94.95.96.97.98.99.100.101.102.103.104.105.106.107.108.109.110.111.112.113.114.115.116.117.118.119.120.121.122.123.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 1. 1M training examples, 6D features, 100 test queries.
//// ANSWER: > 2. 1M training examples, 6D features, 100 test queries. > 3. 1M training examples, 6D features, 100 test queries. > 4. 1M training examples, 6D features, 100 test queries. > 5. 1M training examples, 6D features, 100 test queries. > 6. 1M training examples, 6D features, 100 test queries. > 7. 1M training examples, 6D features, 100 test queries. > 8. 1M training examples, 6D features, 100 test queries. > 9. 1M training examples, 6D features, 100 test queries. > 10. 1M training examples, 6D features, 100 test queries. > 11. 1M training examples, 6D features, 100 test queries. > 12. 1M training examples, 6D features, 100 test queries. > 13. 1M training examples, 6D features, 100 test queries. > 14. 1M training examples, 6D features, 100 test queries. > 15. 1M training examples, 6D features, 100 test queries. > 16. 1M training examples, 6D features, 100 test queries. > 17. 1M training examples, 6D features, 100 test queries.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 2. Classifier for kindergarten special education requiring justification.
//// ANSWER: > 3. Classifier for kindergarten special education requiring justification. > 4. Classifier for kindergarten special education requiring justification. > 5. Classifier for kindergarten special education requiring justification. > 6. Classifier for kindergarten special education requiring justification. > 7. Classifier for kindergarten special education requiring justification. > 8. Classifier for kindergarten special education requiring justification. > 9. Classifier for kindergarten special education requiring justification. > 10. Classifier for kindergarten special education requiring justification. > 11. Classifier for kindergarten special education requiring justification. > 12. Classifier for kindergarten special education requiring justification. > 13. Classifier for kindergarten special education requiring justification. > 14. Classifier for kindergarten special education requiring justification. > 15. Classifier for kindergarten special education requiring justification. > 16. Classifier for kindergarten special education requiring justification. > 17. Classifier for kindergarten special education requiring justification. > 18. Classifier for kindergarten special education requiring justification. > 19. Classifier for kindergarten special education requiring justification. > 20. Classifier for kindergarten special education requiring justification. > 21. Classifier for kindergarten special education requiring justification. > 22. Classifier for kindergarten special education requiring justification. > 23. Classifier for kindergarten special education requiring justification. > 24.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 3. Book preference prediction with 1M features and frequent updates.
//// ANSWER: > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3. Book preference prediction with 1M features and frequent updates. > 3.

QUESTION: 6.034 Quiz 2, Spring 2005 > 6 Learning Algorithms (16 pts) For each scenario, choose the best algorithm and justify: > 4. Rainfall prediction from ocean currents/tides.
//// ANSWER: > 3. Rainfall prediction from wind/snow/rain > 2. Rainfall prediction from wind/snow/rain > 1. Rainfall prediction from wind/snow/rain > 6. Rainfall prediction from ocean currents/tides. > 5. Rainfall prediction from wind/snow/rain > 4. Rainfall prediction from wind/snow/rain > 3. Rainfall prediction from wind/snow/rain > 2. Rainfall prediction from wind/snow/rain > 1. Rainfall prediction from wind/snow/rain > 6. Rainfall prediction from ocean currents/tides. > 5. Rainfall prediction from wind/snow/rain > 4. Rainfall prediction from wind/snow/rain > 3. Rainfall prediction from wind/snow/rain > 2. Rainfall prediction from wind/snow/rain > 1. Rainfall prediction from wind/snow/rain > 6. Rainfall prediction from ocean currents/tides. > 5. Rainfall prediction from wind/snow/rain > 4. Rainfall prediction from wind/snow/rain > 3. Rainfall prediction from wind/snow/rain > 2.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 1. Perceptron
//// ANSWER: : > 2. Iterator: > 3. Random walk: > 4. Random walk with step: > 5. Random walk with step: > 6. Random walk with step: > 7. Random walk with step: > 8. Random walk with step: > 9. Random walk with step: > 10. Random walk with step: > 11. Random walk with step: > 12. Random walk with step: > 13. Random walk with step: > 14. Random walk with step: > 15. Random walk with step: > 16. Random walk with step: > 17. Random walk with step: > 18. Random walk with step: > 19. Random walk with step: > 20. Random walk with step: > 21. Random walk with step: > 22. Random walk with step: > 23. Random walk with step: > 24. Random walk with step: > 25. Random walk with step: > 26. Random walk with step: > 27. Random walk with step: > 28. Random walk with step: > 29. Random walk with step: > 30. Random walk with step: > 31. Random walk with step: > 32. Random walk with step: > 33. Random walk with step: > 34.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 2. Linear SVM
//// ANSWER: > 3.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 3. Decision Tree (fixed depth)
//// ANSWER: > 4. Decision Tree (random depth) > 5. Decision Tree (random depth) > 6. Decision Tree (random depth) > 7. Decision Tree (random depth) > 8. Decision Tree (random depth) > 9. Decision Tree (random depth) > 10. Decision Tree (random depth) > 11. Decision Tree (random depth) > 12. Decision Tree (random depth) > 13. Decision Tree (random depth) > 14. Decision Tree (random depth) > 15. Decision Tree (random depth) > 16. Decision Tree (random depth) > 17. Decision Tree (random depth) > 18. Decision Tree (random depth) > 19. Decision Tree (random depth) > 20. Decision Tree (random depth) > 21. Decision Tree (random depth) > 22. Decision Tree (random depth) > 23. Decision Tree (random depth) > 24. Decision Tree (random depth) > 25. Decision Tree (random depth) > 26. Decision Tree (random depth) > 27. Decision Tree (random depth) > 28. Decision Tree (random depth) > 29. Decision Tree (random depth) > 30. Decision Tree (random depth) > 31. Decision Tree (random depth) > 32.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 4. Neural Network (no regularization)
//// ANSWER: > 5. Random Forest (no regularization) > 6. Random Forest (regularization) > 7. Random Forest (regularization) > 8. Random Forest (regularization) > 9. Random Forest (regularization) > 10. Random Forest (regularization) > 11. Random Forest (regularization) > 12. Random Forest (regularization) > 13. Random Forest (regularization) > 14. Random Forest (regularization) > 15. Random Forest (regularization) > 16. Random Forest (regularization) > 17. Random Forest (regularization) > 18. Random Forest (regularization) > 19. Random Forest (regularization) > 20. Random Forest (regularization) > 21. Random Forest (regularization) > 22. Random Forest (regularization) > 23. Random Forest (regularization) > 24. Random Forest (regularization) > 25. Random Forest (regularization) > 26. Random Forest (regularization) > 27. Random Forest (regularization) > 28. Random Forest (regularization) > 29. Random Forest (regularization) > 30. Random Forest (regularization) > 31. Random Forest (regularization) > 32. Random Forest (regularization) > 33.

QUESTION: 6.034 Quiz 2, Spring 2005 > 7 Error vs Complexity (15 pts) For each algorithm, specify: > 5. SVM (\(c < \infty\))
//// ANSWER: > 4. SVM (\(c > \infty\)) > 3. SVM (\(c < \infty\)) > 2. SVM (\(c > \infty\)) > 1. SVM (\(c < \infty\)) > 0. SVM (\(c > \infty\)) > 1. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0. SVM (\(c < \infty\)) > 0.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 1. 2-NN (equally weighted)
//// ANSWER: > 2. 3-NN (equally weighted) > 3. 4-NN (equally weighted) > 4. 5-NN (equally weighted) > 5. 6-NN (equally weighted) > 6. 7-NN (equally weighted) > 7. 8-NN (equally weighted) > 8. 9-NN (equally weighted) > 9. 10-NN (equally weighted) > 10. 11-NN (equally weighted) > 11. 12-NN (equally weighted) > 12. 13-NN (equally weighted) > 13. 14-NN (equally weighted) > 14. 15-NN (equally weighted) > 15. 16-NN (equally weighted) > 16. 17-NN (equally weighted) > 17. 18-NN (equally weighted) > 18. 19-NN (equally weighted) > 19. 20-NN (equally weighted) > 20. 21-NN (equally weighted) > 21. 22-NN (equally weighted) > 22. 23-NN (equally weighted) > 23. 24-NN (equally weighted) > 24. 25-NN (equally weighted) > 25.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 2. Regression Trees (leaf size=1)
//// ANSWER: > 3. Regression Trees (leaf size=2) > 4. Regression Trees (leaf size=3) > 5. Regression Trees (leaf size=4) > 6. Regression Trees (leaf size=5) > 7. Regression Trees (leaf size=6) > 8. Regression Trees (leaf size=7) > 9. Regression Trees (leaf size=8) > 10. Regression Trees (leaf size=9) > 11. Regression Trees (leaf size=10) > 12. Regression Trees (leaf size=11) > 13. Regression Trees (leaf size=12) > 14. Regression Trees (leaf size=14) > 15. Regression Trees (leaf size=15) > 16. Regression Trees (leaf size=16) > 17. Regression Trees (leaf size=17) > 18. Regression Trees (leaf size=18) > 19. Regression Trees (leaf size=19) > 20. Regression Trees (leaf size=20) > 21. Regression Trees (leaf size=21) > 22. Regression Trees (leaf size=22) > 23. Regression Trees (leaf size=23) > 24.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 3. Linear Neural Network
//// ANSWER: (3 pts) > 4. Regression (2 pts) > 5. Regression (1 pts) > 6. Regression (1 pts) > 7. Regression (1 pts) > 8. Regression (1 pts) > 9. Regression (1 pts) > 10. Regression (1 pts) > 11. Regression (1 pts) > 12. Regression (1 pts) > 13. Regression (1 pts) > 14. Regression (1 pts) > 15. Regression (1 pts) > 16. Regression (1 pts) > 17. Regression (1 pts) > 18. Regression (1 pts) > 19. Regression (1 pts) > 20. Regression (1 pts) > 21. Regression (1 pts) > 22. Regression (1 pts) > 23. Regression (1 pts) > 24. Regression (1 pts) > 25. Regression (1 pts) > 26. Regression (1 pts) > 27. Regression (1 pts) > 28. Regression (1 pts) > 29. Regression (1 pts) > 30. Regression (1 pts) > 31.

QUESTION: 6.034 Quiz 2, Spring 2005 > 8 Regression (12 pts) Draw regression outputs for: > 4. Multi-layer Neural Network
//// ANSWER: (12 pts) > 4.1. Linear Regression (12 pts) > 4.2. Regression with Multiple Layer Neural Networks (12 pts) > 4.3. Regression with Multiple Layer Neural Networks with Multiple Regression Outputs (12 pts) > 4.4. Regression with Multiple Layer Neural Networks with Multiple Regression Outputs with Multiple Regression Outputs (12 pts) > 4.5.

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 1. Evaluate if separators satisfy SVM conditions: (a) \(x_1 + x_2 = 0\) (b) \(x_1 + 1.5x_2 = 0\) (c) \(x_1 + 2x_2 = 0\) (d) \(2x_1 + 3x_2 = 0\)
//// ANSWER: (e) \(x_1 + 4x_2 = 0\) (f) \(x_1 + 5x_2 = 0\) (g) \(x_1 + 6x_2 = 0\) (h) \(x_1 + 7x_2 = 0\) (i) \(x_1 + 8x_2 = 0\) (j) \(x_1 + 9x_2 = 0\) (k) \(x_1 + 10x_2 = 0\) (l) \(x_1 + 11x_2 = 0\) (m) \(x_1 + 12x_2 = 0\) (n) \(x_1 + 13x_2 = 0\) (o) \(x_1 + 14x_2 = 0\) (p) \(x_1 + 15x_2 = 0\) (q) \(x_1 + 16x_2 = 0\) (r) \(x_1 + 17x_2 = 0\) (s) \(x_1 + 18x_2 = 0\) (t) \(x_1 + 19x_2 = 0\) (u) \(x_1 + 20x_2 = 0\) (

QUESTION: 6.034 Quiz 2, Spring 2005 > 9 SVM (12 pts) Data points: Negative: (-1, 0), (2, -2) Positive: (1, 0) > 2. Match kernels to decision boundaries: (a) Polynomial (degree=2) (b) Polynomial (degree=3) (c) RBF (\(\sigma=0.5\)) (d) RBF (\(\sigma=1.0\))
//// ANSWER: (e) RBF (\(\sigma=2.0\)) (f) RBF (\(\sigma=3.0\)) (g) RBF (\(\sigma=4.0\)) (h) RBF (\(\sigma=5.0\)) (i) RBF (\(\sigma=6.0\)) (j) RBF (\(\sigma=7.0\)) (k) RBF (\(\sigma=8.0\)) (l) RBF (\(\sigma=9.0\)) (m) RBF (\(\sigma=10.0\)) (n) RBF (\(\sigma=11.0\)) (o) RBF (\(\sigma=12.0\)) (p) RBF (\(\sigma=13.0\)) (q) RBF (\(\sigma=14.0\)) (r) RBF (\(\sigma=15.0\)) (s) RBF (\(\sigma=16.0\)) (t) RBF (\(\sigma=17.

