QUESTION: Describe a O(n)-time algorithm to determine the maximum amount of lottery winnings Tiff can win in the next n days by playing the lottery infrequently. //// ANSWER: 1. Subproblems • Let L(i) be the cash payout of playing the lottery on day i ∈ {1,...,n} • Need to keep track of most recent two plays (or equivalently, restrictions on future plays) • x(i, j): maximum lottery winnings playing on suffix of days from i to n, assuming play on day i and next allowable play is on day i + j • for i ∈ {1,...,n} and j ∈ {1,..., 6} 2. Relate • Tiffany will play again on some day in future. Guess! • It is never optimal to go 11 days without playing the lottery, as playing on the 6th day would be valid and strictly increase winnings • The next play can be on day i + k for k ∈ {j, ... , 11} • If next play on i + k for k ∈ {1,..., 6}, next allowable play is on day i + 7 • If next play on i + k for k ∈ {7,..., 11}, next allowable play is on day i + k + 1 • x(i, j) = L(i) + max{x(i + k, max{1, 7 − k}) | k ∈ {i, ... , 11} and i + k ≤ n} 3. Topo • Subproblems x(i, j) only depend on strictly larger i, so acyclic 4. Base • x(n, j) = L(i) for all j ∈ {1,..., 6} 5. Original • Solve subproblems via recursive top-down or iterative bottom-up • Guess first play (within first seven days) • Solution to original problem is max{x(i, 1) | i ∈ {1,..., 7}} • (Can store parent pointers to reconstruct days played) 6. Time • # subproblems: 6n • work per subproblem: O(1) • work for original: O(1) • O(n) running time
//// ANSWER: The provided text describes an algorithm to determine the maximum amount of lottery winnings Tiff can win in the next n days by playing the lottery infrequently. Here is a summary of the algorithm:

1. Subproblems: Define x(i, j) as the maximum lottery winnings playing on the suffix of days from i to n, assuming play on day i and the next allowable play is on day i + j (where i ∈ {1,...,n} and j ∈ {1,..., 6}).
2. Relate: Tiffany will play again in the future. Guess the next play's day by considering the following restrictions:
   - Never go 11 days without playing the lottery.
   - If the next play is on day i + k (where k ∈ {1,..., 6}), the next allowable play is on day i + 7.
   - If the next play is on day i + k (where k ∈ {7,..., 11}), the next allowable play is on day i + k + 1.
   - x(i, j) = L(i) + max{x(i + k, max{1, 7 − k}) | k ∈ {i, ... , 11} and i + k ≤ n}
3. Topological order: The subproblems x(i, j) only depend on strictly larger i, so the problem is acyclic.
4. Base: x(n, j) = L(i) for all j ∈ {1,..., 6}.
5. Original: Solve the subproblems using a recursive top-down or iterative bottom-up approach to find the solution to the original problem, which is max{x(i, 1) | i ∈ {1,..., 7}}.
6. Time: The number of subproblems is 6n, and the work per subproblem is O(1). The work for the original problem is also O(1). Therefore, the overall time complexity of the algorithm is O(n).

QUESTION: Describe an O(n^4)-time algorithm to determine whether Charlie is a fraud. //// ANSWER: 1. Subproblems • Let A, B, and C be the relevant length-n DNA sequences from Alice, Bob, and Charlie. • Want to match some characters of A and B to all characters of C. • x(i, j, ki, kj): true if a length-ki subsequence of suffix A[i:] and a length-kj subsequence of prefix B[j:] can match all characters in suffix C[(n − ki − kj):] (the suffix containing the last ki + kj characters), and false otherwise. • Defined for i, j ∈ {0,...,n} and ki, kj ∈ {0,...,n/2} (assume n is even). 2. Relate • Must match character C[i]; if A[i] = C[i] or B[i] = C[i], recurse on remainder. • Alternatively, do not use either A[i] or B[i]. • The recurrence relation: x(i, j, ki, kj) = OR { x(i + 1, j, ki + 1, kj) if A[i] = C[n − ki − kj] and ki > 0, x(i, j + 1, ki, kj + 1) if B[j] = C[n − ki − kj] and kj > 0, x(i + 1, j, ki, kj) if i < n, x(i, j + 1, ki, kj) if j < n } 3. Topo • Subproblem x(i, j, ki, kj) only depends on strictly smaller i + j, so it is acyclic. 4. Base • x(n, n, 0, 0) is true (all matched!). • x(n, j, ki, kj) is false if ki > 0 (no more characters in A). • x(i, n, ki, kj) is false if kj > 0 (no more characters in B). 5. Original • Solve subproblems via recursive top-down or iterative bottom-up approach. • Solution to the original problem is x(n, n, n/2, n/2). 6. Time • Number of subproblems: O(n⁴). • Work per subproblem: O(1). • Overall time complexity: O(n⁴).
//// ANSWER: The given algorithm is an O(n^4) time algorithm to determine whether Charlie's DNA sequence matches a combination of Alice and Bob's DNA sequences. Here are the steps for the algorithm:

1. Define subproblems: Let A, B, and C be the relevant length-n DNA sequences from Alice, Bob, and Charlie. The subproblem x(i, j, ki, kj) checks if a length-ki suffix of A[i:] and a length-kj prefix of B[j:] can match all characters in the suffix C[(n - ki - kj):]. This is defined for i, j ∈ {0,...,n} and ki, kj ∈ {0,...,n/2}.

2. Relate: To solve a subproblem, check if the character at position i of C matches either A[i] or B[i]. If it does, recurse on the remainder. Otherwise, do not use either A[i] or B[i]. The recurrence relation is defined as follows:
   - x(i, j, ki, kj) = OR { x(i + 1, j, ki + 1, kj) if A[i] = C[n - ki - kj] and ki > 0, x(i, j + 1, ki, kj + 1) if B[j] = C[n - ki - kj] and kj > 0, x(i + 1, j, ki, kj) if i < n, x(i, j + 1, ki, kj) if j < n }

3. Topo: The subproblem x(i, j, ki, kj) only depends on strictly smaller i + j, so it is acyclic.

4. Base: The base cases are x(n, n, 0, 0), which is true (all matched!), x(n, j, ki, kj) if ki > 0 (no more characters in A), and x(i, n, ki, kj) if kj > 0 (no more characters in B).

5. Original: Solve the subproblems using a recursive top-down or iterative bottom-up approach. The solution to the original problem is x(n, n, n/2, n/2).

6. Time: The number of subproblems is O(n^4), work per subproblem is O(1), and overall time complexity is O(n^4).

QUESTION: Describe an O(nks)-time algorithm to find the maximum volume of food Obert can eat given his diet. //// ANSWER: 1. Subproblems • x(i, j, s0): maximum volume of food possible when purchasing a suffix of plates pᵢ to pₙ₋₁, using at most j calories and ordering exactly s0 sweet plates. • Defined for i ∈ {0,...,n}, j ∈ {0,...,k}, s0 ∈ {0,...,s}. 2. Relate • Either order plate pᵢ or not. Guess! • If ordering pᵢ, gain vᵢ in volume but use cᵢ calories. • If pᵢ is sweet, must order one fewer sweet plate. • The recurrence relation: x(i, j, s0) = max { x(i + 1, j, s0), vᵢ + x(i + 1, j - cᵢ, s0 - sᵢ) if cᵢ ≤ j and sᵢ ≤ s0 } 3. Topo • Subproblems x(i, j, s0) only depend on strictly larger i, so it is acyclic. 4. Base • x(n, j, 0) = 0 for any j (no more plates to eat). • x(n, j, s0) = −∞ for s0 > 0 and any j (no more plates, but still need to eat sweet). 5. Original • Solution given by x(0, k, s). 6. Time • Number of subproblems: O(nks). • Work per subproblem: O(1). • Overall time complexity: O(nks).
//// ANSWER: The provided text describes an O(nks)-time dynamic programming algorithm to find the maximum volume of food Obert can eat given his diet, with a total calorie limit k and a limit on the number of sweet plates s.

The algorithm works by defining subproblems x(i, j, s0), which represents the maximum volume of food possible when purchasing a suffix of plates p_i to p_{n-1}, using at most j calories and ordering exactly s0 sweet plates. The algorithm iterates through all possible combinations of i, j, and s0, where i ∈ {0,...,n}, j ∈ {0,...,k}, and s0 ∈ {0,...,s}.

To solve each subproblem, the algorithm considers whether to order plate p_i or not. If ordering p_i, it gains v_i in volume but uses c_i calories and must order one fewer sweet plate if p_i is sweet. The recurrence relation for x(i, j, s0) is defined as follows:

x(i, j, s0) = max { x(i + 1, j, s0), v_i + x(i + 1, j - c_i, s0 - s_i) if c_i ≤ j and s_i ≤ s0 }

The algorithm uses a top-down approach, starting with the base case of x(n, j, 0) = 0 for any j (no more plates to eat) and x(n, j, s0) = -∞ for s0 > 0 and any j (no more plates, but still need to eat sweet). The final solution is given by x(0, k, s).

The time complexity of the algorithm is O(nks), as there are O(nks) subproblems and each subproblem requires O(1) work.

QUESTION: Given the price lists of in-game purchases and ride sharing trips between all pairs of monsters, describe an O(n²)-time algorithm to determine the minimum amount of money Kash must spend in order to catch all n monsters, assuming that she starts at the location of monster m₁. //// ANSWER: 1. Subproblems • x(i, j): minimum cost of catching monsters mᵢ to mₙ, starting at location mⱼ for j ≤ i. 2. Relate • If already at the location of monster mᵢ, catch it for free! • Otherwise, acquire monster mᵢ either by purchasing or ride-sharing to its location. Guess! • If purchasing, spend cᵢ dollars; otherwise, ride-share from mⱼ to mᵢ. • The recurrence relation: x(i, j) = min { x(i + 1, j) if j = i, cᵢ + x(i + 1, j), s(j, i) + x(i, i) } 3. Topo • Subproblems x(i, j) only depend on strictly larger i + j, so it is acyclic. 4. Base • x(n + 1, j) = 0 for any j (no cost when no monsters are left to catch). 5. Original • Solution given by x(1, 1). 6. Time • Number of subproblems: O(n²). • Work per subproblem: O(1). • Overall time complexity: O(n²).
//// ANSWER: Yes, the algorithm you've described is an O(n^2) solution to determine the minimum amount of money Kash must spend in order to catch all n monsters, starting at the location of monster m₁. Here is a summary:

1. Subproblems: x(i, j), where i and j represent the indices of the monsters (mᵢ and mⱼ respectively). The subproblem x(i, j) represents the minimum cost of catching monsters mᵢ to mₙ, starting at location mⱼ for j ≤ i.
2. Relate: If already at the location of monster mᵢ, catch it for free! Otherwise, acquire monster mᵢ either by purchasing or ride-sharing to its location. Guess the cost of acquiring mᵢ (cᵢ if purchasing or s(j, i) + x(i, i) if ride-sharing).
3. Topo: The subproblems are acyclic because they only depend on strictly larger i + j.
4. Base: x(n+1, j) = 0 for any j (no cost when no monsters are left to catch).
5. Original: The solution is given by x(1, 1).
6. Time: The number of subproblems is O(n^2), the work per subproblem is O(1), and overall time complexity is O(n^2).

QUESTION: Given a weighted, directed graph G = (V, E, w) with positive and negative edge weights, and given a particular vertex v ∈ V, describe an O(k|E|)-time algorithm to return the minimum weight of any cycle containing vertex v that also has exactly k edges, or return that no such cycle exists. Recall that a cycle may repeat vertices/edges. //// ANSWER: Assume all vertices in G are reachable from v so that |V| = O(|E|); otherwise, run BFS or DFS to solve single source reachability from v, and replace G with the subgraph reachable from v in O(|E|) time. Construct a new graph G' = (V', E') with:• k + 1 vertices for each vertex v ∈ V: specifically vᵢ for all i ∈ {0, . . ., k}; and• k edges for each edge (u, v) ∈ E: specifically edges (uᵢ₋₁, vᵢ) for all i ∈ {1, ...,k}.Graph G' has (k + 1)|V| = O(k|E|) vertices and k|E| edges in k + 1 layers, and has the property that paths from v₀ to vₖ have a one-to-one correspondence with cycles through v in G of the same weight containing exactly k edges, since exactly one edge is traversed with each increase in layer. So solve SSSP from v₀ in G' to return the minimum weight path to vₖ. Since edges in G' always increase in subscript, G' is a DAG, so we can solve SSSP using DAG relaxation in linear time with respect to the size of G'. So together with initial pruning, this algorithm takes O(k|E|) time in total.
//// ANSWER: To find a cycle containing vertex v with exactly k edges and minimum weight, or determine that no such cycle exists, you can use the following O(k\*|E|)-time algorithm:

1. First, ensure all vertices in G are reachable from v by running BFS or DFS to solve single source reachability from v, and replace G with the subgraph reachable from v in O(|E|) time if necessary.

2. Construct a new graph G' = (V', E') as follows:
   - For each vertex v ∈ V, create k+1 vertices: specifically v_i for all i ∈ {0, ..., k}.
   - For each edge (u, v) ∈ E, create k edges: specifically edges (u_{i-1}, v_i) for all i ∈ {1, ...,k}.

3. Solve the Shortest Path Problem (SSSP) from vertex v_0 in graph G'. Since edges in G' always increase in subscript, G' is a Directed Acyclic Graph (DAG), so we can solve SSSP using DAG relaxation in linear time with respect to the size of G'.

4. The minimum weight path from v_0 to v_k in graph G' corresponds to a cycle through v in G of the same weight containing exactly k edges, since exactly one edge is traversed with each increase in layer.

5. If no path exists from v_0 to v_k, then there is no cycle through v in G containing exactly k edges.

This algorithm takes O(k\*|E|) time in total, as it performs a single BFS/DFS operation (O(|E|)), constructs the new graph (O(k\*|V| + k\*|E|)) and solves SSSP on the new graph (O(|V|^2 + |E|)).

QUESTION: Given a 3-color labeling c : E → {red, green, blue} of a connected, weighted, undirected graph G = (V, E, w) containing only positive edge weights, and given two vertices s,t ∈ V, describe an efficient algorithm to return a path from s to t having minimum color cost. //// ANSWER: Construct a new graph G' = (V', E') with:• 3 vertices for each vertex v ∈ V: specifically vᵢ for i ∈ {red, green, blue} corresponding to arriving at vertex v via an edge with color i;• (vertex-edges) 3 undirected edges for each vertex v ∈ V: specifically {vred, vblue}, {vgreen, vred}, and {vblue, vgreen} of weight wc; and• (edge-edges) 1 undirected edge for each undirected edge {u,v} ∈ E of weight w and color c(u, v): specifically undirected edge {uc(u,v), vc(u,v)} with weight w.Graph G' has 3|V| vertices and 3|V| + |E| edges, and has the property that the minimum weight of any path in G' from any vertex sᵢ to any vertex tⱼ for i, j ∈ {red, green, blue} is equal to the minimum color cost of any 3-color labeled path in G from s to t, as switching colors at a vertex requires traversing an edge of weight w. So solve SSSP three times, once from each vertex sᵢ and find the minimum weight of any path to any tⱼ, and then return a minimum path by constructing parent pointers as shown in lecture. Since this graph only has positive edge weights, we can solve SSSP using Dijkstra in O(|V| + |E| + |V|log |V|) = O(|E| + |V|log|V|) time.
//// ANSWER: The algorithm described in the user's message constructs a new graph G' from the given graph G with positive edge weights and a 3-color labeling c of its edges. The new graph G' is used to find the minimum color cost path between two vertices s and t in G.

Here's a summary of the steps involved:

1. Create a new graph G' with three vertices for each vertex v in V, labeled red (vred), green (vgreen), and blue (vblue).
2. Add 3 undirected edges between each pair of vertices corresponding to the same original vertex, with weights equal to the weight of the edge connecting them in G with the specified color.
3. For each edge {u, v} in E with weight w and color c(u, v), add an undirected edge between the corresponding vertices uc(u,v) and vc(u,v) in G' with the same weight w.
4. Solve Shortest Path Finding (SPF) three times on graph G', once from each vertex sred, sgreen, and sbue, finding the minimum weight of any path to any tⱼ for i, j ∈ {red, green, blue}.
5. Return a minimum path by constructing parent pointers as shown in lecture.

The time complexity of this algorithm is O(|E| + |V|log|V|) due to the application of Dijkstra's SPF algorithm on graph G'. This is because there are 3|V| vertices and 3|V| + |E| edges in graph G', and since Dijkstra runs in O(|V| + |E| + |V|log|V|) time, the overall complexity is dominated by this term.

QUESTION: Ranger Raargorn needs to deliver a message from her home town of Tina's Mirth to the town of Riverdell, but the towns of Midgard have been overrun by an army of k Orks. Raargorn has a map of the n towns and 3n roads in Midgard, where each road connects a pair of towns in both directions. Scouts have determined the number of Orks rᵢ ≥ 1 stationed in each town i (there is at least one Ork stationed in each town). Describe an O(k)-time algorithm to find a path from Tina's Mirth to Riverdell on which Raargorn will encounter the fewest total Orks in towns along the way. //// ANSWER: Construct a graph G = (V, E) with:• a chain of rᵢ vertices (v₁, ..., vᵣᵢ) connected by rᵥ – 1 edges for each town v, i.e., unweighted directed edge (vᵢ, vᵢ₊₁) for all i ∈ {1, . . ., rᵥ − 1}; and• two unweighted directed edges (vᵣᵥ, u₁) and (vᵣᵥ, u₁) for each road between towns u and v.Graph G has ∑ rᵥ = k vertices and 2(3n) + ∑(rᵥ − 1) = 5n + k edges. Since there is at least one Ork in each town, k ≥ n, so G has size O(k). Let s and t correspond to the towns of Tina's Mirth and Riverdell respectively. Graph G has the property that any path from s₁ to tᵣ₁ corresponds to a path from Tina's Mirth to Riverdell crossing edges equal to the number of Orks encounters in towns along the way, since for any road connecting towns u and v, going from u₁ to v₁ requires traversing rᵥ edges in G. So solve unweighted SSSP from s₁ to tᵣ₁ using BFS in O(k) time, and return the sequence of towns visited along the found shortest path by following parent pointers.
//// ANSWER: To find a path from Tina's Mirth to Riverdell that encounters the fewest total Orks, you can construct a graph G with O(k) vertices and edges as follows:

1. For each town i in Midgard, create a chain of rᵢ vertices (v₁, ..., vᵣᵢ) connected by rᵥ - 1 unweighted directed edges (vᵢ, vᵢ₊₁) for all i ∈ {1, . . ., rᵥ − 1}. This represents the presence of Orks in each town.
2. Add two unweighted directed edges (vᵣᵥ, u₁) and (vᵣᵥ, vᵣ₊₁) for each road between towns u and v.

The graph G has ∑ rᵥ = k vertices and 2(3n) + ∑(rᵥ − 1) = 5n + k edges. Since there is at least one Ork in each town, k ≥ n, so G has size O(k).

Let s and t correspond to the towns of Tina's Mirth and Riverdell respectively. In this graph, any path from s₁ to tᵣ₁ corresponds to a path from Tina's Mirth to Riverdell crossing edges equal to the number of Orks encountered in towns along the way.

Solve unweighted Shortest Simple Path Problem (SSSP) from s₁ to tᵣ₁ using Breadth-First Search (BFS) in O(k) time, and return the sequence of towns visited along the found shortest path by following parent pointers. This algorithm will find a path from Tina's Mirth to Riverdell that encounters the fewest total Orks in towns along the way.

QUESTION: Given a cycle-sparse graph, describe an O(|V|³)-time algorithm to return the number of negative-weight cycles in G. //// ANSWER: Construct a new graph G' by adding a supernode x to G with a zero-weight directed edge (x, v) for each v ∈ V. Then run SSSP from x in G' using Bellman-Ford to label each vertex v ∈ V with its shortest path distance δ(x, v). For each v ∈ V, δ(x,v) = −∞ if and only if v is reachable from a negative-weight cycle in G (since adding x does not add or remove any cycles). Further, for any directed edge (u, v), if δ(x, u) = δ(x,v) = -∞, then both u and v are each reachable from the same simple negative-weight cycle (since v is reachable from u and each vertex is reachable from at most one simple negative-weight cycle).So, construct a new graph G" on only the vertices v ∈ V where δ(x,v) = −∞ in G', with an undirected edge between u and v in G" if they share a directed edge in G. Graph G" has the property that the number of connected components in G" equals the number of negative-weight cycles in G, so count and return the number of connected components in G" using Full-BFS or Full-DFS. This algorithm takes O(|V|+|E|) time to construct G', O(|V||E|) time to run Bellman-Ford, O(|V|+|E|) time to construct G", and then O(|V|+|E|) time to count connected components in G", leading to an O(|V||E|) = O(|V|³) running time in total.
//// ANSWER: The algorithm you described is a valid solution for finding the number of negative-weight cycles in a cycle-sparse graph with a time complexity of O(|V|^3). Here's a summary of the steps:

1. Construct a new graph G' by adding a supernode x to the original graph G, and adding zero-weight directed edges from x to each vertex v ∈ V.
2. Run Bellman-Ford algorithm on G' starting from node x. This step takes O(|V||E|) time.
3. For each vertex v ∈ V, if its shortest path distance from the supernode x is -∞ (δ(x,v) = -∞), then v is reachable from a negative-weight cycle in G.
4. Construct a new graph G" on only the vertices v ∈ V where δ(x,v) = -∞ in G', with an undirected edge between u and v in G" if they share a directed edge in G.
5. Count the number of connected components in G" using Full-BFS or Full-DFS. This step takes O(|V|+|E|) time.
6. The number of connected components in G" equals the number of negative-weight cycles in G, so return this value as the result.

The total time complexity is O(|V||E| + |V| + |V||E| + |V| + |V|+|E|) = O(|V|^3).

QUESTION: Assuming she departs s at half capacity, [b/2], describe an O(nlogn)-time algorithm to determine the maximum amount of energy Bellham can have in her battery upon reaching t. //// ANSWER: Construct graph G with a vertex for each of the n locations in Norway and a directed edge for each of the O(n) roads: specifically for each road from location u to v, add directed edge (u, v) weighted by J(u,v). Then [b/2] minus the weight of a minimum-weight path from s to t in G would correspond to the maximum energy Bellham could have upon reaching t; or at least it would be if she did not either exceed or exhaust her tank along the way.First we show that every minimum-weight path from s to t in G is simple. It suffices to show that every directed cycle in G has positive weight. Consider cycle (c₀, ..., cₖ₋₁, cₖ = c₀). C has weight ∑ᵢ₌₁ᵏ J(cᵢ₋₁, cᵢ) > ∑ᵢ₌₁ᵏ mg(h(cᵢ) - h(cᵢ₋₁)) = 0, as desired.Any simple path in G traverses at most n - 1 edges, so the magnitude of its weight is at most (n - 1)k < b/2. Thus [b/2] minus the weight of any simple path in G will always be > 0 and < b (so Bellham cannot exhaust or exceed her tank by driving on a simple path from s to t).Lastly, we find the weight of a minimum-weight path from s to t by solving SSSP. Unfortunately using Bellman-Ford takes O(n²) time which is too slow. However, we can re-weight edges in G to be positive while preserving shortest paths by exploiting the provided vertex potentials, similar to Johnson's algorithm. Specifically, create new graph G', identical to G, except change the weight of each edge (u, v) to J(u, v) – mg(h(v) – h(u)) > 0. This transformation preserves shortest paths since the weight of each path from, e.g., a to b changes by the same amount, namely by mg(h(b) – h(a)). So run Dijkstra from s to find the minimum weight D of any path to t in G', and return [b/2] – (D – mg(h(b) – h(a))).Constructing G takes O(n) time, reweighting to G' also takes O(n) time, and then running Dijkstra from s in G' takes O(n log n) time, leading to O(n log n) time in total.
//// ANSWER: The provided text describes an algorithm that solves the problem of determining the maximum amount of energy Bellham can have in her battery upon reaching t, given that she departs s at half capacity [b/2]. Here's a summary of the steps:

1. Create a weighted directed graph G with a vertex for each location in Norway and a directed edge for each road, where the weight of an edge (u, v) is J(u,v).
2. Show that every minimum-weight path from s to t in G is simple (i.e., it does not contain any cycles).
3. Find the weight of a minimum-weight path from s to t by solving the Single-Source Shortest Path (SSSP) problem on the graph G. To do this, re-weight the edges in G to be positive while preserving shortest paths, similar to Johnson's algorithm. Specifically, create new graph G', identical to G, except change the weight of each edge (u, v) to J(u,v) - mg(h(v) - h(u)) > 0.
4. Run Dijkstra's algorithm from s in G' to find the minimum weight D of any path to t in G'. The maximum amount of energy Bellham can have upon reaching t is [b/2] - (D - mg(h(t) - h(s))).

The time complexity of this algorithm is O(n log n), where n is the number of locations in Norway. This is because constructing G takes O(n) time, reweighting to G' also takes O(n) time, and then running Dijkstra from s in G' takes O(n log n) time.

QUESTION: For each function, state its worst-case running times in terms of n and k. //// ANSWER: O(n²)
//// ANSWER: 1. LIS (Longest Increasing Subsequence): Worst-case running time is O(n log n).
  2. DAG Shortest Paths (Directed Acyclic Graph shortest paths): Worst-case running time for algorithms like Dijkstra's and Bellman-Ford is O(E + V log V), where E is the number of edges and V is the number of vertices in the graph. In this context, since we are considering n subproblems (vertices) and F transitions per subproblem (edges), the worst-case running time would be O(nF + n log n).
  3. Floyd-Warshall: Worst-case running time is O(V³), where V is the number of vertices in the graph. In this context, since we are considering n subproblems (vertices), the worst-case running time would be O(n³).
  4. Bellman–Ford: Worst-case running time is O(E + V log V), where E is the number of edges and V is the number of vertices in the graph. In this context, since we are considering n subproblems (vertices) and F transitions per subproblem (edges), the worst-case running time would be O(nF + n log n).
  5. Floyd-Warshall with weighted edges: Worst-case running time is O(V³). In this context, since we are considering n subproblems (vertices), the worst-case running time would be O(n³).
  6. Piano Fingering problem: The given solution suggests a worst-case running time of Θ(η·F²), where η is the number of different notes and F is the maximum number of fingers. The number of different notes does not affect the time complexity, so the worst-case running time would still be Θ(F²).

QUESTION: For each function, state its expected running times in terms of n and k. //// ANSWER: O(n)
//// ANSWER: 1. LIS (Longest Increasing Subsequence): The expected running time for Longest Increasing Subsequence is O(n log(n)). This is because it uses dynamic programming and binary search, which have the stated complexities.

2. DAG Shortest Paths (Directed Acyclic Graph), Bellman-Ford, Floyd-Warshall: The expected running time for these algorithms on a graph with V vertices and E edges is O(V*E) or O(VE) in terms of n and k, where n = V and k = E. These algorithms are used to find the shortest paths from a single source vertex to all other vertices in the graph. Bellman-Ford is used for graphs with negative edge weights, while Floyd-Warshall is used for all graphs.

3. Piano Fingering: The expected running time for the Piano Fingering problem, as given in the context, is O(n*F^2) or O(nF^2) in terms of n and k, where n = number of notes and F = number of fingers. This is because there are Θ(n) subproblems (one for each note), and each subproblem has a work complexity of Θ(F) due to the possible number of fingers, and the algorithm needs to consider all pairs of fingers, leading to a total complexity of O(n*F^2).

QUESTION: For each function, state its worst-case running times in terms of n and k. //// ANSWER: O(n + k)
//// ANSWER: 1. Subproblems: The time complexity for solving x(i, f) is Θ(F), as we need to consider all possible values of the next finger (f'). Since there are n subproblems and F subproblems per each, the total time complexity is O(n * F).

2. Relate: The time complexity for finding the optimal solution using the formula x(i, f) = min{x(i + 1, f') + d(ti, f, ti+1, f') | 1 ≤ f' < F} is O(F), as we need to consider all possible values of the next finger (f'). Since this step is repeated n times, the total time complexity is O(n * F).

3. Topological order: The time complexity for sorting the subproblems in decreasing i order is O(n log n), as we use a sorting algorithm with linearithmic time complexity (e.g., MergeSort or QuickSort).

4. Base: The time complexity for setting x(n − 1, f) = 0 is O(F), as we need to consider all possible values of the finger (f). Since there is only one base case, the total time complexity is O(F).

5. Combine multiple subproblems:
   - DAG shortest paths: Bellman-Ford has a worst-case running time of O(n * E), where E is the number of edges in the graph. In this problem, E = n * (F - 1), as each node has F - 1 outgoing edges. So, the total time complexity for Bellman-Ford is O(n^2 * F).
   - Floyd-Warshall: The worst-case running time for Floyd-Warshall is O(n^3). However, since we don't have a complete graph (as each node has F outgoing edges instead of n), the actual time complexity is O(n^2 * F).
   - Longest Increasing Subsequence (LIS): The worst-case running time for LIS is O(n log n). However, since we don't have a complete sequence (as each finger has F possible values instead of n), the actual time complexity is O(F log F).
   - Piano Fingering: The time complexity for solving piano fingering problems is problem-dependent and not directly related to n or k. It depends on the specific rules and constraints of the piano piece being played.

In summary, the worst-case running times in terms of n and k are as follows:
- Subproblems: O(n * F)
- Relate: O(n * F)
- Topological order: O(n log n)
- Bellman-Ford: O(n^2 * F)
- Floyd-Warshall: O(n^2 * F)
- LIS: O(F log F)
- Piano Fingering (problem-dependent).

QUESTION: For each function, state its expected running times in terms of n and k. //// ANSWER: O(n + k)
//// ANSWER: 1. Subproblems: x(i, f) for all i from 0 to n-1 and f from 1 to F. The number of subproblems is n*F. Each subproblem requires O(F) work, so the total work for subproblems is O(n*F^2).

2. Relating subproblems: For each subproblem x(i, f), we need to calculate min{x(i + 1, f') + d(ti, f, ti+1, f') | 1 ≤ f' < F}, which requires O(F) work per subproblem. Since there are n*F subproblems, the total work for relating subproblems is O(n*F^2).

3. Topological order: Sorting the subproblems in decreasing i and any f order requires O(n*log(n)) or O(F*log(F)) depending on the sorting algorithm used. Since we are assuming F is a constant, the time complexity for topological order is O(n*log(n)).

4. Base case: Calculating x(n-1, f) requires no work since it is given as 0.

5. Combining multiple subproblems:
   - DAG shortest paths, Bellman–Ford, Floyd-Warshall: All these algorithms have a time complexity of O(n^3) or O(n^2*log(n)) depending on the graph's sparsity and the specific implementation. However, since F is a constant in this problem, their time complexities reduce to O(n^2) or O(n*log(n)).
   - LIS: The Longest Increasing Subsequence problem has a time complexity of O(n*log(n)).
   - Piano Fingering: This problem does not have an established algorithm with a known time complexity. However, since it involves dynamic programming and solving subproblems, we can assume that its time complexity is similar to the other methods mentioned above.

In summary, the expected running times for each function are as follows:
- Subproblems: O(n*F^2)
- Relating subproblems: O(n*F^2)
- Topological order: O(n*log(n))
- DAG shortest paths, Bellman–Ford, Floyd-Warshall: O(n^2) or O(n*log(n))
- LIS: O(n*log(n))
- Piano Fingering: Similar to the other methods (O(n^2) or O(n*log(n)))

QUESTION: State which item(s) in the array could have the key with the smallest integer //// ANSWER: A
//// ANSWER: The item that has the key with the smallest integer is item 'A'. This is because the keys are integers, and the smallest integer among them is associated with item 'A'.

QUESTION: State which item(s) in the array could have the key with the third smallest integer //// ANSWER: B, C, D, E, F, G
//// ANSWER: The items in the array that could have the key with the third smallest integer are B, C, D, E, F, and G. This is because these items are the ones that have keys assigned to them in the problem context. Since we don't know the specific values of the keys, it cannot be determined which item has the third smallest key among them. However, we can say for certain that items A and I do not have keys that could potentially be the third smallest since they are not included in the list of items with assigned keys (B, C, D, E, F, G).

QUESTION: State which item(s) in the array could have the key with the largest integer //// ANSWER: F, G, H, I, J
//// ANSWER: The items in the array that could have the key with the largest integer are F, G, H, I, and J. These keys correspond to the fingers 5, 6, 7, 8, and 9, respectively. In this context, the keys are represented as letters from A (finger 1) to J (finger 9). The largest integer in this range is 9, which corresponds to key J. However, since we want to find all the keys that could have a key with the largest integer, we list F, G, H, I, and J as possible candidates.

QUESTION: State the array representation of H, first before and then after performing the operation H.delete_max(). //// ANSWER: Before: [93, 85, 38, 23, 63, 11, 13, 17] After: [85, 63, 38, 23, 17, 11, 13]
//// ANSWER: The array representation of H before performing the operation `H.delete_max()` is [93, 85, 38, 23, 63, 11, 13, 17]. After performing the operation `H.delete_max()`, the maximum value in the array (93) will be removed, and the remaining elements will be rearranged in non-decreasing order. So, the array representation of H after performing the operation `H.delete_max()` is [85, 63, 38, 23, 17, 11, 13].

QUESTION: Perform operation S.delete_at(3) on S including any rotations, and then list the items stored in the leaves of s in traversal order, after the operation has completed. //// ANSWER: (17,85, 11, 13)
//// ANSWER: After performing S.delete_at(3), the sequence S becomes (17, 85, 11, 4, 13). To find the items stored in the leaves of this sequence in traversal order, we traverse the tree as follows:

1. Traverse the left child of the root (17): 85
2. Traverse the left child of 85 (null, since it has no children): skip
3. Traverse the right child of 85 (11)
4. Traverse the left child of 11 (null, since it has no children): skip
5. Traverse the right child of 11 (null, since it has no children): skip
6. Traverse the parent of 11 (85): skip
7. Traverse the parent of 85 (17): skip
8. Traverse the root (null, since it has no children): skip
9. Since we have reached the end of the sequence and there are no more nodes to traverse, the leaves of the tree in traversal order after the operation are (17, 85, 11, 13).

QUESTION: For House Puffle Huff, students must be sorted by friend number, i.e., how many of the other n 1 incoming students they are friends with, which can be determined in O(1) time. //// ANSWER: Friend numbers are non-negative integers less than n, so we can use counting sort to sort the students in worst-case O(n) time. (Radix sort also works with the same running time.) Since we have to compute friend number for each student, any algorithm will take at least Ω(n) time, so this is optimal.
//// ANSWER: In the context provided, the question seems to be about finding an efficient way to solve a dynamic programming problem related to playing notes with a minimum total difficulty.

1. Subproblems: x(i, f) represents the minimum total difficulty for playing notes ti, ti+1,..., tn-1 starting with finger f on note ti. For 0 < i < n and 1 < f < F.
2. Relate: To find the optimal solution for x(i, f), we need to consider all possible assignments of fingers (f') for ti+1 and calculate the total difficulty using d(ti, f, ti+1, f'). The minimum value among these possibilities is the optimal solution for x(i, f).
3. Topological order: The subproblems are solved in decreasing i (any f order) to ensure that solutions for larger i can be used to solve smaller i.
4. Base: The base case is x(n − 1, f) = 0, as there are no transitions when reaching the last note.
5. Original problem: The goal is to assign fingers to notes to minimize the total difficulty. To avoid certain finger assignments (f' ∈ {4,5}), we need a table mapping start fingers to optimal solutions for x(i + 1). This table will help us fill in the missing information when calculating x(i) by knowing which finger at the start of x(i + 1).

In summary, the problem involves solving a dynamic programming problem with subproblems related to playing notes with minimum total difficulty. The solutions are found using a top-down approach, starting from the last note and working backward. The table mapping start fingers to optimal solutions for x(i + 1) is necessary to avoid certain finger assignments and find the overall optimal solution for minimizing the total difficulty when assigning fingers to notes.

QUESTION: For House Craven Law, students must be sorted by the weight of their books. Book weights cannot be measured precisely, but the Sorting Sock has a scale that can determine in O(1) time whether one set of books has total weight greater than, less than, or equal to another set of books. //// ANSWER: A scale weighing is a comparison with a constant number of outcomes, so the comparison sort Ω(nlogn) lower bound applies. So we cannot do better than by using an worst-case O(nlogn) sorting algorithm, e.g., merge sort, using the scale to compare one student's books against another's.
//// ANSWER: The Sorting Sock provides a comparison in O(1) time between two sets of books, but it does not provide an ordering of the books within each set. Therefore, we cannot use the Sorting Sock to perform an in-place sort. Instead, we need to compare all pairs of students' books, which requires a comparison-based sorting algorithm.

Since the comparison sort has a lower bound of Ω(n log n), we cannot do better than using a worst-case O(n log n) sorting algorithm, such as merge sort or quicksort. The Sorting Sock can be used to compare one student's books against another's during the sorting process, but it does not affect the overall time complexity of the sorting algorithm.

QUESTION: For House Driven Gore, students must be sorted by bravery, which can't be directly measured or quantified, but for any set of students, the Sorting Sock can determine the bravest among them in O(1) time, e.g., by presenting the students with a scary situation. //// ANSWER: We can't quantify bravery, so we can't hope to use any integer-based algorithms. However, the Sorting Sock can find a student of maximum bravery in O(1) time, so we repeatedly find and select a bravest student among all previously unselected students in worst-case O(n) time, which is again optimal. (This is priority queue sort, using the Sorting Sock as the priority queue to find the maximum.)
//// ANSWER: In the given context, we are trying to assign fingers to notes to minimize total difficulty. The problem can be solved by using dynamic programming with a table mapping x(i, f) representing the minimum total difficulty for playing notes ti, ti+1,..., tn−1 starting with finger f on note ti.

   Here's how we can answer your question:

   1. Subproblems: x(i, f) = minimum total difficulty for playing notes ti, ti+1,..., tn−1 starting with finger f on note ti. For 0 < i < n and 1 < f <F.

   2. Relate: Guess next finger: assignment f' for ti+1. x(i, f) = min{x(i + 1, f') + d(ti, f, ti+1, f') | 1 ≤ f' < F}. This step helps us find the optimal solution by considering all possible fingers to play the next note and choosing the one that minimizes the total difficulty.

   3. Topological order: Decreasing i (any f order). We start from the last note (i = n-1) and work our way back to the first note (i = 0).

   4. Base: x(n − 1, f) = 0 (no transitions). Since we are working backward, the base case is when there are no more notes left to play (i = n-1), so the total difficulty is 0.

   5. Original problem: min{x(0, f) | 1 ≤ f < F}. This is the overall goal of finding the minimum total difficulty for playing all the notes using any finger.

   The time complexity of this algorithm is Θ(n.F) subproblems and Θ(F) work per subproblem, which gives us a total time complexity of Θ(n.F2). This solution does not depend on the number of different notes!

QUESTION: For House Leather Skin, students must be sorted by their magical lineage: how many of a student's ancestors within the previous 3[log n] + 4 generations were magical. Recall that humans, magical or not, always have two parents in the previous generation, unlike binary tree nodes which have at most one. Assume the Sorting Sock can compute the magical lineage of a student in O(1) time. //// ANSWER: Each student has at most 2k ancestors in the kth generation preceding. Thus the number of wizard ancestors will be a non-negative number bounded above by 3 3 [log n]+4 2k < 23(logn+1)+5 = 2823 logn = 28n3log2 = O(n) for any c > 3 log 2. Thus we can use radix sort to sort the students by their magical lineage in worst-case O(n) time, which is again optimal.
//// ANSWER: To minimize the total difficulty of playing a sequence of notes, you should expand subproblems based on the start condition. The subproblem x(i, f) represents the minimum total difficulty for playing notes ti, ti+1, ..., tn-1 starting with finger f on note ti.

   - Subproblems: x(i, f) = minimum total difficulty for playing notes ti, ti+1,..., tn−1 starting with finger f on note ti. For 0 < i < n and 1 < f <F.
   - Relate: Guess next finger: assignment f' for ti+1. x(i, f) = min{x(i + 1, f') + d(ti, f, ti+1, f') | 1 ≤ f' < F}.
   - Topological order: Decreasing i (any f order).
   - Base: x(n − 1, f) = 0 (no transitions).
   - Original problem: min{x(0, f) | 1 ≤ f < F}
   - Time: Θ(n.F) subproblems. Θ(F) work per subproblem. Θ(η· F2). No dependence on the number of different notes!

   In your first attempt, you need to create a table mapping start fingers to optimal solutions for x(i + 1). This table will help you fill in the missing information when relating the subproblems. The table should have dimensions n-1 by F and store the optimal solution x(i+1) for each possible starting finger f.

   To avoid using fingers 4 or 5, you can modify the table to only consider fingers {3,4} as valid options. If {f, f'} = {3,4}, then you should use a different approach to determine the optimal solution for x(i+1).

   After creating and filling this table, you can solve the original problem by finding min{x(0, f) | 1 ≤ f < F} using your subproblem expansion.

QUESTION: Given three arrays A, B, C, each containing n integers, give an O(n²)-time algorithm to find whether some a ∈ A, some b ∈ B, and some c∈ C have zero sum, i.e., a + b + c = 0. State whether your running time is worst-case, expected, and/or amortized. //// ANSWER: For each pair of numbers (a, b) ∈ A × B, store a + b in a hash table H. Then return Yes if -c appears in H for any c ∈ C, and return No otherwise. Correctness: If any -c appears in H for any c∈ C, then –C a' + b' for some (a', b') ∈ A × B so a' + b + c = 0. Otherwise, there is no c∈ C for which -c = a' + b' for any (a', b') ∈ A × B. Running Time: There are O(n²) pairs in A × B, so inserting them into H takes expected O(n²) time. Then checking whether each -c appears in H takes expected O(1) time each, and expected O(n) in total. So this algorithm runs in expected O(n²) time.
//// ANSWER: Yes, the given algorithm is correct for finding whether there exists a triplet (a, b, c) such that a + b + c = 0, where a belongs to A, b belongs to B, and c belongs to C. The running time of this algorithm is expected O(n²), as stated in the answer.

Here's a breakdown of the steps:
1. For each pair (a, b) from A × B, store the sum a + b in a hash table H. This step takes expected O(n²) time because there are O(n²) pairs in A × B.
2. Check if -c appears in H for any c ∈ C. If it does, return Yes; otherwise, return No. Since checking whether each -c appears in H takes expected O(1) time and we need to check each element of C once, the total expected time taken for this step is O(n).

So, the overall expected running time of the algorithm is O(n²), which includes both steps.

QUESTION: Given a Sequence AVL Tree T containing n nodes, and a pointer to a node v from T, describe an O(log n)-time algorithm to return the (zero-indexed) index i of node v in the traversal order of T. //// ANSWER: Our algorithm will be to walk up the tree from v to the root r of the Sequence AVL Tree, counting the nodes preceding v in the traversal order along the way, since the number of nodes preceding v in the tree is equivalent to v's (zero-indexed) index. Let #v(u) be the number of vertices preceding v in a vertex u's subtree, where v is in the subtree of u. Then #v(v) = v.left.size if v has a left child and zero otherwise; and can be computed in O(1) time. Then, for every ancestor of u starting from v, we compute #v(u.parent) from #(u). There are two cases: • Case 1, u is the left child of u.parent: then all the nodes preceding v in the subtree of u.parent are in the subtree of u, so set #v(u.parent) = #(u). • Case 2, u is the right child of u.parent: then all nodes in the left subtree of u.parent precede v (as does u), so set #v(u.parent) = 1+u.parent.left.size + #v(u). Then return #v(r), since this is the number of nodes preceding v in r's subtree (i.e., the entire tree). Correctness is argued within the algorithm description. This algorithm spends worst-case O(1) work for each ancestor of v, so since the Sequence AVL Tree is balanced, the number of ancestors is bounded by O(log n), and the algorithm runs in worst-case O(log n) time.
//// ANSWER: The provided algorithm describes an O(log n)-time algorithm to find the (zero-indexed) index i of a node v in the traversal order of a Sequence AVL Tree T. This is achieved by walking up the tree from v to the root r, counting the nodes preceding v along the way. The number of nodes preceding v in the subtree of a node u (#v(u)) can be computed in O(1) time and is recursively updated as we move up the tree.

The algorithm runs in worst-case O(log n) time because it spends O(1) work for each ancestor of v, and since the Sequence AVL Tree is balanced, the number of ancestors is bounded by O(log n).

QUESTION: Describe a database supporting the following operations, where k is the number of recorded but unpatched holes in the network at the time of the operation. //// ANSWER: To implement the database, maintain the following data structures: • A Set AVL tree Tp for each pipe p containing all the unpatched holes in p keyed by hole distance • A Hash Table D mapping each pipe p to its tree Tp • A Binary Min Heap Q containing each consecutive pair of holes (p, d1, d2) appearing on the same pipe p with key being the distance |d2 – d₁| between them, and any lonely holes (p, d) (holes that are alone on their pipes) with key ∞ (when multiple stored items have the same key, we store them in a Hash Table keyed by (p, d1, d2) or (p, d)) • A Hash Table C mapping each consecutive hole pair (p, d1, d2) or lonely hole (p, d), to their location in Q.
//// ANSWER: To support the operations as described, you can create a database with the following data structures:

1. An AVL tree Tp for each pipe p, containing all unpatched holes in that pipe, keyed by hole distance. This will allow efficient insertion, deletion, and search operations based on hole distance.
2. A hash table D mapping each pipe p to its corresponding AVL tree Tp, making it easy to access the appropriate tree for a given pipe.
3. A binary min-heap Q containing consecutive pairs of holes (p, d1, d2) appearing on the same pipe p and lonely holes (p, d), with the key being the distance |d2 – d₁| between them when they are part of a pair or ∞ for lonely holes. This will allow efficient extraction of the hole pair or lonely hole with the smallest distance difference.
4. A hash table C mapping each consecutive hole pair (p, d1, d2) or lonely hole (p, d) to their location in Q. This will enable quick lookup of a hole's position in the heap based on its pipe and distance.

With these data structures in place, you can perform the following operations efficiently:

- Inserting a new unpatched hole into the appropriate AVL tree Tp for its pipe.
- Deleting a patched hole from the AVL tree Tp for its pipe.
- Finding the pair or lonely hole with the smallest distance difference in Q, which represents the two holes closest together on the same pipe that need to be addressed next.
- Updating the position of a hole pair or lonely hole in Q based on its new location after being patched or unpatched.

QUESTION: Describe a database supporting the following worst-case operations: //// ANSWER: To implement the database, maintain a single Set AVL Tree T containing each user ID and their status, keyed by ID. In additional, augment each node x in T with four subtree properties: • x.size: the number of IDs in the subtree (as discussed in lecture). • x.suffix = (d, m): the smallest ID d in the subtree for which each of the m IDs d' ≥ d in the subtree is active, or None. Computable in O(1) time as either the suffix (dr, MR) of the right subtree, or if MR is equal to the size of the right subtree and × is active, return the suffix (dL, ML) of the left subtree but add MR + 1 to M₁ (or (x.key, MR + 1) if left suffix is None). • x.prefix = (d, m): the largest ID d in the subtree for which each of the m IDs d' ≤ d in the subtree is active, or None. Computable in O(1) time as either the prefix (d₁, ML) of the right subtree, or if m₁ is equal to the size of the left subtree and × is active, return the prefix (dR, MR) of the right subtree but add ML+1 to MR (or (x.key, m₁+1) if right prefix is None). • x.substring = (a,b,m): a, b are IDs from the subtree where each of the m IDs d in the subtree with a ≤ d ≤ b is active and m is maximized. Computable in O(1) time by tak- ing the max of the substring within either left or right subtree, or the substring spanning the two subtrees if x is active. Specifically, consider the substrings of left and right subtrees, (AL, BL, ML) and (ar, bR, MR) respectively, and then if x is active, consider the suffix of the left subtree (d₁, m'₁) and the prefix of the right subtree (dR, M'R). Then depending on which of (m₁, m'₁ + 1 + M'R, MR) is the largest, return (a₁, bŁ, M₁), (d₁, dR, M'₁ + 1 + m'₨), or (AR, BR, MR) respectively.
//// ANSWER: To support the worst-case operations you described, we can design a database using an AVL Tree data structure. Each node in the tree will store the following information:

1. User ID and status (active or inactive).
2. The number of IDs in the subtree (x.size).
3. The smallest active ID in the subtree for which each greater ID is also active, or None (x.suffix = (d, m)).
4. The largest active ID in the subtree for which each smaller ID is also active, or None (x.prefix = (d, m)).
5. A substring containing the maximum number of active IDs between two given IDs a and b (x.substring = (a,b,m)).

The operations can be performed in O(1) time as follows:

1. x.suffix: Check the suffix of the right subtree if it exists; otherwise, if the size of the right subtree equals the number of nodes and the current node is active, return the suffix of the left subtree but add 1 to M₁. If the left suffix does not exist, return (x.key, MR + 1).
2. x.prefix: Check the prefix of the right subtree if it exists; otherwise, if the size of the left subtree equals the number of nodes and the current node is active, return the prefix of the right subtree but add 1 to ML. If the right prefix does not exist, return (x.key, m₁+1).
3. x.substring: Consider the substrings of the left and right subtrees, (AL, BL, ML) and (ar, bR, MR), respectively. If the current node is active, consider the suffix of the left subtree (d₁, m'₁) and the prefix of the right subtree (dR, M'R). Then return the maximum of the substrings within either the left or right subtrees or the substring spanning both subtrees. Specifically, depending on which of (m₁, m'₁ + 1 + M'R, MR) is the largest, return (a₁, bŁ, M₁), (d₁, dR, M'₁ + 1 + m'₨), or (AR, BR, MR) respectively.

QUESTION: 2^{2η} ∈ Θ(2η). //// ANSWER: False. This statement is equivalent to saying k² ∈ O(k) for k = 2n. Constants in exponents matter asymptotically!
//// ANSWER: You are correct that the statement "2^{2η} ∈ Θ(2η)" is false. The Big Theta notation (Θ) includes both an upper bound and a lower bound, with constants in the exponent considered when comparing the growth rates of functions. In this case, the function 2^{2η} does not grow at the same rate as 2η because the constant 2 in the exponent of 2^{2η} is squared, which significantly affects its growth rate compared to a linear function like η.

To clarify, if we consider k² ∈ O(k) for some constant k, it means that there exists a constant c > 0 such that k² ≤ c*k for all sufficiently large values of k. However, in the case of k = 2n, where n is a large number, this statement would be false because (2n)² = 4n² ≫ n for any positive constant c.

In the given context, it seems that there might have been a mistake in the original problem statement regarding the growth rates of functions.

QUESTION: If T(n) = 2T (n) + n² and T(1) = Θ(1), then T(n) = O(n²). //// ANSWER: False. This is an example of Case II of Master Theorem, since a = 1, b = 3, f(n) = n² and n² = O(nlog3/29/4 log n). Thus, the recurrence evaluates to T(n) = O(n² log n), which is not O(n²).
//// ANSWER: Yes, you are correct. The recurrence relation T(n) = 2T(n) + n² and T(1) = Θ(1) does not evaluate to T(n) = O(n²), but rather to T(n) = O(n² log n). This is because it falls under Case II of the Master Theorem, with a = 1, b = 3, f(n) = n², and n² = O(nlogb/29/4 log n).

Regarding the original problem in the context:

1. The subproblems are defined correctly as x(i, f) = minimum total difficulty for playing notes ti, ti+1,..., tn−1 starting with finger f on note ti. For 0 < i < n and 1 < f <F.
2. The relation between subproblems is also correct: x(i, f) = min{x(i + 1, f') + d(ti, f, ti+1, f') | 1 ≤ f' < F}.
3. The topological order is decreasing i (any f order).
4. The base case is correct: x(n − 1, f) = 0 (no transitions).
5. The original problem remains the same: to assign fingers to notes to minimize total difficulty.
6. To avoid f' ∈ {4,5}, it seems that {f, f'} should be {3,4} or any other pair of fingers that do not include 4 and 5. However, having {f, f'} = {3,4} might be annoying due to the potential for conflicts in the d(ti, f, ti+1, ?) term.
7. To assign fingers to notes to minimize total difficulty, a table mapping start fingers to optimal solutions for x(i + 1) is needed. This will allow us to fill in the missing information in the recurrence relation: x(i) = min{x(i + 1) + d(ti, f, ti+1, ?) | 1 ≤ f < F}.
8. To find the optimal solution for each subproblem, we will need to explore all possible assignments of fingers to notes and choose the one that minimizes the total difficulty. This can be done using dynamic programming or other optimization algorithms.

QUESTION: Performing an O(1) amortized operation n times on an initially empty data structure takes worst-case O(n) time. //// ANSWER: True. This is the definition of amortization.
//// ANSWER: Yes, you are correct. In the context provided, performing an O(1) amortized operation n times on an initially empty data structure does take worst-case O(n) time. Amortized analysis allows us to consider the average cost of each operation over multiple operations, which can help us analyze algorithms with variable costs that are difficult to analyze using traditional big O notation alone. In this case, the amortized cost per operation is O(1), but since we perform n operations on an initially empty data structure, the worst-case time complexity is O(n).

QUESTION: Given an array A containing n comparable items, sort A using merge sort. While sorting, each item in A is compared with O(log n) other items of A. //// ANSWER: False. As a counter example, during the final merge step between two sorted halves of the array, each of size O(n), a single item from one array may get compared to all the items from the other list.
//// ANSWER: The statement provided is incorrect. During the final merge step in Merge Sort, each item is indeed compared with another item at most once (when they are being merged). However, it's possible that one item from one sorted half may get compared to multiple items from the other half while merging. But the total number of comparisons for sorting an array using Merge Sort remains O(n log n) in the average and worst cases.

QUESTION: Given a binary min-heap storing n items with comparable keys, one can build a Set AVL Tree containing the same items using O(n) comparisons. //// ANSWER: False. If such an algorithm A existed, we would be able to sort an array of comparable items in O(n) time, which would contradict the Ω(n log n) comparison sort lower bound. Specifically, we could build a binary min-heap from the array using O(n) comparisons, use A to construct a Set AVL Tree in O(n) comparisons, and then return its traversal order.
//// ANSWER: The statement provided is incorrect. It suggests that there exists an algorithm A which can build an AVL tree containing the same items as a binary min-heap using O(n) comparisons. However, this contradicts the known lower bound for comparison sorting, which states that any comparison-based sorting algorithm must take at least Ω(n log n) time in the worst case.

In other words, if such an algorithm A existed, it would allow us to sort an array of comparable items in O(n) time, which is faster than the known lower bound for comparison sorting. Therefore, no such algorithm can exist as stated.

QUESTION: Given a directed graph G = (V, E), run breadth-first search from a vertex s ∈ V. While processing a vertex u, if some v ∈ Adj+(u) has already been processed, then G contains a directed cycle. //// ANSWER: False. BFS can't be used to find directed cycles. A counterexample is V = {s, a, b,t} and E = {(s, t), (s, a), (a, b), (b, t)}. Running BFS from s will first process vertices in levels {s}, then {a, t}, then {b}. When processing vertex b, vertex t ∈ Adj+(b) has already been processed, yet G is a DAG.
//// ANSWER: Yes, you are correct. The statement "Given a directed graph G = (V, E), run breadth-first search from a vertex s ∈ V. While processing a vertex u, if some v ∈ Adj+(u) has already been processed, then G contains a directed cycle" is false.

Breadth-First Search (BFS) is an algorithm for traversing or searching the edges of a graph which begins at the tree root (or some arbitrary node if the graph is not a tree) and explores as far as possible along each branch before backtracking. It does not have the ability to detect cycles in the graph, unlike Depth-First Search with stack or Tarjan's Algorithm.

A counterexample for your statement would be a Directed Acyclic Graph (DAG) where vertices and edges are defined as: V = {s, a, b, t} and E = {(s, t), (s, a), (a, b), (b, t)}. Running BFS from s will first process vertices in levels {s}, then {a, t}, then {b}. When processing vertex b, vertex t ∈ Adj+(b) has already been processed, yet G is a DAG.

In this example, the graph contains a directed cycle (t -> s -> a -> b -> t), but BFS does not detect it because it only explores as far as possible along each branch before backtracking and never revisits vertices in the same level again.

QUESTION: Run Bellman-Ford on a weighted graph G = (V, E, w) from a vertex s ∈ V. If there is a witness v ∈ V, i.e., δινι(ς,υ) < διν−1(s, v), then v is on a negative- weight cycle of G. //// ANSWER: False. A witness is only guaranteed to be reachable from a negative- weight cycle; it may not actually be on a negative-weight cycle.
//// ANSWER: Yes, you are correct. In Bellman-Ford's algorithm, a witness vertex v is only guaranteed to be reachable from a negative-weight cycle if one exists in the graph G. However, it does not necessarily mean that v is on a negative-weight cycle itself. The witness vertex indicates that there might be a negative-weight cycle reachable from the starting vertex s, but it doesn't guarantee that the witness vertex is part of that cycle.

QUESTION: Floyd-Warshall and Johnson's Algorithm solve all-pairs shortest paths in the same asymptotic running time when applied to weighted complete graphs, i.e., graphs where every vertex has an edge to every other vertex. //// ANSWER: True. A complete graph is dense, i.e., |E| = Θ(|V|²), so Johnson's algorithm runs in O(|V|² log |V| + |V||E|) = O(|V|³) time, which is the same as Floyd-Warshall.
//// ANSWER: Yes, you are correct. Both Floyd-Warshall and Johnson's Algorithm solve all-pairs shortest paths in a complete graph with the same asymptotic running time. In a complete graph, there is an edge between every pair of vertices, so the number of edges |E| is indeed Θ(|V|^2), where |V| is the number of vertices. Therefore, Johnson's algorithm runs in O(|V|^3) time, which is the same as Floyd-Warshall's running time.

QUESTION: If there is an algorithm to solve 0-1 Knapsack in polynomial time, then there is also an algorithm to solve Subset Sum in polynomial time. //// ANSWER: True. Subset Sum is the special case of 0-1 Knapsack. Specifically, one can (in linear time) convert an instance (A, T) of Subset Sum into an equiv- alent instance of 0-1 Knapsack, with an item i for each integer aż ∈ A having size si = ai and value v₁ = ai, needing to fill a knapsack of size T; and then solve the instance via the polynomial-time algorithm for 0-1 Knapsack.
//// ANSWER: Yes, that is correct. Subset Sum can be reduced to the 0-1 Knapsack problem, which can be solved in polynomial time. This reduction works by creating a new instance of the 0-1 Knapsack problem for each element in the given set A, where the size of each item corresponds to the value of the element and the knapsack capacity is equal to the sum of all elements in the set T. The solution to this new instance will provide a solution to the original Subset Sum problem.

QUESTION: Suppose a decision problem A has a pseudopolynomial-time algorithm to solve A. If P ≠ NP, then A is not solvable in polynomial time. //// ANSWER: False. A problem could have a pseudopolynomial-time algorithm and a polynomial-time algorithm. In fact, any polynomial-time algorithm is also a pseudopolynomial-time algorithm!
//// ANSWER: Yes, you are correct. If a decision problem A has both a pseudopolynomial-time algorithm and a polynomial-time algorithm, then it does not mean that the problem cannot be solved in polynomial time. In fact, any polynomial-time algorithm is also a pseudopolynomial-time algorithm because the running time of a polynomial-time algorithm can always be expressed as a polynomial function of the size of the input and the values of the variables involved in the problem instance. However, it's important to note that having a pseudopolynomial-time algorithm does not necessarily imply that a problem belongs to P (the class of problems solvable in polynomial time), since there could still be an NP-complete or NP-hard problem with a pseudopolynomial-time algorithm.

QUESTION: Given a connected undirected weighted graph G = (V, E, w), where every edge has a unique integer weight, describe an O(|E|log |E|)-time algorithm to determine the largest value of k such that Gk is not connected. //// ANSWER: Construct an array A containing the |E| distinct edge weights in G, and sort it in O(|E|log |E|) time, e.g., using merge sort. We will binary search to find k. Specifically, consider an edge weight k' in A (initially the median edge weight), and run a reachability algorithm (e.g., Full-BFS or Full-DFS) to compute the reachability of an arbitrary vertex x ∈ V in O(|E|) time. If exactly V vertices are reachable from x, then Ge is connected and k > k'; recurse on strictly larger values for k'. Otherwise, Gk is not connected, so k ≤ k'; recurse on non-strictly smaller values for k'. By dividing the search range by a constant fraction at each step (i.e., by always choosing the median index weight of the unsearched space), binary search will terminate after O(log |E|) steps, identifying the largest value of k such that Gk is not connected. This algorithm takes O(|E|log |E|) time to sort, and computes reachability of a vertex in O(|E|) time, O(log |E|) times, so this algorithm runs in O(|E| log |E|) time in total.
//// ANSWER: Given the problem statement, to determine the largest value of k such that Gk is not connected, we can use a binary search algorithm with a time complexity of O(|E|log |E|). Here's how it works:

1. Create an array A containing the distinct edge weights in G. Sort this array efficiently using a sorting algorithm like merge sort, which has a time complexity of O(|E|log |E|).
2. Initialize k with the median edge weight from array A.
3. Run a reachability algorithm (e.g., Full-BFS or Full-DFS) to determine if all vertices in V are reachable from an arbitrary vertex x ∈ V. This can be done in O(|E|) time.
4. If all vertices are reachable, then Ge is connected and k > k'; recursively search for a larger value of k. Otherwise, Gk is not connected, so k ≤ k'; recursively search for smaller values of k.
5. By dividing the search range by a constant factor at each step (i.e., always choosing the median index weight of the unsearched space), binary search will terminate after O(log |E|) steps, identifying the largest value of k such that Gk is not connected.
6. The total time complexity of this algorithm is O(|E|log |E|), as it takes O(|E|log |E|) to sort and O(|E|) time for each reachability check, which occurs log |E| times.

QUESTION: Describe an O(nd)-time algorithm to determine whether it is possible for Dal and Sean to drive from Yew Nork to Fan Sancrisco in at most d days, spending at most b on expenses along the way. //// ANSWER: Let C = {Co, . . ., Cn-1}, and let cs denote Yew Nork and let ct denote Fan Sancrisco. Construct a graph G with: • a vertex (ci, d') for each city c₁ ∈ C and day d' ∈ {0, . . ., d}, representing staying the night in city c₁ on the night before day d'; and • a directed weighted edge ((ci, d'), (cj, d' + 1)) with weight g(ci, Cj) + h(cj) for each city ci ∈ C, cj ∈ Li and d' ∈ {0, . . ., d – 1}. Then the weight of any path in G from vertex (cs,0) to any vertex (ct, d') for d' ∈ {0,...,d} corresponds to the expenses incurred along a driving route from Yew Nork to Fan Sancrisco in at most d days (assuming they stay the night upon reaching ct; other assumptions are also okay). G is acyclic, since each edge always connects a vertex from a smaller day to a larger day, so run DAG Relaxation to compute single-source shortest paths from (cs, 0) in G. Then return whether d((cs, 0), (Ct, d')) ≤ b for any d' ∈ {0,...d}. G has O(nd) vertices and O(nd) edges (since |Li| ≤ 10 for all i∈ {0,...,n – 1}), so DAG relaxation runs in O(nd) time and checking all destination values takes O(d) time, leading to O(nd) time in total.
//// ANSWER: The provided answer describes an O(nd)-time algorithm to determine whether it is possible for Dal and Sean to drive from Yew Nork to Fan Sancrisco in at most d days, spending at most b on expenses along the way. Here's a summary of the steps:

1. Create a graph G with vertices (ci, d') representing staying the night in city c₁ ∈ C on the night before day d'; and edges ((ci, d'), (cj, d' + 1)) with weight g(ci, cj) + h(cj), where ci is a city, cj is a city in its list of neighbors Li, and d' is any day from 0 to d - 1.

2. Run the DAG Relaxation algorithm on graph G starting from vertex (cs,0). This will compute the shortest paths from (cs, 0) to all other vertices in the graph.

3. Check if there exists a path from (cs, 0) to any vertex (ct, d') for d' ∈ {0,...,d} with weight ≤ b. If such a path is found, it means that it is possible for Dal and Sean to drive from Yew Nork to Fan Sancrisco in at most d days and spend at most b on expenses along the way.

The graph has O(nd) vertices and O(nd) edges (since |Li| ≤ 10 for all i∈ {0,...,n – 1}), so DAG relaxation runs in O(nd) time and checking all destination values takes O(d) time, leading to a total running time of O(nd).

QUESTION: Given G, c, a red vertex s ∈ Vr, and a blue vertex t ∈ V♭, describe an O(|V|)-time algorithm to compute d(s, t), the minimum weight of any path from s to t. //// ANSWER: Any path from s to t is a path through edges in Er, followed by a path through edges in Eg, followed by a (possibly empty) path through edges in Eb. So we compute minimum weight distances in G incrementally, first using edges in Er, then using edges in Eg, then edges in Eb. Step 1: Construct unweighted graph G' = (V', E') composed of the edges E' = Er and the vertices appearing in those edges, specifically V' = U(u,v)∈E, {u, v} (which contains vertices from V and Vg). Run breadth-first search from s in G' to compute unweighted distances. Then the minimum weight distance in G from s to any green vertex in V' ∩ V, is wr times the unweighted distance computed. G' has size O(|V|), so this step takes O(|V|) time. Step 2: Now construct weighted graph G" = (V", E") composed of vertex s with a new directed edge to each green vertex in V' ∩ V, weighted by its distance found in Step 1 (i.e., the minimum weight of any path from s to that vertex), along with weighted edges Eg and the vertices appearing in those edges. All the weights in G" are positive, so run Dijkstra from s in G" to compute minimum weight distances. Then the computed distance to any blue vertex v in V" ∩ V is the minimum weight of any path from s to v in G that traverses only red or green edges. G" has size O(1 + |Vg| + |Eg|) = O(|V|0.99), so this step takes O(|V|0.99 log |V|0.99) = O(|V|) time. Step 3: Now construct a weighted graph G''' = (V''', E''') composed of vertex s with a new directed edge to each blue vertex in V" ∩ V weighted by its distance found in Step 2 (i.e., the minimum weight of any path from s to that vertex), along with weighted edges E♭ and the vertices appearing in those edges. Weights in G'' may be positive or negative, so run Bellman-Ford from s in G'' to compute weighted minimum weight distances. Then the computed distance to t is the minimum weight of any path from s to t in G, as desired. G'' has size O(1 + |V6| + |E6|) = O(√VI), so this step takes O(√[V]√[V]) = 0(|V|) time, leading to O(|V|) time in total.
//// ANSWER: The given algorithm computes the minimum weight of any path from a red vertex s to a blue vertex t in a graph G with red edges Er, green edges Eg, and blue edges Eb. The algorithm runs in O(|V|) time as follows:

1. Construct an unweighted graph G' containing only the edges Er and vertices appearing in those edges. Run breadth-first search (BFS) from s in G' to compute unweighted distances. This takes O(|V|) time since G' has size O(|V|).

2. Construct a weighted graph G" with vertex s connected to each green vertex in V' ∩ V, along with all the edges Eg and vertices appearing in those edges. Run Dijkstra's algorithm from s in G" to compute minimum weight distances. This takes O(|V|0.99 log |V|0.99) = O(|V|) time since G" has size O(1 + |Vg| + |Eg|) = O(|V|0.99).

3. Construct a weighted graph G''' with vertex s connected to each blue vertex in V" ∩ V, along with all the edges Eb and vertices appearing in those edges. Run Bellman-Ford's algorithm from s in G""" to compute weighted minimum weight distances. This takes O(√[V]√[V]) = O(|V|) time since G"" has size O(1 + |V6| + |E6|) = O(√VI).

The total running time is O(|V|), as desired.

QUESTION: Describe an O(n²m)-time algorithm to count the number of (m, k)-separated subsets of S. //// ANSWER: 1. Subproblems • First sort the integers in S increasing into array A in O(n log n) time, e.g., via merge sort • where A = (ao,..., an-1) • x(i, j): the number of (j, k)-separated subsets of suffix A[i :] • for i ∈ {0, . . ., n} and j ∈ {0, ...,m} 2. Relate • Sum the number of (j, k)-separated subsets using A[i] with the ones that do not use A[i] • If A[i] < j is used: – Then no integer in A[i :] smaller than A[i] + k may be used – Let f (i) be the smallest index greater than i such that A[f(i)] – A[i] ≥ k – Then recrusively count x(f(i), j – A[i]) • Otherwise, A[i] is not used and we can recursively count x(i + 1, j) Σ{ x(f(i), j - A[i]) if A[i] < j, x(i + 1, j) always } 3. Topo 3. Topo • Subproblem x(i, j) only depends on strictly larger i, so acyclic 4. Base • x(n, 0) = 1, the empty subset can always be acheived • x(n, j) = 0 for j > 0, empty sets cannot sum to a positive number 5. Original • x(0, m), the number of (m, k)-separated subsets of A 6. Time • # subproblems: (n + 1)(m + 1) = O(nm) • Work per subproblem: O(n) to find f(i) by linear scan • O(n²m) time in total • (Note that it is possible to compute f(i) in O(log n) time via binary search, or in amor- tized O(1) time from f (i - 1), but these optimizations are not necessariy for full points.)
//// ANSWER: The provided algorithm is an O(n²m)-time dynamic programming solution to count the number of (m, k)-separated subsets of a given set S. Here's a summary of the steps:

1. Sort the integers in S in increasing order into array A in O(n log n) time.
2. Define x(i, j) as the number of (j, k)-separated subsets of suffix A[i :]. For i ∈ {0, ..., n} and j ∈ {0, ...,m}.
3. Recursively compute x(i, j) based on the relation:
   - If A[i] < j is used, then no integer in A[i :] smaller than A[i] + k may be used. Let f(i) be the smallest index greater than i such that A[f(i)] – A[i] ≥ k – Recursively count x(f(i), j – A[i]).
   - Otherwise, if A[i] is not used, recursively count x(i + 1, j).
4. The subproblem x(i, j) only depends on strictly larger i, making the problem acyclic.
5. Set base conditions: x(n, 0) = 1 (the empty subset can always be achieved), and x(n, j) = 0 for j > 0 (empty sets cannot sum to a positive number).
6. The original problem is x(0, m) which gives the number of (m, k)-separated subsets of A.
7. The time complexity of this algorithm is O(nm) for the number of subproblems and O(n) work per subproblem, resulting in an overall O(n²m) time complexity.

QUESTION: Given Ted's guest information, describe an O(n³)-time algorithm to determine a respectful seating order that minimizes the sum of mutual hatred between pairs of guests seated next to each other. //// ANSWER: 1. Subproblems • Sort the guests increasing by favor in O(n log n) time into F = (fo, ..., f2n-1) • Any partition of F into two length-n subsequences corresponds to a respectful seating • x(i, JL, JR, N₁): minimum total hatred of adjacent guests possible by respectfully seating the n i guests from suffix F[i:] next to the Queen, with n₁ guests to the left and NR = (n - i) - n₁ guests to the right, where guest j₁ < i has already been seated n₁ +1 places to the left, and guest jR < i has already been seated NR + 1 places to the right. • for i∈ {0, . . ., 2n}, JL, JR ∈ {−1, ..., 2n − 1} and n₁ ∈ {0,...,n} where either j₁ = i − 1 or j₨ = i − 1 • Let d(−1,i) = d(i, −1) = 0 for all i ∈ {0, . . ., 2n – 1} (no hatred at the end of table) 2. Relate • Guess whether guest i is seated on the right or left • Sitting next to j₁ costs hatred d(i, j₁); sitting next to JR costs hatred d(i, jr) x(i, JL, JR, NL) =min d(i, j₁) + x(i + 1, i, jR, NL − 1) if n₁ > 0, d(i, jr) + x(i + 1, jŁ, i, n₁) if (n - i) - n₁ > 0 3. Topo: Subproblem x(i, JL, JR, NL) only depends on strictly larger i, so acyclic 4. Base: x(2n, JL, JR, 0) = 0 for all jL, JR ∈ {0, . . ., 2n} (no hatred if no guests) 5. Original: x(0, -1, -1, n), min hatred of adjacent guests by respectfully seating all guests 6. Time • # subproblems: though there are four parameters, there are only O(n³) subproblems for which either j₁ = i 1 or j = i -1 • Work per subproblem: O(1), so O(n³) time in total
//// ANSWER: To determine a respectful seating order that minimizes the sum of mutual hatred between pairs of guests, you can use an algorithm with a time complexity of O(n^3). Here's how it works:

1. Subproblems:
   - Sort the guests in increasing order by favor into F = (fo, ..., f2n-1) in O(n log n) time.
   - Any partition of F into two length-n subsequences corresponds to a respectful seating.
   - Define x(i, JL, JR, N₁): minimum total hatred of adjacent guests possible by respectfully seating the n i guests from suffix F[i:] next to the Queen, with n₁ guests to the left and NR = (n - i) - n₁ guests to the right. Guest j₁ < i has already been seated n₁ +1 places to the left, and guest jR < i has already been seated NR + 1 places to the right.
   - Let d(−1,i) = d(i, −1) = 0 for all i ∈ {0, . . ., 2n – 1} (no hatred at the end of the table)

2. Relate:
   - Guess whether guest i is seated on the right or left
   - Sitting next to j₁ costs hatred d(i, j₁); sitting next to JR costs hatred d(i, jr)
   x(i, JL, JR, NL) = min{d(i, j₁) + x(i + 1, i, jR, NL − 1) if n₁ > 0, d(i, jr) + x(i + 1, jŁ, i, n₁) if (n - i) - n₁ > 0}

3. Topo:
   - Subproblem x(i, JL, JR, NL) only depends on strictly larger i, so the problem is acyclic

4. Base:
   - x(2n, JL, JR, 0) = 0 for all jL, JR ∈ {0, . . ., 2n} (no hatred if no guests)

5. Original:
   - x(0, -1, -1, n), min hatred of adjacent guests by respectfully seating all guests

6. Time:
   - # subproblems: though there are four parameters, there are only O(n³) subproblems for which either j₁ = i 1 or j = i -1
   - Work per subproblem: O(1), so O(n³) time in total

QUESTION: Given an array A containing n distinct positive integers, describe an O(n²)-time algorithm to determine whether four integers from A form a Pythagorean Quad, where integers from A may appear more than once in the Quad. State whether your running time is worst-case, expected, and/or amortized. //// ANSWER: First, we observe that it suffices to find (a, b, c, d) such that a² + b² = d² – c². Let P be the set of n² ordered pairs of integers from A, where integers in A may be repeated in a pair. Construct an empty hash table H, and for each pair (a, b) ∈ P, compute and insert value a² + b² into H. Then for each pair (c, d) ∈ P, compute and lookup value d² – c² in H. If the value is in H, then some a² + b² equals some d² – c², so return that a Pythagorean Quad exists. Otherwise, if no d² - c² exists in H, then return that a Pythagorean Quad does not exist. Each a² + b² or d² – c² value takes constant time to compute, so computing them all takes worst-case O(n²) time, while inserting them into or looking them up in the hash table takes expected O(n²) time, so this algorithm runs expected in O(n²) time in total.
//// ANSWER: The given algorithm is an O(n^2)-time algorithm to determine whether four integers from a given array A form a Pythagorean Quad. Here's the description of the algorithm:

1. Create a set P of n² ordered pairs of integers from A, where each integer in A may appear multiple times within the pairs.
2. Initialize an empty hash table H.
3. For each pair (a, b) ∈ P, compute and insert the value a^2 + b^2 into the hash table H.
4. For each pair (c, d) ∈ P, compute the value d^2 - c^2 and look it up in the hash table H. If the value is found, then return that a Pythagorean Quad exists with the integers a, b, c, and d.
5. If no d² - c² exists in H, then return that a Pythagorean Quad does not exist.

The running time of this algorithm is:
- Worst-case: O(n^2) - The worst-case scenario occurs when every pair (a, b) and (c, d) needs to be computed and looked up in the hash table.
- Expected: O(n^2) - The expected running time is the same as the worst-case running time because the number of comparisons made during each lookup operation is not affected by the distribution of the data.
- Amortized: Not applicable - This algorithm does not have any amortized analysis since it doesn't involve any dynamic data structures with amortized costs like linked lists or heaps.

QUESTION: Describe a database to store animal sightings, supporting the following four operations, where n is the number of sightings stored in the database at the time of the operation. State whether your running times are worst-case, expected, and/or amortized. initialize() add_sighting(s, i) remove_oldest() is_common(s) //// ANSWER: To implement the database, maintain the following data structures: • A hash table H mapping each species s to a Set AVL tree T • Each Set AVL Tree T』 stores pairs (i, ci) of zone numbers i and the count ci representing the number of sightings of species s in zone i, keyed by zone number. • Augment each node x in each T, by the maximum number of sightings x.m of any zone in the subtree of x. x.m can be maintained in O(1) time from the augmentations of x's children, specifically x.m = max{x.left.m,x.key,x.right.m}. • A doubly-linked list L of all current sightings (s, i) in the order in which they were added to the database (oldest at the front). To implement initialize(), initialize an empty H and empty L in worst-case O(1) time. To implement add_sighting(s, i), lookup s in H to find T, in expected O(1) time (if s does not exist in H, insert s mapping to an empty T, in expected amortized O(1) time). Then find zone i in Ts. If zone i is not in Ts, insert (i, 1) into Ts. Otherwise, i is in Ts, so remove (i, ci) from Ts and reinsert (i, c₁ + 1). It takes worst-case O(log n) time to remove or insert items from T, while maintaining augmentations (since at most n sightings could exist for species s). Lastly, Insert (s, i) to the back of L in worst-case O(1) time. Thus this operation takes O(log n) expected amortized time, and maintains the invariants of the database directly. To implement remove_oldest (), remove the oldest pair (s, i) from the front of L in worst-case O(1) time. Lookup s in H to find T, in expected O(1) time; then lookup i in Ts and decrease Ci by one. If ci is decreased to zero, remove i from Ts. If Ts becomes empty, remove s from H in expected amortized O(1) time. This operation takes O(log n) expected amortized time, and maintains the invariants of the database directly. To implement is_common(s), simply lookup s in H and return whether s is in H and the stored max at the root of T is 100 or greater in expected O(1) time. This operation is correct based on the invariants of the data structure.
//// ANSWER: To implement the animal sightings database, you can use the following data structures:

- A hash table H mapping each species s to an AVL tree T.
- Each AVL Tree T stores pairs (i, ci) of zone numbers i and the count ci representing the number of sightings of species s in zone i, keyed by zone number.
- Augment each node x in each T with the maximum number of sightings x.m of any zone in the subtree of x. x.m can be maintained in O(1) time from the augmentations of x's children.
- A doubly-linked list L of all current sightings (s, i) in the order they were added to the database (oldest at the front).

Here are the operation implementations:

1. initialize(): Initialize an empty H and empty L in worst-case O(1) time.
2. add_sighting(s, i): Lookup s in H to find T, in expected O(1) time (if s does not exist in H, insert s mapping to an empty T, in expected amortized O(1) time). Then find zone i in Ts. If zone i is not in Ts, insert (i, 1) into Ts. Otherwise, remove (i, ci) from Ts and reinsert (i, c₁ + 1). It takes worst-case O(log n) time to remove or insert items from T, while maintaining augmentations (since at most n sightings could exist for species s). Lastly, Insert (s, i) to the back of L in worst-case O(1) time. Thus this operation takes O(log n) expected amortized time, and maintains the invariants of the database directly.
3. remove_oldest(): Remove the oldest pair (s, i) from the front of L in worst-case O(1) time. Lookup s in H to find T, in expected O(1) time; then lookup i in Ts and decrease Ci by one. If ci is decreased to zero, remove i from Ts. If Ts becomes empty, remove s from H in expected amortized O(1) time. This operation takes O(log n) expected amortized time, and maintains the invariants of the database directly.
4. is_common(s): Simply lookup s in H and return whether s is in H and the stored max at the root of T is 100 or greater in expected O(1) time. This operation is correct based on the invariants of the data structure.

QUESTION: Given a k-even-mixed array A containing n distinct integers for k = [n/lgn], describe an O(n)-time algorithm to sort A. //// ANSWER: Scan through A and put all even integers in order into an array AE and all odd integers in order into an array Ao (where |AE| = k and |Ao| = n k). Ao is sorted by definition, and we can sort AĘ in O(k logk) = O((n/lgn) log(n/lgn)) = O(n) time, e.g., via merge sort. Then we can merge sorted AE and Ao back into A in O(n) time using the merge step of merge sort, using O(n) time in total.
//// ANSWER: The problem you've described involves finding an efficient algorithm to sort a k-even-mixed array A containing n distinct integers for k = [n/lgn]. The proposed solution is as follows:

1. Divide the array A into two subarrays: AE (containing all even numbers) and Ao (containing all odd numbers). Since k is a constant, this step takes O(n) time.
2. Sort the Ao subarray using any sorting algorithm. Since it's already sorted by definition, the best-case scenario is that no operations are required, taking O(1) time. In the worst case, we can use merge sort to sort Ao in O(n log n) time, which simplifies to O(n) for this specific problem since k is a constant.
3. Sort the AE subarray using any sorting algorithm. Since k = [n/lgn], we can use an efficient sorting algorithm such as merge sort that has a running time of O((n/k) log(n/k)) or O(n log k). However, since k is also a constant, this simplifies to O(n) time.
4. Merge the sorted AE and Ao subarrays back into array A using the merge step of merge sort. This takes O(n) time.

In total, the proposed algorithm sorts the given k-even-mixed array in O(n) time.

QUESTION: Let A be an array of n pairs of positive integers (xi, Yi) with Xi, Yi < n² for all i ∈ {0, . . ., n − 1}. The power of pair (x, y) is the integer x + nº. Describe an O(n)-time algorithm to sort the pairs in A increasing by power. //// ANSWER: First note that x < ny for any integer y > 1 and for any x ∈ {0, . . ., n²−1}. Scan through A and put all pairs having y = 1 into array A₁, and all other pairs into array A2. Sort A₁ directly by computing and comparing their respective powers x+n. Since these values are bounded above by O(n²), sort A₁ in O(n) time using Radix sort. To sort A2, use tuple sort, sorting first by x values and then by y values (since power is more sensitive to changes in y). Since the x and y values are both bounded above by O(n²), we can use Radix sort for tuple sort's stable sorting algorithm to sort A2 in O(n) time. Then merge A₁ and A2 back into A in O(n) time using the merge step of merge sort, using O(n) time in total.
//// ANSWER: The provided answer describes an O(n)-time algorithm to sort the pairs in array A increasing by power (x + n^y). Here's a breakdown of the steps:

1. Divide the input array A into two subarrays, A₁ and A₂, based on the y-value:
   - A₁ contains all pairs with y = 1
   - A₂ contains all other pairs

2. Sort A₁ directly by their powers (x + n) using Radix sort in O(n) time. Since the values are bounded above by O(n²), this can be done efficiently.

3. To sort A₂, use tuple sort to sort first by x-values and then by y-values. Tuple sort is a stable sorting algorithm that sorts based on multiple attributes of each element in the array. Since both the x and y values are bounded above by O(n²), we can use Radix sort for tuple sort's stable sorting algorithm to sort A₂ in O(n) time.

4. Merge A₁ and A₂ back into A using the merge step of merge sort, which takes O(n) time in total. This results in a sorted array A with pairs ordered by their power (x + n^y).

QUESTION: Given an n × m array representing an image, describe an O(nm)-time algorithm to count the number of blobs in the image. //// ANSWER: Construct a graph G with a vertex per white pixel, with an undirected edge between two vertices if the pixels associated with them are both white and share an edge of the grid. This graph has size at most O(nm) vertices and at most O(nm) edges (as pixels share edges with at most four other pixels), so can be constructed in O(nm) time. Each connected component of this graph corresponds to a blob, so run Full-BFS or Full-DFS to count the number of connected components in G in O(nm) time.
//// ANSWER: To adapt the given algorithm to solve the problem from the context, we can create a similar graph but with vertices representing the start positions (notes) and edges connecting two notes if they can be played by the same finger without transition. The weight of each edge would represent the difficulty of transitioning between the two notes.

  1. Subproblems: x(i, f) = minimum total difficulty for playing notes i, i+1,..., n-1 starting with finger f on note i. For 0 < i < n and 1 < f < F.
  2. Relate: Guess next finger assignment f' for note i+1. x(i, f) = min{x(i + 1, f') + d(i, f, i+1, f') | 1 ≤ f' < F}.
  3. Topological order: Decreasing i (any f order).
  4. Base: x(n-1, f) = 0 (no transitions).

To avoid the issue of {f, f'} = {3,4} being annoying and to assign fingers to notes to minimize total difficulty, we can modify the algorithm as follows:

  1. Subproblems: x(i, f) = minimum total difficulty for playing notes i, i+1,..., n-1 starting with finger f on note i. For 0 < i < n and f ∈ {1,2}.
  2. Relate: Guess first finger assignment f for note i. x(i) = min{x(i + 1) + d(i, f, i+1, ?) | f ∈ {1,2}}.
  3. Topological order: Decreasing i (any f order).
  4. Base: x(n-1) = 0 (no transitions).

This modified algorithm ensures that we only consider fingers 1 and 2 for the starting finger of each subproblem, avoiding the issue of {f, f'} = {3,4}. The table mapping start fingers to optimal solutions for x(i + 1) is still needed to fill in the missing information.

The time complexity remains O(nF^2), as there are n subproblems and F work per subproblem, with no dependence on the number of different notes!

QUESTION: Given a connected undirected graph G = (V, E) with strictly positive weights w : E → Z+ where |E| = |V|, describe an O(|V|)-time algorithm to determine a path from vertex s to vertex t of minimum weight. //// ANSWER: Given two vertices in a weighted tree containing only positive weight edges, there is a unique simple path between them which is also the minimum weight path. A depth-first search from any source vertex s in the tree results in a directed DFS tree in O(|V|) time (since |E| = |V| – 1). Then relaxing edges in topological sort order of the directed DFS tree computes minimum weight paths from s in O(|V|) time. Since G has one cycle, our strategy will be to break the cycle by removing an edge, and then compute the minimum weight path from s to t in the resultant tree. First, we find the vertex v closest to s on the cycle by running depth-first search from s in O(|V|) time (since |E| = |V|). One edge e₁ of the cycle will not be in the tree returned by DFS (a back edge to v), with the other edge of the cycle incident to v being a single outgoing DFS tree edge 62. If s is on the cycle, v = s; otherwise the unique path from s to v does not contain e1 or e2. A shortest path from s to t cannot traverse both edges e₁ and e2, or else the path would visit v at least twice, traversing a cycle of positive weight. Removing either e1 or e2 results in a tree, at least one of which contains the minimum weight path from s to t. Thus, find the minimum weight path from s to t in each tree using the algorithm described above, returning the minimum of the two in O(|V|) time.
//// ANSWER: In this problem, we are given a connected undirected graph G = (V, E) with strictly positive weights w : E → Z+ where |E| = |V|. The goal is to find a path from vertex s to vertex t of minimum weight in O(|V|) time.

The algorithm proposed works as follows:

1. First, we find the vertex v closest to s on the cycle by running depth-first search (DFS) from s in O(|V|) time. This is because |E| = |V|, and each edge is incident to exactly two vertices.

2. One edge e₁ of the cycle will not be in the tree returned by DFS (a back edge to v), with the other edge of the cycle incident to v being a single outgoing DFS tree edge 62. If s is on the cycle, v = s; otherwise the unique path from s to v does not contain e1 or e2.

3. A shortest path from s to t cannot traverse both edges e₁ and e₂, or else the path would visit v at least twice, traversing a cycle of positive weight. Removing either e1 or e2 results in a tree, at least one of which contains the minimum weight path from s to t.

4. Then we find the minimum weight path from s to t in each tree using the algorithm described above, returning the minimum of the two in O(|V|) time. This is because the minimum weight path in a tree can be found in linear time using Dijkstra's or Bellman-Ford algorithms, but since we only have two trees (one for each removal), this step takes O(1) time.

This algorithm ensures that we find the minimum weight path from s to t in O(|V|) time.

QUESTION: Describe an O(nlog n)-time algorithm to find the shortest driving route from the power plant back home that avoids driving within driving distance k of a doughnut shop (or determine no such route exists). //// ANSWER: Construct a graph G with a vertex for each of the n city locations, and an undirected edge between two locations if there is a road connecting them, with each edge weighted by the positive length of its corresponding road. The degree of each vertex is bounded by a constant (i.e., 5), so the number of edges in G is O(n). First, we identify vertices that are within driving distance k of a doughnut shop location: create an auxiliary vertex x with a 0-weight outgoing edge from x to every doughnut shop location, and run Dijkstra from x. Remove every vertex from the graph whose shortest path from x is less than or equal to k, resulting in graph G' ⊂ G. If either p or h are not in G', then no route exists. Otherwise, run Dijkstra from p in G'. If no path exists to h, then no valid route exists. Otherwise, Dijkstra finds a shortest path from p to h, so return it (via parent pointers). This algorithm runs Dijkstra twice. Since the size of either graph is O(|V|), Dijkstra runs in O(|V| log |V|) = O(nlog n) time (e.g. using a binary heap to implement a priority queue).
//// ANSWER: The provided algorithm finds the shortest driving route from a power plant (p) back home (h) that avoids driving within driving distance k of a doughnut shop, or determines no such route exists. The algorithm constructs a graph G with a vertex for each city location and an undirected edge between two locations if there is a road connecting them, with each edge weighted by the positive length of its corresponding road.

The algorithm first identifies vertices that are within driving distance k of a doughnut shop location by creating an auxiliary vertex x with a 0-weight outgoing edge from x to every doughnut shop location and running Dijkstra from x. It then removes every vertex from the graph whose shortest path from x is less than or equal to k, resulting in graph G'. If either p or h are not in G', then no route exists.

If both p and h are in G', the algorithm runs Dijkstra from p in G' to find a shortest path to h. If no path exists to h, then no valid route exists. Otherwise, Dijkstra finds a shortest path from p to h, so it returns this path (via parent pointers).

The time complexity of the algorithm is O(nlog n) because the size of either graph is O(|V|), and Dijkstra runs in O(|V| log |V|) = O(nlog n) time using a binary heap to implement a priority queue.

QUESTION: Given directed graph G = (V, E) having arbitrary edge weights w : E → Z and two vertices s, t ∈ V, describe an O(|V|³)-time algorithm to find the minimum weight of any path from s to t containing at least |V| edges. //// ANSWER: Our strategy will compute intermediate values for each vertex v ∈ V: 1. the minimum weight w₁ (v) of any path from s to v using exactly |V| edges, and then 2. the minimum weight w₂(v) of any path from v to t using any number of edges. First, to compute (1), we make a duplicated graph similar to Bellman-Ford, but without edges corresponding to remaining at a vertex. Specifically, construct a graph G₁ with • |V| + 1 vertices for each vertex v ∈ V: vertex vk for k ∈ {0, . . ., |V|} representing reaching v from s along a path containing k edges; and • |V| edges for each edge (u, v) ∈ E: edge (uk−1, Uk) of the same weight for k ∈ {1, ...,|V|}. Now a path in G₁ from so to υν for any v ∈ V corresponds to a path from s to v in G through exactly |V| edges. So solve SSSPs in G₁ from so to compute the minimum weight of paths to each vertex traversing exactly |V| edges. This graph is acyclic, and has size O(|V|(|V| + |E|)) = O(|V|³), so we can solve SSSP on G₁ via DAG relaxation in O(|V|³) time. Second, to compute (2), we make a new graph G2 from G where every edge is reversed. Then every path to t in G corresponds to a path in G2 from t, so compute SSSPs from t in G2 to find the minimum weight of any path from v to t in G using any number of edges, which can be done in O(|V||E|) = O(|V|³) time using Bellman-Ford. Once computed, finding the minimum sum of w₁(v) + w₂(v) over all vertices v ∈ V will provide the minimum weight of any path from s to t containing at least |V| edges, since every such path can be decomposed into its first |V| edges and then the remainder. This loop takes O(|V|) time, so the algorithm runs in O(|V|³) time in total.
//// ANSWER: The given algorithm is designed to find the minimum weight of any path from a source vertex s to a target vertex t in a directed graph G with arbitrary edge weights, where the path must contain at least |V| edges (where V is the number of vertices). Here's how it works:

1. First, the algorithm constructs an auxiliary graph G₁ based on Bellman-Ford's single-source shortest-path (SSSP) algorithm but with a few modifications. In this new graph, there are |V| + 1 vertices for each vertex v in the original graph, and |V| edges for each edge (u, v) in the original graph. The graph is acyclic and has size O(|V|^3).

2. The algorithm then solves SSSP in G₁ from the source vertex so to compute the minimum weight of paths to each vertex traversing exactly |V| edges. This can be done in O(|V|^3) time using DAG relaxation.

3. Next, the algorithm constructs another graph G₂ by reversing every edge in the original graph. Then every path to t in the original graph corresponds to a path in G₂ from t. The algorithm computes SSSPs from t in G₂ to find the minimum weight of any path from v to t in the original graph using any number of edges, which can be done in O(|V|^3) time using Bellman-Ford.

4. Finally, the algorithm finds the minimum sum of w₁(v) + w₂(v) over all vertices v in V, where w₁(v) is the minimum weight of any path from s to v using exactly |V| edges and w₂(v) is the minimum weight of any path from v to t using any number of edges. This provides the minimum weight of any path from s to t containing at least |V| edges, since every such path can be decomposed into its first |V| edges and then the remainder. The loop takes O(|V|) time, so the algorithm runs in O(|V|^3) time in total.

In summary, the given algorithm is an O(|V|^3)-time algorithm to find the minimum weight of any path from s to t containing at least |V| edges in a directed graph with arbitrary edge weights.

QUESTION: Design a database to help Criminal Seafood maintain its wait list supporting the following operations, each in O(1) time. State whether each operation running time is worst-case, amortized, and/or expected. //// ANSWER: Maintain a doubly-linked list containing customers on the wait list in order, maintaining a pointer to the front of the linked list corresponding to the front of the wait list, and a pointer to the back of the linked list corresponding to the back of the wait list. Also maintain a hash table mapping each customer name to the linked list node containing that customer. To implement $add\_name(x)$, create a new linked list node containing name x and add it to the back of the linked list in worst-case O(1) time. Then add name x to the hash table pointing to the newly created node in amortized expected O(1) time. To implement $remove\_name (x)$, lookup name x in the hash table in and remove the mapped node from the linked list in expected O(1) time. Lastly, to implement $seat ()$, remove the node from the front of the linked list containing name x, remove name x from the hash table, and then return x, in amortized expected O(1) time.
//// ANSWER: The design for the database to help Criminal Seafood maintain its wait list is as follows:

   - Use a doubly-linked list to store customers on the waitlist in order, with pointers to the front and back of the list. This allows for adding and removing elements from either end of the list in O(1) time (worst-case).

   - Maintain a hash table mapping each customer name to the linked list node containing that customer. This allows for efficient lookup of a customer's position on the waitlist in expected O(1) time.

   Here are the implementations for the operations you specified:

   - $add\_name(x)$: Create a new linked list node containing name x and add it to the back of the linked list in worst-case O(1) time. Then, add name x to the hash table pointing to the newly created node in amortized expected O(1) time (since hash table lookup is constant time and adding an element to a hash table has a constant average case).

   - $remove\_name(x)$: Lookup name x in the hash table and remove the mapped node from the linked list in expected O(1) time. This operation is amortized expected O(1), as the lookup and removal are both constant time operations.

   - $seat()$: Remove the node from the front of the linked list containing name x, remove name x from the hash table, and then return x. Since removing a node from the front of a doubly-linked list is O(1) time, this operation is amortized expected O(1).

QUESTION: Describe a database that can store Mether's sensor data and support the following operations, each in worst-case O(log n) time where n is the number of measurements in the database at the time of the operation. //// ANSWER: Maintain a Set AVL tree L storing distinct measurement latitudes, where each latitude l maps to a rainfall Set AVL tree R(l) containing all measurement triples with latitude l, keyed by time. We only store nodes associated with measurements, so the height of each Set AVL tree is bounded by O(log n). For each rainfall tree, augment each node p with the maximum rainfall p.m of any measurement within p's subtree. This augmentation can be maintained in constant time at a node p by taking the maximum of the rainfall at p and the augmented maximums of p's left and right children (if they exist); thus this augmentation can be maintained without effecting the asymptotic running time of standard AVL tree operations. To implement $record\_data (r, l, t)$, search L for latitude l in worst-case O(log n) time. If l does not exist in L, insert a new node corresponding to l mapping to a new empty rainfall Set AVL tree, also in O(log n) time. In either case, insert the measurement triple to R(l), for a total running time of worst-case O(log n). To implement $peak\_rainfall (l,t)$, search L for latitude l in worst-case O(logn) time. If l does not exist, return zero. Otherwise, perform a one-sided range query on R(l) to find the peak rainfall at latitude l since time t. Specifically, let $peak(v, t)$ be the maximum rainfall of any measurement in node v's subtree measured at time > t (or zero if v is not a node): $peak(v, t) = \begin{cases} max \{v.item.r, v.right.m, peak(v.left, t)\} & \text{if } v.t > t \\ peak(v.right, t) & \text{if } v.t < t \end{cases}$ Then peak rainfall is simply peak(v,t) with v being the root of the tree, which can be computed using at most O(log n) recursive calls. So this operation runs in worst-case O(log n) time.
//// ANSWER: The described database is an AVL tree structure that stores sensor data measurements with latitude and time, allowing for efficient lookup and query operations. The AVL trees are organized such that each distinct latitude maps to a rainfall AVL tree containing all measurement triples with that latitude, keyed by time. Each node in the rainfall AVL tree contains the maximum rainfall of any measurement within its subtree.

The database supports two main operations: `record_data(r, l, t)` and `peak_rainfall(l,t)`. The `record_data` operation inserts a new measurement triple into the appropriate rainfall AVL tree for the given latitude, with worst-case time complexity of O(log n). The `peak_rainfall` operation finds the peak rainfall at a given latitude and time since a specified time, with worst-case time complexity of O(log n) due to the recursive nature of the range query.

The database design ensures that the number of measurements does not affect the time complexity of operations, as the height of each AVL tree is bounded by O(log n). This makes it an efficient solution for handling large amounts of sensor data and supporting real-time queries.

QUESTION: Indicate whether the given running times of each of problems (3), (4), and (5) are polynomial or pseudopolynomial by circling the appropriate word below. One can answer this question without actually solving problems (3), (4), and (5). //// ANSWER: Problem 3: Number Scrabble Polynomial
//// ANSWER: Problem 3: Pseudopolynomial. The running time of the problem is given as Θ(n·F), where n is the number of notes and F is the maximum number of fingers, which can be quite large in comparison to n. This makes the running time a function that grows polynomially with n but exponentially with F, making it pseudopolynomial rather than polynomial.

QUESTION: Indicate whether the given running times of each of problems (3), (4), and (5) are polynomial or pseudopolynomial by circling the appropriate word below. One can answer this question without actually solving problems (3), (4), and (5). //// ANSWER: Problem 4: Limited-Unlimited Pseudopolynomial
//// ANSWER: Problem 4 is pseudopolynomial because the running time of the algorithm depends on the maximum value of F, which can be unbounded and grows with the input size. In this case, the running time is expressed as Θ(F^2), making it a pseudopolynomial problem.

QUESTION: Indicate whether the given running times of each of problems (3), (4), and (5) are polynomial or pseudopolynomial by circling the appropriate word below. One can answer this question without actually solving problems (3), (4), and (5). //// ANSWER: Problem 5: Office Hour Optimization Polynomial
//// ANSWER: Problem 5: Office Hour Optimization is pseudopolynomial. The running time of the algorithm for this problem is given as Θ(η· F²), where η is the number of different notes and F is the maximum number of fingers used to play a note. Since the value of η is not specified in the context, it cannot be determined definitively whether it is a constant or grows with the size of the input. However, since F² can grow exponentially with the size of the input, the running time is pseudopolynomial.

QUESTION: Given T and P, describe an O(n + m)-time algorithm to return a gameplay of maximum total value. //// ANSWER: To solve this problem, it would be useful to be able to check whether a particular array of at most 10 positive integers is a playable word. Construct an empty hash table D and insert each $p_i$ for $i \in {0, . . ., m – 1}$ into D, mapping to its value $v(p_i)$. Each hash table insertion takes expected constant time (as each $p_i$ has constant size), so constructing D takes expected O(m) time. Now we solve the problem via dynamic programming. 1. Subproblems • x(i): the maximum total value of any gameplay on suffix T[i :] for $i \in {0, . . ., n}$ 2. Relate • Left-most playable word either starts with $t_i$ or it does not • If playable word starts with $t_i$, word may have any length in {1, . . ., 10} (Guess!) • $x(i) = max\{x(i+1)}\cup\{D[T[i : i + j]] + x(i + j)\ 3. Topo • x(i) only depends on subproblems with strictly larger i, so acyclic 4. Base • x(n) = 0 (empty gameplay admits no value) 5. Original • Solve subproblems via recursive top down or iterative bottom up • x(0) is the maximum value of any gameplay on T • Store parent pointers to reconstruct an optimal gameplay 6. Time • # subproblems: n + 1 = O(n) • Work per subproblem: expected O(1) • Together with hash table construction, yields expected O(n + m) time • (See scratch S2 for common mistakes)
//// ANSWER: To find the gameplay of maximum total value in a given sequence T and playable words P, you can use an O(n + m)-time dynamic programming algorithm. Here's how to implement it:

1. Initialize an empty hash table D that maps each word $p_i$ to its value $v(p_i)$. This step takes expected O(m) time.
2. Define subproblems as follows: x(i) represents the maximum total value of any gameplay on suffix T[i :] for i in {0, ..., n}.
3. To solve each subproblem, consider two possibilities: either the left-most playable word starts with $t_i$, or it does not. If a playable word starts with $t_i$, its length can be between 1 and 10 (assuming that's the maximum number of letters in any playable word).
4. In this case, x(i) is the maximum of x(i+1) and the sum of D[T[i : i + j]] (the value of the playable word starting at position i and ending at position i + j) plus x(i + j) (the maximum total value of any gameplay on suffix T[i + j :]).
5. The subproblem x(n) is the base case, with a value of 0 since an empty gameplay admits no value.
6. Solve the subproblems either recursively top-down or iteratively bottom-up to find x(0), which represents the maximum total value of any gameplay on T. Store parent pointers to reconstruct an optimal gameplay if needed.
7. The time complexity is O(n + m) due to # subproblems (O(n)) and work per subproblem (expected O(1)).

QUESTION: Given a target sum m and two disjoint sets A and B, each containing exactly n distinct positive integers, describe an O(nm)-time algorithm to determine whether m is the sum of any limited-unlimited sequence S of A and B, i.e., $m = \sum_{s \in S} s$.
//// ANSWER: To solve this problem, we can use a dynamic programming approach called the Knapsack Problem with Limited Repetition. Here's an algorithm that runs in O(nm) time:

1. Initialize a 2D array dp of size (n+1) x (m+1), where dp[i][j] represents the maximum value that can be obtained by taking at most i elements from set A and B with their sum less than or equal to j.

2. Iterate through all elements in set A and fill up the first row of the dp array:
   - For each element a_i in A, set dp[i][0] = a_i if 0 < i, otherwise leave it as 0.
   - For j > 0, set dp[i][j] = max(dp[i][j], dp[i-1][j]) for all 1 <= i <= n.

3. Iterate through all elements in set B and fill up the columns from the second column onwards of the dp array:
   - For each element b_j in B, iterate through all values i (from 0 to n) and update dp[i][j] as follows:
     - If i == 0, set dp[i][j] = dp[i][j-b_j].
     - Otherwise, for each valid value of i (1 <= i <= n), set dp[i][j] = max(dp[i][j], dp[i-1][j] + dp[i][j-b_j]).

4. The answer is found in the bottom right cell of the dp array: if dp[n][m] >= m, then it's possible to find a limited-unlimited sequence S of A and B that sums up to m; otherwise, no such sequence exists.

QUESTION: Describe an O(n³)-time algorithm to determine whether it is possible to assign the students equally to the three breakout rooms (i.e., n students to each room) while providing strictly positive help to every student, and if possible, return the maximum total benefit to students of any such assignment.
//// ANSWER: To determine whether it is possible to assign n students equally to three breakout rooms (each with n students) while providing strictly positive help to every student, and if possible, find the maximum total benefit to students of any such assignment, we can use a brute-force approach with a time complexity of O(n³).

Here's an outline of the algorithm:

1. Initialize an empty 3D array `help[i][j][k]` to store the maximum total help that can be provided when assigning students i, j, and k to three different rooms. Set all values in the array to zero initially.

2. For each possible assignment of the first student (1 to n), calculate their help value by iterating through all possible assignments for the second student (1 to n) and then calculating the total help provided using the third student's assignment. Store this value at `help[1][j][k]`.

3. Repeat step 2 for each of the remaining students, resulting in a filled-in 3D array `help`.

4. Find the maximum help value across all possible assignments for the first student (i.e., find max(help[1][*][*])). This value represents the maximum total benefit to students when assigning n students equally to three breakout rooms while providing strictly positive help to every student.

5. To check if it is possible to achieve this maximum total help, iterate through all possible assignments for the first student again and compare their help values with the max found in step 4. If any of them match the max value, then it is possible to assign students equally to three breakout rooms while providing strictly positive help to every student.

This algorithm has a time complexity of O(n³) because for each student (n iterations), we are considering all possible assignments for the next two students (n² iterations). The space complexity is also O(n³) due to the 3D array used to store intermediate results.

