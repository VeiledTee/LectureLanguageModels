[
    {
        "question": "What are the main topics covered in Lecture 9?",
        "answer": "Graphs and Breadth-First Search. Topics include: Graph Applications, Graph Definitions, Graph Representations, Neighbor Sets/Adjacencies, Paths, Graph Path Problems, Shortest Paths Tree, Breadth-First Search (BFS)"
    },
    {
        "question": "What is a graph G?",
        "answer": "A graph G = (V, E) is a set of vertices V and a set of pairs of vertices E \u2286 V \u00d7 V.  Directed edges are ordered pairs, e.g., (u, v) for u, v \u2208 V. Undirected edges are unordered pairs, e.g., {u, v} for u, v \u2208 V i.e., (u, v) and (v, u).  In this class, we assume all graphs are simple: edges are distinct, e.g., (u, v) only occurs once in E (though (v, u) may appear), and edges are pairs of distinct vertices, e.g., u \u2260 v for all (u, v) \u2208 E Simple implies |E| = O(|V|\u00b2), since |E| \u2264 $\\binom{|V|}{2}$ for undirected, \u2264 2$\\binom{|V|}{2}$ for directed"
    },
    {
        "question": "Explain Neighbor Sets/Adjacencies",
        "answer": "The outgoing neighbor set of u \u2208 V is Adj+(u) = {v \u2208 V | (u, v) \u2208 E}. The incoming neighbor set of u \u2208 V is Adj\u00af(u) = {v \u2208 V | (v, u) \u2208 E}. The out-degree of a vertex u \u2208 V is deg+(u) = |Adj+(u)|. The in-degree of a vertex u \u2208 V is deg\u00af(u) = |Adj\u00af(u)|. For undirected graphs, Adj\u00af(u) = Adj+(u) and deg\u00af(u) = deg+(u). Dropping superscript defaults to outgoing, i.e., Adj(u) = Adj+(u) and deg(u) = deg+(u)"
    },
    {
        "question": "How do you represent a graph?",
        "answer": "To store a graph G = (V, E), we need to store the outgoing edges Adj(u) for all u \u2208 V. First, need a Set data structure Adj to map u to Adj(u). Then for each u, need to store Adj(u) in another data structure called an adjacency list. Common to use direct access array or hash table for Adj, since want lookup fast by vertex. Common to use array or linked list for each Adj(u) since usually only iteration is needed.  For the common representations, Adj has size (|V|), while each Adj(u) has size (deg(u)). Since \u03a3\u1d64\u2208\u1d65 deg(u) \u2264 2|E| by handshaking lemma, graph storable in \u0398(|V| + |E|) space. Thus, for algorithms on graphs, linear time will mean \u0398(|V|+|E|) (linear in size of graph)"
    },
    {
        "question": "What is a path in a graph?",
        "answer": "A path is a sequence of vertices p = (V1, V2, . . ., Vk) where (Vi, Vi+1) \u2208 E for all 1 < i < k. A path is simple if it does not repeat vertices. The length l(p) of a path p is the number of edges in the path.  The distance \u03b4(u, v) from u \u2208 V to v \u2208 V is the minimum length of any path from u to v, i.e., the length of a shortest path from u to v (by convention, \u03b4(u, v) = \u221e if u is not connected to v)"
    },
    {
        "question": "What are some Graph Path Problems?",
        "answer": "SINGLE_PAIR_REACHABILITY(G, s, t): is there a path in G from s \u2208 V to t \u2208 V? SINGLE_PAIR_SHORTEST_PATH(G, s, t): return distance d(s, t), and a shortest path in G = (V, E) from s \u2208 V to t \u2208 V. SINGLE_SOURCE_SHORTEST_PATHS(G, s): return d(s, v) for all v \u2208 V, and a shortest-path tree containing a shortest path from s to every v \u2208 V (defined below)"
    },
    {
        "question": "How does Breadth-First Search (BFS) work?",
        "answer": "How to compute \u03b4(s, v) and P(v) for all v \u2208 V? Store \u03b4(s, v) and P(v) in Set data structures mapping vertices v to distance and parent. (If no path from s to v, do not store v in P and set d(s, v) to \u221e). Idea! Explore graph nodes in increasing order of distance. Goal: Compute level sets L\u2081 = {v | v \u2208 V and d(s, v) = i} (i.e., all vertices at distance i). Claim: Every vertex v \u2208 Li must be adjacent to a vertex u \u2208 Li\u22121 (i.e., v \u2208 Adj(u)). Claim: No vertex that is in Lj for some j < i, appears in Li. Invariant: \u03b4(s, v) and P(v) have been computed correctly for all v in any Lj for j < i. Base case (i = 1): Lo = {s}, d(s, s) = 0, P(s) = None. Inductive Step: To compute Li: for every vertex u in Li\u22121: * for every vertex v \u2208 Adj(u) that does not appear in any Lj for j < i: add v to Li, set d(s, v) = i, and set P(v) = u. Repeatedly compute L\u2081 from Lj for j < i for increasing i until L\u2081 is the empty set. Set (s, v) = \u221e for any v \u2208 V for which \u03b4(s, v) was not set"
    },
    {
        "question": "What is the running time analysis for Breadth-First Search?",
        "answer": "Store each L\u2081 in data structure with \u0472(|L\u2081|)-time iteration and O(1)-time insertion (i.e., in a dynamic array or linked list). Checking for a vertex v in any Lj for j < i can be done by checking for v in P. Maintain & and P in Set data structures supporting dictionary ops in O(1) time (i.e., direct access array or hash table). Algorithm adds each vertex u to \u2264 1 level and spends O(1) time for each v \u2208 Adj(u). Work upper bounded by O(1) \u00d7 \u03a3\u03c4\u03b5\u03bd deg(u) = O(|E|) by handshake lemma. Spend (|V|) at end to assign d(s, v) for vertices v \u2208 V not reachable from s. So breadth-first search runs in linear time! O(|V| + |E|)"
    },
    {
        "question": "Describe the figures present in the lecture notes.",
        "answer": "There are three figures representing graphs. G1 consists of vertices 0, 1, 2, and 3 with edges (0,1), (1,2), (2,3), and (3,0). G2 has vertices 0, 1, and 2 with directed edges (0,1), (1,2), and (2,2). G3 contains vertices s, a, b, c, d, e, f, and g. The edges are (s,a), (s,b), (a,b), (a,c), (b,c), (c,d), (c,e), (d,e), (d,f), (e,f) and g is an isolated node."
    },
    {
        "question": "What topics were previously covered?",
        "answer": "Weighted graphs, shortest-path weight, negative-weight cycles; Finding shortest-path tree from shortest-path weights in O(|V| + |E|) time; DAG Relaxation: algorithm to solve SSSP on a weighted DAG in O(|V| + |E|) time; SSSP for graph with negative weights: Compute \u03b4(s, v) for all v \u2208 V (-\u221e if v reachable via negative-weight cycle); If a negative-weight cycle reachable from s, return one"
    },
    {
        "question": "Exercise 1:",
        "answer": "Given undirected graph G, return whether G contains a negative-weight cycle Solution: Return Yes if there is an edge with negative weight in G in O(|E|) time"
    },
    {
        "question": "Exercise 2:",
        "answer": "Given SSSP algorithm A that runs in O(|V|(|V| + |E|) time, show how to use it to solve SSSP in O(|V||E|) time\nSolution: Run BFS or DFS to find the vertices reachable from s in O(|E|) time\nMark each vertex v not reachable from s with d(s, v) = \u221e in O(|V|) time\nMake graph G' = (V', E') with only vertices reachable from s in O(|V| + |E|) time\nRun A from s in G'.\nG' is connected, so |V'| = O(|E'|) = O(|E|) so A runs in O(|V||E|) time"
    },
    {
        "question": "Restrictions",
        "answer": "BFS graph is General and Weights is Unweighted with a running time O(|V|+|E|) Lecture L09; DAG Relaxation graph is DAG and Weights is Any with a running time O(|V|+|E|) Lecture L11; Bellman-Ford graph is General and Weights is Any with a running time O(|V|\u00b7 |E|) Lecture L12 (Today!); Dijkstra graph is General and Weights is Non-negative with a running time O(|V| log |V| + |E|) Lecture L13"
    },
    {
        "question": "What are simple shortest paths?",
        "answer": "If graph contains cycles and negative weights, might contain negative-weight cycles; If graph does not contain negative-weight cycles, shortest paths are simple!; Claim 1: If d(s, v) is finite, there exists a shortest path to v that is simple; Proof: By contradiction: Suppose no simple shortest path; let \u03c0 be a shortest path with fewest vertices\n\u03c0 not simple, so exists cycle C in \u03c0; C has non-negative weight (or else d(s, v) = -\u221e)\nRemoving C from \u03c0 forms path \u03c0' with fewer vertices and weight w(\u03c0') \u2264 w(\u03c0); Since simple paths cannot repeat vertices, finite shortest paths contain at most |V| 1 edges"
    },
    {
        "question": "What are negative cycle witnesses?",
        "answer": "k-Edge Distance \u03b4\u03ba(s, v): the minimum weight of any path from s to v using \u2264 k edges\nIdea! Compute \u03b4|v|\u20131(s, v) and d|v|(s, v) for all v \u2208 V\nIf \u03b4(s, v) \u2260 \u2212\u221e, \u03b4(s, v) = \u03b4\n\u03bd\n\u22121 (s, v), since a shortest path is simple (or nonexistent)\nIf \u03b4\u03bd (s, \u03c5) < \u03b4\u03b9\u03bd\u03b9-1(8, \u03c5)\n* there exists a shorter non-simple path to v, so dv (s, v) = \u2212\u221e\n* call v a (negative cycle) witness\nHowever, there may be vertices with -\u221e shortest-path weight that are not witnesses\nClaim 2: If (s, v) = \u2212\u221e, then v is reachable from a witness\nProof: Suffices to prove: every negative-weight cycle reachable from s contains a witness\nConsider a negative-weight cycle C reachable from s\nFor v \u2208 C, let v' \u2208 C denote v's predecessor in C, where \u03a3\u03bd\u03b5\u03c2 \u03c9(\u03bd', v) < 0\nThen \u03b4\u03b9\u03bd\u03b9 (s, v) \u2264 \u03b4\n\u03bd\n\u22121(s, v')+w(\u03bd', \u03c5) (RHS weight of some path on \u2264 |V| vertices)\nSo \u03a3\u03b4\u03b9\u03bd\u03b9(\u03c2, \u03c5) \u2264 \u03a3 \u03b4\n\u03bd\n1-1(s, v') + \u03a3\u03c9(\u03bd', v) < \u03a3\u03b4\u03b9\u03bd-1(5, \u03c5)\nVEC\nVEC\nVEC\nVEC\nIf C contains no witness, \u03b4\u03b9\u03bd\u2081(s, v) \u2265 \u03b4\u03b9\u03bd\u00a6-1(s, v) for all v \u2208 C, a contradiction"
    },
    {
        "question": "Describe Bellman-Ford",
        "answer": "Idea! Use graph duplication: make multiple copies (or levels) of the graph\n|V| + 1 levels: vertex vk in level k represents reaching vertex v from s using \u2264 k edges\nIf edges only increase in level, resulting graph is a DAG!\nConstruct new DAG G' = (V', E') from G = (V, E):\nG' has |V|(|V| + 1) vertices vk for all v \u2208 V and k \u2208 {0, . . ., |V|}\nG' has |V|(|V| + |E|) edges:\n* |V| edges (Uk\u22121, Uk) for k \u2208 {1, ..., |V|} of weight zero for each v \u2208 V\n* |V| edges (Uk\u22121, Uk) for k \u2208 {1, ...,|V|} of weight w(u, v) for each (u, v) \u2208 E\nRun DAG Relaxation on G' from so to compute \u03b4(50, vk) for all vk \u2208 V'\nFor each vertex: set d(s, v) = d(so, U|V|\u22121)\nFor each witness u \u2208 V where d(so, u\n\nv\n) < \u03b4(so, U|V|\u22121):\nFor each vertex v reachable from u in G:\n* set d(s, v) = 18"
    },
    {
        "question": "Give an example of the graph",
        "answer": "The graph G consists of vertices a, b, c, and d with edges a to b (-5), b to c (1), c to d (3), a to c (6), a to d (4), and d to b (-1). Graph G' consists of 5 levels of duplicated vertices a, b, c, and d.  \u03b4(ao, uk) are as follows: for k=0, a is 0, b, c, and d are \u221e; for k=1, a is 0, b is -5, c is 6, and d is \u221e; for k=2, a is 0, b is -5, c is -9, and d is 9; for k=3, a is 0, b is -5, c is -9, and d is -6; for k=4, a is 0, b is -7, c is -9, and d is -6.  The resulting shortest paths are delta(a, v): a is 0, b is \u221e, c is \u221e, and d is \u221e."
    },
    {
        "question": "What is the correctness of Bellman-Ford?",
        "answer": "Claim 3: \u03b4(8o, vk) = \u03b4\u03ba(s, v) for all v \u2208 V and k \u2208 {0, . . ., |V|}\nProof: By induction on k:\nBase case: true for all v \u2208 V when k = 0 (only vo reachable from so is v = s)\nInductive Step: Assume true for all k < k', prove for k = k'\n\u03b4(8\u03bf, \u03c5\u03ba\u03b9) = min{d(so, uk\u22121) + W(Uk'\u22121, Uk') | Uk\u2032\u22121 \u2208 Adj\u00af(vk')}\nmin{{\u03b4(80, Uk\u22121) + w(u, v) | u \u2208 Adj\u00af(v)} \u222a {(50, Uk\u2032\u22121)}}\n-1\n= min{{\u03b4\u03ba-1(s, u) + w(u, v) | u \u2208 Adj\u00af(v)} \u222a {\u03b4\u03ba\u03b9\u22121(8, v)}} (by induction)\n= \u03b4\u03ba\u03b9(\u03c2, \u03c5)\nClaim 4: At the end of Bellman-Ford d(s, v) = \u03b4(s, v)\nProof: Correctly computes d|v|\u22121(s, v) and d|v|(s, v) for all v \u2208 V by Claim 3\nIf d(s, v) \u2260 \u2212\u221e, correctly sets d(s, v) = d\u2081v-1(s, v) = \u03b4(s, v)\nThen sets d(s, v) = \u2212\u221e for any v reachable from a witness; correct by Claim 2"
    },
    {
        "question": "What is the running time?",
        "answer": "G' has size O(|V|(|V| + |E|)) and can be constructed in as much time\nRunning DAG Relaxation on G' takes linear time in the size of G'\nDoes O(1) work for each vertex reachable from a witness\nFinding reachability of a witness takes O(|E|) time, with at most O(|V|) witnesses: O(|V||E|)\n(Alternatively, connect super node x to witnesses via 0-weight edges, linear search from x)\nPruning G at start to only subgraph reachable from s yields O(|V||E|)-time algorithm"
    },
    {
        "question": "What are Extras: Return Negative-Weight Cycle or Space Optimization",
        "answer": "Claim 5: Shortest so \u03c5\u03bd path \u03c0 for any witness v contains a negative-weight cycle in G\nProof: Since \u03c0 contains |V| + 1 vertices, must contain at least one cycle C in G\nC has negative weight (otherwise, remove C to make path \u03c0' with fewer vertices and\nw(\u03c0') \u2264 w(\u03c0), contradicting witness v)\nCan use just O(|V|) space by storing only \u03b4(so, Uk\u22121) and d(so, vk) for each k from 1 to |V|\nTraditionally, Bellman-Ford stores only one value per vertex, attempting to relax every edge\nin |V| rounds; but estimates do not correspond to k-Edge Distances, so analysis trickier\nBut these space optimizations don't return a negative weight cycle"
    },
    {
        "question": "What is Dynamic Programming Review?",
        "answer": "Recursion where subproblem dependencies overlap, forming DAG. 'Recurse but re-use' (Top down: record and lookup subproblem solutions) and 'Careful brute force\u201d (Bottom up: do each subproblem in order)"
    },
    {
        "question": "What are the Dynamic Programming Steps (SRT BOT)?",
        "answer": "1. Subproblem definition subproblem \\( x \\in X \\)\nDescribe the meaning of a subproblem in words, in terms of parameters. Often subsets of input: prefixes, suffixes, contiguous substrings of a sequence.\nOften multiply possible subsets across multiple inputs. Often record partial state: add subproblems by incrementing some auxiliary variables.\n2. Relate subproblem solutions recursively \\( x(i) = f(x(j), . . .) \\) for one or more \\( j < i \\)\nIdentify a question about a subproblem solution that, if you knew the answer to, reduces the subproblem to smaller subproblem(s). Locally brute-force all possible answers to the question.\n3. Topological order to argue relation is acyclic and subproblems form a DAG.\n4. Base cases: State solutions for all (reachable) independent subproblems where relation breaks down\n5. Original problem: Show how to compute solution to original problem from solutions to subproblem(s). Possibly use parent pointers to recover actual solution, not just objective function.\n6. Time analysis: \\( \\Sigma_{x \\in X} work(x) \\), or if work(x) = O(W) for all \\( x \\in X \\), then \\( |X| \\cdot O(W) \\). work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time"
    },
    {
        "question": "What is Longest Common Subsequence (LCS)?",
        "answer": "Given two strings A and B, find a longest (not necessarily contiguous) subsequence of A that is also a subsequence of B. Example: A = hieroglyphology, B = michaelangelo. Solution: hello or heglo or iello or ieglo, all length 5. Maximization problem on length of subsequence. Subproblems: \\( x(i, j) = \\) length of longest common subsequence of suffixes A[i:] and B[j :]. For 0 \u2264 i \u2264 |A| and 0 \u2264 j \u2264 |B|. Relate: Either first characters match or they don't. If first characters match, some longest common subsequence will use them. (if no LCS uses first matched pair, using it will only improve solution). (if an LCS uses first in A[i] and not first in B[j], matching B[j] is also optimal). If they do not match, they cannot both be in a longest common subsequence. Guess whether A[i] or B[j] is not in LCS. Topological order: Subproblems \\( x(i, j) \\) depend only on strictly larger i or j or both. Simplest order to state: Decreasing i + j. Nice order for bottom-up code: Decreasing i, then decreasing j. Base: \\( x(\u0456, |B|) = x(|A|, j) = 0 \\) (one string is empty). Original problem: Length of longest common subsequence of A and B is x(0,0). Store parent pointers to reconstruct subsequence. If the parent pointer increases both indices, add that character to LCS. Time: # subproblems: (|A| + 1) \u00b7 (|B| + 1). work per subproblem: O(1). O(|A|\u00b7 |B|) running time"
    },
    {
        "question": "What is Longest Increasing Subsequence (LIS)?",
        "answer": "Given a string A, find a longest (not necessarily contiguous) subsequence of A that strictly increases (lexicographically). Example: A = carbohydrate. Solution: abort, of length 5. Maximization problem on length of subsequence. Attempted solution: Natural subproblems are prefixes or suffixes of A, say suffix A[i:]. Natural question about LIS of A[i :]: is A[i] in the LIS? (2 possible answers). But then how do we recurse on A[i + 1 :] and guarantee increasing subsequence? Fix: add constraint to subproblems to give enough structure to achieve increasing property. Subproblems: \\( x(i) = \\) length of longest increasing subsequence of suffix A[i :] that includes A[i]. For 0 \u2264 i \u2264 |A|. Relate: We're told that A[i] is in LIS (first element). Next question: what is the second element of LIS? Could be any A[j] where j > i and A[j] > A[i] (so increasing) Or A[i] might be the last element of LIS. \\( x(i) = max\\{1 + x(j) | i < j < |A|, A[j] > A[i]\\} \\cup \\{1\\} \\). Topological order: Decreasing i. Base: No base case necessary, because we consider the possibility that A[i] is last. Original problem: What is the first element of LIS? Guess! Length of LIS of A is max\\{x(i) | 0 \u2264 i < |A|\\}. Store parent pointers to reconstruct subsequence. Time: # subproblems: |A|. work per subproblem: O(|A|). O(|A|\u00b2) running time. Exercise: speed up to O(|A|log|A|) by doing only O(log|A|) work per subproblem, via AVL tree augmentation"
    },
    {
        "question": "What is Alternating Coin Game?",
        "answer": "Given sequence of n coins of value vo, V1,..., Un 1. Two players (\u201cme\u201d and \u201cyou\u201d) take turns. In a turn, take first or last coin among remaining coins. My goal is to maximize total value of my taken coins, where I go first. First solution exploits that this is a zero-sum game: I take all coins you don't. Subproblems: Choose subproblems that correspond to the state of the game. For every contiguous subsequence of coins from i to j, 0 \u2264 i \u2264 j < n. \\( x(i, j) = \\) maximum total value I can take starting from coins of values vi, . . ., Vj. Relate: I must choose either coin i or coin j (Guess!). Then it's your turn, so you'll get value x(i + 1, j) or x(i, j 1), respectively. To figure out how much value I get, subtract this from total coin values. Topological order: Increasing j i. Base: x(i,i) = vi. Original problem: x(0,\u03b7 1). Store parent pointers to reconstruct strategy. Time: # subproblems: \u0398(n\u00b2). work per subproblem: \u0398(n) to compute sums. \u0398(n\u00b3) running time. Exercise: speed up to O(n\u00b2) time by precomputing all sums \\( \\Sigma_{k=i+1}^{-i} v_k \\) in \u0398(n\u00b2) time, via dynamic programming (!). Second solution uses subproblem expansion: add subproblems for when you move next. Subproblems: Choose subproblems that correspond to the full state of the game. Contiguous subsequence of coins from i to j, and which player p goes next. \\( x(i, j,p) = \\) maximum total value I can take when player p \\( \\in \\{\\text{me}, \\text{you}\\}\\) starts from coins of values Vi, . . ., Vj. Relate: Player p must choose either coin i or coin j (Guess!). If p = me, then I get the value; otherwise, I get nothing. Then it's the other player's turn. Topological order: Increasing j i. Base: x(i, i, me) = Vi and x(i, i, you) = 0. Original problem: x(0,n 1, me). Store parent pointers to reconstruct strategy. Time: # subproblems: \u0398(n\u00b2). work per subproblem: \u0398(1). \u0398(n\u00b2) running time"
    },
    {
        "question": "What are Subproblem Constraints and Expansion?",
        "answer": "We've now seen two examples of constraining or expanding subproblems. If you find yourself lacking information to check the desired conditions of the problem, or lack the natural subproblem to recurse on, try subproblem constraint/expansion! More subproblems and constraints give the relation more to work with, so can make DP more feasible. Usually a trade-off between number of subproblems and branching/complexity of relation. More examples next lecture"
    },
    {
        "question": "What are the operations complexities for Array, Linked List, and Dynamic Array for build(X), get_at(i), insert_first(x), insert_last(x), set_at(i,x) delete_first(), delete_last (), insert_at(i, x), delete_at(i)?",
        "answer": "Array: n,1,n,n,n; Linked List: n,n,1,n,n; Dynamic Array: n,1,n,1(a),n; Goal: n,logn,logn,logn,logn"
    },
    {
        "question": "What are the operations complexities for Array, Sorted Array, Direct Access Array, and Hash Table for build (X), find(k), insert (x), find_min(), find_prev (k), delete (k), find_max(), find_next (k)?",
        "answer": "Array: n,n,n,n,n; Sorted Array: nlog n,log n,n,1,log n; Direct Access Array: U,1,1,U,U; Hash Table: n(e), 1(e),1(a)(e),n,n; Goal: nlogn, log n, log n, log n, log n"
    },
    {
        "question": "How do Binary Trees work?",
        "answer": "Pointer-based data structures (like Linked List) can achieve worst-case performance; Binary tree is pointer-based data structure with three pointers per node; Node representation: node.{item,parent, left, right}"
    },
    {
        "question": "What are the terminologies associated with Binary Trees?",
        "answer": "The root of a tree has no parent (Ex: <A>); A leaf of a tree has no children (Ex: <C>, <E>, and <F>); Define depth(<X>) of node <X> in a tree rooted at <R> to be length of path from <X> to <R>; Define height(<X>) of node <X> to be max depth of any node in the subtree rooted at <X>; Design operations to run in $O(h)$ time for root height h, and maintain $h = O(log n)$; A binary tree has an inherent order: its traversal order\nevery node in node <X>'s left subtree is before <X>\nevery node in node <X>'s right subtree is after <X>"
    },
    {
        "question": "How to list nodes in traversal order?",
        "answer": "List nodes in traversal order via a recursive algorithm starting at root:\nRecursively list left subtree, list self, then recursively list right subtree; Runs in $O(n)$ time, since $O(1)$ work is done to list each node; Example: Traversal order is (<F>, <D>, <B>, <E>, <A>, <C>); Right now, traversal order has no meaning relative to the stored items; Later, assign semantic meaning to traversal order to implement Sequence/Set interfaces"
    },
    {
        "question": "How to do Tree Navigation?",
        "answer": "Find first node in the traversal order of node <X>'s subtree (last is symmetric); If <X> has left child, recursively return the first node in the left subtree; Otherwise, <X> is the first node, so return it; Running time is $O(h)$ where h is the height of the tree; Example: first node in <A>'s subtree is <F>; Find successor of node <X> in the traversal order (predecessor is symmetric); If <X> has right child, return first of right subtree; Otherwise, return lowest ancestor of <X> for which <X> is in its left subtree; Running time is $O(h)$ where h is the height of the tree; Example: Successor of: <B> is <E>, <E> is <A>, and <C> is None"
    },
    {
        "question": "How to do Dynamic Operations?",
        "answer": "Change the tree by a single item (only add or remove leaves):\nadd a node after another in the traversal order (before is symmetric); remove an item from the tree; Insert node <Y> after node <X> in the traversal order; If <X> has no right child, make <Y> the right child of <X>; Otherwise, make <Y> the left child of <X>'s successor (which cannot have a left child); Running time is O(h) where h is the height of the tree; Delete the item in node <X> from <X>'s subtree; If <X> is a leaf, detach from parent and return; Otherwise, <X> has a child; * If <X> has a left child, swap items with the predecessor of <X> and recurse; * Otherwise <X> has a right child, swap items with the successor of <X> and recurse; Running time is O(h) where h is the height of the tree"
    },
    {
        "question": "How Set Binary Tree works?",
        "answer": "Idea! Set Binary Tree (a.k.a. Binary Search Tree / BST):; Traversal order is sorted order increasing by key; Equivalent to BST Property: for every node, every key in left subtree < node's key < every key in right subtree; Then can find the node with key k in node <X>'s subtree in O(h) time like binary search:\nIf k is smaller than the key at <X>, recurse in left subtree (or return None); If k is larger than the key at <X>, recurse in right subtree (or return None); Otherwise, return the item stored at <X>; Other Set operations follow a similar pattern; see recitation"
    },
    {
        "question": "How Sequence Binary Tree works?",
        "answer": "Idea! Sequence Binary Tree: Traversal order is sequence order; How do we find ith node in traversal order of a subtree? Call this operation subtree_at(i); Could just iterate through entire traversal order, but that's bad, O(n); However, if we could compute a subtree's size in O(1), then can solve in O(h) time; How? Check the size n\u2081 of the left subtree and compare to i\nIf i < nL, recurse on the left subtree; If i > n\u2081, recurse on the right subtree with i' = i - NL \u2013 1; Otherwise, i = n\u2081, and you've reached the desired node!; Maintain the size of each node's subtree at the node via augmentation; Add node.size field to each node; When adding new leaf, add +1 to a. size for all ancestors a in O(h) time; When deleting a leaf, add -1 to a. size for all ancestors a in O(h) time; Sequence operations follow directly from a fast subtree_at(i) operation; Naively, build(X) takes O(nh) time, but can be done in O(n) time; see recitation"
    },
    {
        "question": "What are the operations complexities for Binary Tree for Set Data Structure?",
        "answer": "For operations build(X), find(k), insert (x), find_min(), find_prev (k), delete (k), find_max(), find_next (k) the complexities are n log n, h, h, h, h, h, h, h"
    },
    {
        "question": "What are the operations complexities for Binary Tree for Sequence Data Structure?",
        "answer": "For operations build (X), get_at(i), set_at(i,x), insert_first (x), delete_first(), insert_last (x), insert_at(i, x), delete_last(), delete_at(i) the complexities are n, h, h, h, h, h, h, h, h"
    },
    {
        "question": "How to make a balanced Binary Tree?",
        "answer": "Keep a binary tree balanced after insertion or deletion; Reduce O(h) running times to O(log n) by keeping h = O(log n)"
    },
    {
        "question": "What is the Priority Queue Interface?",
        "answer": "It is used to keep track of many items and quickly access/remove the most important. Some examples include router with limited bandwidth, process scheduling in operating system kernels, discrete-event simulation, and graph algorithms."
    },
    {
        "question": "How are items ordered in a Priority Queue?",
        "answer": "Items are ordered by key = priority, representing a Set interface."
    },
    {
        "question": "What are the common operations for a Priority Queue?",
        "answer": "Common operations include: build(X) to build a priority queue from iterable x, insert(x) to add item x to data structure, delete_max() to remove and return stored item with largest key, and find_max() to return stored item with largest key. These are usually optimized for max or min, not both."
    },
    {
        "question": "How can a priority queue data structure be used for sorting?",
        "answer": "Any priority queue data structure can translate into a sorting algorithm by first building the queue (e.g., inserting items one by one in input order) and then repeatedly deleting the min (or max) to determine the (reverse) sorted order. The running time is \\( T_{build} + n \\cdot T_{delete\\_max} \\le n \\cdot T_{insert} + n \\cdot T_{delete\\_max} \\)."
    },
    {
        "question": "What is the Set AVL Tree Implementation for a Priority Queue?",
        "answer": "Set AVL trees support insert(x), find_min(), find_max(), delete_min(), and delete_max() in \\( O(\\log n) \\) time per operation. This results in a priority queue sort that runs in \\( O(n \\log n) \\) time and essentially is AVL sort from Lecture 7."
    },
    {
        "question": "How is an array used as a priority queue and what are the time complexities of insert and delete_max?",
        "answer": "Elements are stored in an unordered dynamic array. insert(x) appends x to the end in amortized \\( O(1) \\) time, while delete_max() finds the max in \\( O(n) \\), swaps it to the end, and removes it.  insert is quick, but delete_max is slow.  Priority queue sort is selection sort! (plus some copying)"
    },
    {
        "question": "How is a sorted array used as a priority queue and what are the time complexities of insert and delete_max?",
        "answer": "Elements are stored in a sorted dynamic array. insert(x) appends x to the end and swaps it down to its sorted position in \\( O(n) \\) time. delete_max() deletes from the end in \\( O(1) \\) amortized time.  delete_max is quick, but insert is slow. Priority queue sort is insertion sort! (plus some copying)."
    },
    {
        "question": "How can an array be viewed as a complete binary tree?",
        "answer": "An array can be interpreted as a complete binary tree, with maximum \\( 2^i \\) nodes at depth i except at the largest depth, where all nodes are left-aligned. The height of the tree is \\( [\\lg n] \\), making it a balanced binary tree."
    },
    {
        "question": "What is the Max-Heap Property?",
        "answer": "Max-Heap Property at node i: \\( Q[i] > Q[j] \\) for \\( j \\in \\{left(i), right(i)\\} \\). In a max-heap, every node i satisfies \\( Q[i] \\ge Q[j] \\) for all nodes j in subtree(i)."
    },
    {
        "question": "Describe the Heap Insert operation.",
        "answer": "Append the new item x to the end of array in \\( O(1) \\) amortized time, making it the next leaf i in reading order.  Then, call max_heapify_up(i) to swap with parent until Max-Heap Property is satisfied. Correctness: Max-Heap Property guarantees all nodes are greater or equal to descendants, except Q[i] might be greater than some of its ancestors (unless i is the root, so we're done). If swap necessary, same guarantee is true with Q[parent(i)] instead of Q[i]. Running time: height of tree, so \\( O(\\log n) \\)!"
    },
    {
        "question": "Describe the Heap Delete Max operation.",
        "answer": "Swap item at root node i = 0 with the last item at node n-1 in heap array. Then, call max_heapify_down(i): swap root with larger child until Max-Heap Property is satisfied. Correctness: Max-Heap Property guarantees all nodes > descendants, except Q[i] might be < some descendants (unless i is a leaf, so we're done). If swap is necessary, same guarantee is true with Q[j] instead of Q[i]. Running time: height of tree, so \\( O(\\log n) \\)!"
    },
    {
        "question": "Describe Heap Sort.",
        "answer": "Plugging max-heap into priority queue sort gives us a new sorting algorithm. Running time is \\( O(n \\log n) \\) because each insert and delete_max takes \\( O(\\log n) \\). Often include two improvements to this sorting algorithm"
    },
    {
        "question": "Describe In-place Priority Queue Sort",
        "answer": "Max-heap Q is a prefix of a larger array A.  Q is initially zero, eventually |A| (after inserts), then zero again (after deletes). insert() absorbs next item in array at index |Q| into heap. delete_max() moves max item to end, then abandons it by decrementing |Q|. In-place priority queue sort with Array is exactly Selection Sort. In-place priority queue sort with Sorted Array is exactly Insertion Sort. In-place priority queue sort with binary Max Heap is Heap Sort."
    },
    {
        "question": "Describe the Linear Build Heap process",
        "answer": "Treat full array as a complete binary tree from start, then max_heapify_down(i) for i from n \u2212 1 to 0 (leaves up).  The worst-case swaps = \\( \\sum_{i=0}^{n-1} height(i) = \\sum_{i=0}^{n-1} ( \\lg n - \\lg i ) = \\lg n! - n \\cdot \\Theta(\\lg \\frac{n}{\\sqrt{n} (n/e)^n}) = O(n) \\). So can build heap in \\( O(n) \\) time. (Doesn't speed up \\( O(n \\lg n) \\) performance of heap sort)"
    },
    {
        "question": "Describe Sequence AVL Tree Priority Queue",
        "answer": "Where else have we seen linear build time for an otherwise logarithmic data structure? Sequence AVL Tree! Store items of priority queue in Sequence AVL Tree in arbitrary order (insertion order). Maintain max (and/or min) augmentation: node.max = pointer to node in subtree of node with maximum key - This is a subtree property, so constant factor overhead to maintain find_min() and find_max() in \\( O(1) \\) time. delete_min() and delete_max() in \\( O(\\log n) \\) time. build(A) in \\( O(n) \\) time. Same bounds as binary heaps (and more)"
    },
    {
        "question": "How do Multisets relate to the Set interface?",
        "answer": "While our Set interface assumes no duplicate keys, we can use these Sets to implement Multisets that allow items with duplicate keys: Each item in the Set is a Sequence (e.g., linked list) storing the Multiset items with the same key, which is the key of the Sequence In fact, without this reduction, binary heaps and AVL trees work directly for duplicate-key items (where e.g. delete_max deletes some item of maximum key), taking care to use < constraints (instead of \\( \\le \\) in Set AVL Trees)"
    },
    {
        "question": "What is the goal of this class?",
        "answer": "The goal of this class is to teach you to solve computation problems, and to communicate that your solutions are correct and efficient."
    },
    {
        "question": "Define 'Problem' in the context of the lecture.",
        "answer": "A problem is a binary relation from problem inputs to correct outputs. It provides a verifiable predicate (a property) that correct outputs must satisfy and studies problems on large general input spaces."
    },
    {
        "question": "What is an example of problem?",
        "answer": "Given any set of $n$ students, is there a pair of students with same birthday? If birthday is just one of 365, for $n > 365$, answer always true by pigeon-hole. Assume resolution of possible birthdays exceeds $n$ (include year, time, etc.)"
    },
    {
        "question": "Define 'Algorithm'.",
        "answer": "An algorithm is a procedure mapping each input to a single output (deterministic). Algorithm solves a problem if it returns a correct output for every problem input"
    },
    {
        "question": "What is an example of Algorithm?",
        "answer": "An algorithm to solve birthday matching:\nMaintain a record of names and birthdays (initially empty)\nInterview each student in some order. If birthday exists in record, return found pair! Else add name and birthday to record\nReturn None if last student interviewed without success"
    },
    {
        "question": "What is correctness of a program/algorithm and how to prove it?",
        "answer": "Programs/algorithms have fixed size, so we must prove correct by:\nFor small inputs, can use case analysis\nFor arbitrarily large inputs, algorithm must be recursive or loop in some way\nMust use induction (why recursion is such a key concept in computer science)"
    },
    {
        "question": "What is an Example of proving correctness?",
        "answer": "Proof of correctness of birthday matching algorithm\nInduct on $k$: the number of students in record\nHypothesis: if first $k$ contain match, returns match before interviewing student $k + 1$\nBase case: $k = 0$, first $k$ contains no match\nAssume for induction hypothesis holds for $k = k'$, and consider $k = k' + 1$\nIf first $k'$ contains a match, already returned a match by induction\nElse first $k'$ do not have match, so if first $k\u2032 + 1$ has match, match contains $k' + 1$\nThen algorithm checks directly whether birthday of student $k' + 1$ exists in first $k'$"
    },
    {
        "question": "What is 'Efficiency'?",
        "answer": "It refers to how fast does an algorithm produce a correct output?\nCould measure time, but want performance to be machine independent\nIdea! Count number of fixed-time operations algorithm takes to return\nExpect to depend on size of input: larger input suggests longer time\nSize of input is often called 'n', but not always!\nEfficient if returns in polynomial time with respect to input\nSometimes no efficient algorithm exists for a problem! (See L20)"
    },
    {
        "question": "Explain Asymptotic Notation.",
        "answer": "ignore constant factors and low order terms\nUpper bounds ($O$), lower bounds ($\\Omega$), tight bounds ($\\Theta$)\n$\\in$, =, is, order\nTime estimate below based on one operation per cycle on a 1 GHz single-core machine\nParticles in universe estimated < $10^{100}$"
    },
    {
        "question": "What is Model of Computation?",
        "answer": "Specification for what operations on the machine can be performed in $O(1)$ time\nModel in this class is called the Word-RAM\nMachine word: block of w bits (w is word size of a w-bit Word-RAM)\nMemory: Addressable sequence of machine words\nProcessor supports many constant time operations on a $O(1)$ number of words (integers):\ninteger arithmetic: (+, -, *, //, %)\nlogical operators: (&&, ||, !, ==, <, >, <=, =>)\n(bitwise arithmetic: (&, |, <<, >>, ...))\nGiven word a, can read word at address a, write word to address a\nMemory address must be able to access every place in memory\nRequirement: w > # bits to represent largest memory address, i.e., $\\log_2 n$\n32-bit words \u2192 max ~ 4 GB memory, 64-bit words \u2192 max ~ 16 exabytes of memory\nPython is a more complicated model of computation, implemented on a Word-RAM"
    },
    {
        "question": "What is Data Structure?",
        "answer": "A data structure is a way to store non-constant data, that supports a set of operations\nA collection of operations is called an interface\nSequence: Extrinsic order to items (first, last, nth)\nSet: Intrinsic order to items (queries based on item keys)\nData structures may implement the same interface with different performance\nExample: Static Array - fixed width slots, fixed length, static sequence interface\nStaticArray(n): allocate static array of size n initialized to 0 in $O(n)$ time\nStaticArray.get_at(i):return word stored at array index i in $(1)$ time\nStaticArray.set_at(i, x): write word x to array index i in $(1)$ time\nStored word can hold the address of a larger object\nLike Python tuple plus set_at(i, x),Python list is a dynamic array (see L02)"
    },
    {
        "question": "Given the python code provided in the document, write the time analysis.",
        "answer": "Two loops: outer $k \\in {0, . . ., n \u2212 1}$, inner is $i \\in {0, . . ., k}$\nRunning time is $O(n) + \\Sigma(O(1) + k \\cdot O(1)) = O(n^2)$\nQuadratic in n is polynomial. Efficient? Use different data structure for record!"
    },
    {
        "question": "How to Solve an Algorithms Problem?",
        "answer": "1. Reduce to a problem you already know (use data structure or algorithm)\nSearch Problem (Data Structures)\nStatic Array (L01)\nLinked List (L02)\nDynamic Array (L02)\nSorted Array (L03)\nDirect-Access Array (L04)\nHash Table (L04)\nBalanced Binary Tree (L06-L07)\nBinary Heap (L08)\nSort Algorithms\nInsertion Sort (L03)\nSelection Sort (L03)\nMerge Sort (L03)\nCounting Sort (L05)\nRadix Sort (L05)\nAVL Sort (L07)\nHeap Sort (L08)\nShortest Path Algorithms\nBreadth First Search (L09)\nDAG Relaxation (L11)\nDepth First Search (L10)\nTopological Sort (L10)\nBellman-Ford (L12)\nDijkstra (L13)\nJohnson (L14)\nFloyd-Warshall (L18)\n2. Design your own (recursive) algorithm\nBrute Force\nDecrease and Conquer\nDivide and Conquer\nDynamic Programming (L15-L19)\nGreedy / Incremental"
    },
    {
        "question": "What are the Dynamic Programming Steps?",
        "answer": "1. Subproblem definition: subproblem x \u2208 X. Describe the meaning of a subproblem in words, in terms of parameters.\nOften subsets of input: prefixes, suffixes, contiguous substrings of a sequence.\nOften multiply possible subsets across multiple inputs.\nOften record partial state: add subproblems by incrementing some auxiliary variables.\n2. Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i.\nIdentify a question about a subproblem solution that, if you knew the answer to, reduces the subproblem to smaller subproblem(s).\nLocally brute-force all possible answers to the question.\n3. Topological order to argue relation is acyclic and subproblems form a DAG\n4. Base cases: State solutions for all (reachable) independent subproblems where relation breaks down.\n5. Original problem: Show how to compute solution to original problem from solutions to subproblem(s). Possibly use parent pointers to recover actual solution, not just objective function.\n6. Time analysis: \nExex work(x), or if work(x) = O(W) for all x \u2208 X, then |X|\u00b7 O(W)\nwork(x) measures nonrecursive work in relation; treat recursions as taking O(1) time"
    },
    {
        "question": "What are the Subproblems of DAG Shortest Paths?",
        "answer": "Subproblems: \u03b4(s, v) for all v \u2208 V. Relation: \u03b4(s, v) = min{d(s, u) + w(u, v) | u \u2208 Adj(v)} \u222a {\u221e}. Topo. order: Topological order of G"
    },
    {
        "question": "Explain Single-Source Shortest Paths Revisited",
        "answer": "1. Subproblems: Expand subproblems to add information to make acyclic!. \u03b4\u03b5(\u03c2, v) = weight of shortest path from s to v using at most k edges. For v \u2208 V and 0 \u2264 k \u2264 |V|\n2. Relate: Guess last edge (u, v) on shortest path from s to v. \u03b4\u03b5(\u03c2, v) = min{dk-1(s, u) + w(u, v) | (u, v) \u2208 \u0395} \u222a {\u03b4\u03ba\u22121(5, v)}\n3. Topological order: Increasing k: subproblems depend on subproblems only with strictly smaller k\n4. Base: \u03b4\u03bf(\u03c2, 8) = 0 and \u03b4\u03bf(s, v) = \u221e for v \u2260 s (no edges). (draw subproblem graph)\n5. Original problem: If has finite shortest path, then d(s, v) = \u03b4\u03b9\u03bd\u00a6-1(s, v). Otherwise some d|v|(s, v) < \u03b4|\u03bd|\u22121(s, v), so path contains a negative-weight cycle. Can keep track of parent pointers to subproblem that minimized recurrence\n6. Time: # subproblems: |V| \u00d7 (|V| + 1). Work for subproblem \u03b4\u03ba(s, v): O(degin(v)). This is just Bellman-Ford! (computed in a slightly different order)"
    },
    {
        "question": "What is All-Pairs Shortest Paths: Floyd-Warshall?",
        "answer": "Could define subproblems \u03b4\u03ba(u, v) = minimum weight of path from u to v using at most k edges, as in Bellman\u2013Ford. Resulting running time is |V| times Bellman\u2013Ford, i.e., O(|V|\u00b2 \u00b7 |E|) = O(|V|4). Can achieve \u0398(|V|\u00b3) running time (matching Johnson for dense graphs) with a simple dynamic program, called Floyd\u2013Warshall. Number vertices so that V = {1, 2, . . ., |V|}. 1. Subproblems: d(u,v,k) = minimum weight of a path from u to v that only uses vertices from {1,2,..., k} \u222a {u, v}. For u, v \u2208 V and 1 \u2264 k \u2264 |V|. 2. Relate: x(u, v, k) = min{x(u, k, k \u2212 1) + x(k, v, k \u2212 1), x(u, v, k \u2212 1)}. Only constant branching! No longer guessing previous vertex/edge. 3. Topological order: Increasing k: relation depends only on smaller k. 4. Base: x(u, u, 0) = 0, x(u, v, 0) = w(u, v) if (u, v) \u2208 E, x(u, v, 0) = \u221e if none of the above. 5. Original problem: x(u, v, |V|) for all u, v \u2208 V. 6. Time: O(|V|3) subproblems, Each O(1) work, O(|V|\u00b3) in total. Constant number of dependencies per subproblem brings the factor of O(|E|) in the running time down to O(|V|)."
    },
    {
        "question": "Explain Arithmetic Parenthesization",
        "answer": "Input: arithmetic expression a0 *1 A1 *2 A2\u30fb\u30fb\u30fb *n\u22121 An\u22121 where each ai is an integer and each *\u00bf \u2208 {+, \u00d7}. Output: Where to place parentheses to maximize the evaluated expression. Example: 7 + 4\u00d73 +5 \u2192 ((7) + (4)) \u00d7 ((3) + (5)) = 88. Allow negative integers!. Example: 7 + (-4) \u00d7 3 + (-5) \u2192 ((7) + ((-4) \u00d7 ((3) + (-5)))) = 15. 1. Subproblems: Sufficient to maximize each subarray? No! (-3) \u00d7 (-3) = 9 > (\u22122) \u00d7 (-2) = 4. x(i, j, opt) = opt value obtainable by parenthesizing ai *i+1\u00b7\u00b7\u00b7 *j\u22121 Aj\u22121. For 0 < i < j \u2264 n and opt \u2208 {min, max}. 2. Relate: Guess location of outermost parentheses / last operation evaluated. x(i, j, opt) = opt {x(i, k, opt') *k x(k, j, opt'')} | i < k < j; opt', opt'' \u2208 {min, max}}. 3. Topological order: Increasing j \u2013 i: subproblem x(i, j, opt) depends only on strictly smaller j \u2013 i. 4. Base: x(i, i + 1, opt) = ai, only one number, no operations left!. 5. Original problem: X(0, n, max). Store parent pointers (two!) to find parenthesization (forms binary tree!). 6. Time: # subproblems: less than n\u00b7n \u00b7 2 = O(n\u00b2). work per subproblem O(n) \u00b7 2 \u00b7 2 = O(n). O(n\u00b3) running time"
    },
    {
        "question": "Explain Piano Fingering",
        "answer": "Given sequence to, t1, ..., tn\u22121 of n single notes to play with right hand (will generalize to multiple notes and hands later). Performer has right-hand fingers 1, 2, . . ., F (F = 5 for most humans). Given metric d(t, f,t', f') of difficulty of transitioning from note t with finger f to note t' with finger f'. Typically a sum of penalties for various difficulties, e.g.: 1 < f < f' and t t' is uncomfortable. Legato (smooth) play requires t \u2260 t' (else infinite penalty). Weak-finger rule: prefer to avoid f' \u2208 {4,5}. {f, f'} = {3,4} is annoying. Goal: Assign fingers to notes to minimize total difficulty. First attempt: 1. Subproblems: x(i) = minimum total difficulty for playing notes ti, ti+1,..., tn-1. 2. Relate: Guess first finger: assignment f for ti. x(i) = min{x(i + 1) + d(ti, f, ti+1, ?) | 1 \u2264 f < F}. Not enough information to fill in ?. Need to know which finger at the start of x(i + 1). But different starting fingers could hurt/help both x(i + 1) and d(ti, f, ti+1, ?). Need a table mapping start fingers to optimal solutions for x(i + 1). I.e., need to expand subproblems with start condition. Solution: 1. Subproblems: x(i, f) = minimum total difficulty for playing notes ti, ti+1,..., tn\u22121 starting with finger f on note ti. For 0 < i < n and 1 < f <F. 2. Relate: Guess next finger: assignment f' for ti+1. x(i, f) = min{x(i + 1, f') + d(ti, f, ti+1, f') | 1 \u2264 f' < F}. 3. Topological order: Decreasing i (any f order). 4. Base: x(n \u2212 1, f) = 0 (no transitions). 5. Original problem: min{x(0, f) | 1 \u2264 f < F}. 6. Time: \u0398(n. F) subproblems. \u0398(F) work per subproblem. \u0398(\u03b7\u00b7 F2). No dependence on the number of different notes!"
    },
    {
        "question": "Explain Guitar Fingering",
        "answer": "Up to S = number of strings different ways to play the same note. Redefine \"finger\u201d to be tuple (finger playing note, string playing note). Throughout algorithm, F gets replaced by F \u00b7 S. Running time is thus O(n \u00b7 F2 . S2). Multiple Notes at Once: Now suppose t\u2081 is a set of notes to play at time i. Given a bigger transition difficulty function d(t, f, t', f'). Goal: fingering fi : ti \u2192 {1, 2, . . ., F} specifying how to finger each note (including which string for guitar) to minimize. At most TF choices for each fingering fi, where T = maxi |ti|. T < F = 10 for normal piano (but there are exceptions). T \u2264 S for guitar. \u0398(n. TF) subproblems. \u0398(TF) work per subproblem. \u0398(n. T2F) time. \u0398(n) time for T, F < 10. Video Game Appliactions: Guitar Hero / Rock Band F = 4 (and 5 different notes). Dance Dance Revolution: F = 2 feet, T = 2 (at most two notes at once). Exercise: handle sustained notes, using \"where each foot is\u201d (on an arrow or in the middle) as added state for suffix subproblems"
    },
    {
        "question": "Describe the set interface, including the operations for Container, Static, Dynamic, and Order.",
        "answer": "The set interface includes operations for building a set from an iterable, returning the number of stored items, finding a stored item by key, adding an item (with replacement), removing an item by key, iterating in key order, finding the minimum and maximum keys, and finding the next and previous keys relative to a given key."
    },
    {
        "question": "What are the operations and complexities for Array data structure for Container, Static, Dynamic, and Order?",
        "answer": "Container: build(X) - O(n), Static: find(k) - O(n), Dynamic: insert(x) - O(n), delete(k) - O(n), Order: find_min() - O(n), find_max() - O(n), find_next(k) - O(n), find_prev(k) - O(n)."
    },
    {
        "question": "What are the operations and complexities for Sorted Array data structure for Container, Static, Dynamic, and Order?",
        "answer": "Container: build(X) - O(n log n), Static: find(k) - O(log n), Dynamic: insert(x) - O(n), delete(k) - O(n), Order: find_min() - O(1), find_max() - O(1), find_next(k) - O(log n), find_prev(k) - O(log n)."
    },
    {
        "question": "What is sorting?",
        "answer": "Given a sorted array, we can leverage binary search to make an efficient set data structure. Input: (static) array A of n numbers. Output: (static) array B which is a sorted permutation of A.\nPermutation: array with same elements in a different order.\nSorted: B[i - 1] < B[i] for all i \u2208 {1, ..., n}. Example: [8, 2, 4, 9, 3] \u2192 [2, 3, 4, 8, 9].\nA sort is destructive if it overwrites A (instead of making a new array B that is a sorted version of A).\nA sort is in place if it uses O(1) extra space (implies destructive: in place \u2286 destructive)."
    },
    {
        "question": "Explain Permutation Sort.",
        "answer": "There are n! permutations of A, at least one of which is sorted. For each permutation, check whether sorted in O(n). Example: [2, 3, 1] \u2192 {[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]}.\nThe analysis is Correct by case analysis: try all possibilities (Brute Force). Running time: \\(\\Omega(n! \\cdot n)\\) which is exponential :("
    },
    {
        "question": "Explain different ways of Solving Recurrences.",
        "answer": "Substitution: Guess a solution, replace with representative function, recurrence holds true.\nRecurrence Tree: Draw a tree representing the recursive calls and sum computation at nodes.\nMaster Theorem: A formula to solve many recurrences (R03)."
    },
    {
        "question": "Explain Selection Sort.",
        "answer": "Find a largest number in prefix A[:i + 1] and swap it to A[i]. Recursively sort prefix A[:i]. Example: [8, 2, 4, 9, 3], [8, 2, 4, 3, 9], [3, 2, 4, 8, 9], [3, 2, 4, 8, 9], [2, 3, 4, 8, 9]"
    },
    {
        "question": "Explain prefix_max analysis for Selection Sort.",
        "answer": "Base case: for i = 0, array has one element, so index of max is i.\nInduction: assume correct for i, maximum is either the maximum of A[:i] or A[i], returns correct index in either case.\nS(1) = \u0398(1), S(n) = S(n \u2212 1) + \u0398(1).\nSubstitution: S(n) = \u0398(n), cn = \u0398(1) + c(n \u2212 1) \u21d2 1 = \u0398(1).\nRecurrence tree: chain of n nodes with \u0398(1) work per node, \\(\\sum_{i=0}^{n-1} 1 = \\Theta(n)\\)"
    },
    {
        "question": "Explain selection_sort analysis.",
        "answer": "Base case: for i = 0, array has one element so is sorted.\nInduction: assume correct for i, last number of a sorted output is a largest number of the array, and the algorithm puts one there; then A[:i] is sorted by induction.\n\u0422(1) = \u0398(1), T(n) = T(n \u2212 1) + \u0398(n).\nSubstitution: T(n) = \u0398(n\u00b2), cn\u00b2 = O(n) + c(n \u2212 1)\u00b2 \u21d2 c(2n \u2212 1) = \u0398(n).\nRecurrence tree: chain of n nodes with \u0398(i) work per node, \\(\\sum i = \\Theta(n^2)\\)"
    },
    {
        "question": "Explain Insertion Sort.",
        "answer": "Recursively sort prefix A[:i]. Sort prefix A[:i + 1] assuming that prefix A[:i] is sorted by repeated swaps. Example: [8, 2, 4, 9, 3], [2, 8, 4, 9, 3], [2, 4, 8, 9, 3], [2, 4, 8, 9, 3], [2, 3, 4, 8, 9]"
    },
    {
        "question": "Explain insert_last analysis.",
        "answer": "Base case: for i = 0, array has one element so is sorted.\nInduction: assume correct for i, if A[i] >= A[i \u2212 1], array is sorted; otherwise, swapping last two elements allows us to sort A[:i] by induction.\nS(1) = \u0398(1), S(n) = S(n \u2212 1) + \u0398(1) = S(n) = \u0398(\u03b7)"
    },
    {
        "question": "Explain insertion_sort analysis.",
        "answer": "Base case: for i = 0, array has one element so is sorted.\nInduction: assume correct for i, algorithm sorts A[:i] by induction, and then insert_last correctly sorts the rest as proved above.\n\u03a4(1) = \u0398(1), T(n) = T(n \u2212 1) + \u0398(n) = T(n) = \u0398(\u03b7\u00b2)"
    },
    {
        "question": "Explain Merge Sort.",
        "answer": "Recursively sort first half and second half (may assume power of two). Merge sorted halves into one sorted list (two finger algorithm). Example: [7, 1, 5, 6, 2, 4, 9, 3], [1, 7, 5, 6, 2, 4, 3, 9], [1, 5, 6, 7, 2, 3, 4, 9], [1, 2, 3, 4, 5, 6, 7, 9]"
    },
    {
        "question": "Explain merge analysis.",
        "answer": "Base case: for n = 0, arrays are empty, so vacuously correct.\nInduction: assume correct for n, item in A[r] must be a largest number from remaining prefixes of L and R, and since they are sorted, taking largest of last items suffices; remainder is merged by induction.\nS(0) = \u0398(1), S(n) = S(n \u2212 1) + \u0398(1) = S(n) = \u0398(\u03b7)"
    },
    {
        "question": "Explain merge_sort analysis.",
        "answer": "Base case: for n = 1, array has one element so is sorted.\nInduction: assume correct for k < n, algorithm sorts smaller halves by induction, and then merge merges into a sorted array as proved above.\n\u03a4(1) = \u0398(1), T(n) = 2T(n/2) + \u0398(\u03b7).\nSubstitution: Guess T(n) = O(n log n)\ncn log n = O(n) + 2c(n/2)log(n/2) \u21d2 cn log(2) = \u0398(n)\nRecurrence Tree: complete binary tree with depth log2 n and n leaves, level i has 2i nodes with O(n/2i) work each, total: \\(\\sum_{i=0}^{\\log_2 n} (2^i) (n/2^i) = \\sum_{i=0}^{\\log_2 n} n = O(n \\log n)\\)"
    },
    {
        "question": "What are the key points from the review?",
        "answer": "Comparison search lower bound: any decision tree with n nodes has height > [lg(n+1)]-1. Can do faster using random access indexing: an operation with linear branching factor! Direct access array is fast, but may use a lot of space (\u0398(u)). Solve space problem by mapping (hashing) key space u down to m = \u0398(n). Hash tables give expected O(1) time operations, amortized if dynamic. Expectation input-independent: choose hash function randomly from universal hash family. Data structure overview! Last time we achieved faster find. Can we also achieve faster sort?"
    },
    {
        "question": "Describe the operations and order for Array, Sorted Array, Direct Access Array, and Hash Table in terms of build(X), find(k), insert(x), delete(k), find_min(), find_max(), find_prev(k), and find_next(k).",
        "answer": "The image contains a table describing data structures and their operations. Here is the description:\n- Array: build(X) is n, find(k) is n, insert(x) is n, delete(k) is n, and order contains n\n- Sorted Array: build(X) is n log n, find(k) is log n, insert(x) is n, delete(k) is n, and order contains log n\n- Direct Access Array: build(X) is U, find(k) is 1, insert(x) is 1, delete(k) is 1, and order contains U\n- Hash Table: build(X) is n(e), find(k) is 1(e), insert(x) is 1(a)(e), delete(k) is n, and order contains n"
    },
    {
        "question": "Explain the Comparison Sort Lower Bound.",
        "answer": "Comparison model implies that algorithm decision tree is binary (constant branching factor). Requires # leaves L > # possible outputs. Tree height lower bounded by \u03a9(log L), so worst-case running time is O(log L). To sort array of n elements, # outputs is n! permutations. Thus height lower bounded by log(n!) \u2265 log((n/2)^{n/2}) = O(n log n). So merge sort is optimal in comparison model. Can we exploit a direct access array to sort faster?"
    },
    {
        "question": "Describe direct access array sort with example.",
        "answer": "Example: [5, 2, 7, 0, 4]. Suppose all keys are unique non-negative integers in range {0, . . ., u \u2212 1}, so n \u2264 u. Insert each item into a direct access array with size u in O(n). Return items in order they appear in direct access array in \u0398(u). Running time is \u0398(u), which is \u0472(n) if u = O(n). Yay!\nHere is the code:\ndef direct_access_sort(A):\n    \"Sort A assuming items have distinct non-negative keys\"\n    u = 1 + max([x.key for x in A])\n    D = [None] * u\n    for x in A:\n        D[x.key] = x\n    i = 0\n    for key in range(u):\n        if D[key] is not None:\n            A[i] = D[key]\n            i += 1"
    },
    {
        "question": "How to sort if keys are in larger range?",
        "answer": "If keys are in larger range, like u = \u03a9(n\u00b2) < n\u00b2: Represent each key k by tuple (a, b) where k = an + b and 0 \u2264 b < n. Specifically a = [k/n] < n and b = (k mod n) (just a 2-digit base-n number!). This is a built-in Python operation (a, b) = divmod(k, n). Example: [17, 3, 24, 22, 12] \u2192 [(3,2), (0,3), (4,4), (4,2), (2,2)] \u2192 [32, 03, 44, 42, 22](n=5). How can we sort tuples?"
    },
    {
        "question": "Explain Tuple Sort",
        "answer": "Item keys are tuples of equal length, i.e. item x.key = (x.k\u2081,x.k2, x.k2, . . .). Want to sort on all entries lexicographically, so first key k\u2081 is most significant. How to sort? Idea! Use other auxiliary sorting algorithms to separately sort each key. (Like sorting rows in a spreadsheet by multiple columns). What order to sort them in? Least significant to most significant! Exercise: [32, 03, 44, 42, 22] -> [42, 22, 32, 03, 44] -> [03, 22, 32, 42, 44](n=5). Idea! Use tuple sort with auxiliary direct access array sort to sort tuples (a, b). Problem! Many integers could have the same a or b value, even if input keys distinct. Need sort allowing repeated keys which preserves input order. Want sort to be stable: repeated keys appear in output in same order as input. Direct access array sort cannot even sort arrays having repeated keys! Can we modify direct access array sort to admit multiple keys in a way that is stable?"
    },
    {
        "question": "Describe Counting Sort",
        "answer": "Instead of storing a single item at each array index, store a chain, just like hashing! For stability, chain data structure should remember the order in which items were added. Use a sequence data structure which maintains insertion order. To insert item x, insert_last to end of the chain at index x.key. Then to sort, read through all chains in sequence order, returning items one by one.\nHere is the code:\ndef counting_sort(A):\n    \"Sort A assuming items have non-negative keys\"\n    u = 1 + max([x.key for x in A])\n    D = [[] for i in range(u)]\n    for x in A:\n        D[x.key].append(x)\n    i = 0\n    for chain in D:\n        for x in chain:\n            A[i] = x\n            i += 1"
    },
    {
        "question": "Explain Radix Sort",
        "answer": "Idea! If u < n\u00b2, use tuple sort with auxiliary counting sort to sort tuples (a, b). Sort least significant key b, then most significant key a. Stability ensures previous sorts stay sorted. Running time for this algorithm is O(2n) = O(n). Yay! If every key < nc for some positive c = logn (u), every key has at most e digits base n. A c-digit number can be written as a c-element tuple in O(c) time. We sort each of the c base-n digits in O(n) time. So tuple sort with auxiliary counting sort runs in O(cn) time in total. If c is constant, so each key is < nc, this sort is linear O(n)!\nHere is the code:\ndef radix_sort(A):\n    \"Sort A assuming items have non-negative keys\"\n    n = len (A)\n    u = 1 + max((x.key for x in A])\n    c = 1 + (u.bit_length() // n.bit_length())\n    class Obj: pass\n    D = [Obj() for a in A]\n    for i in range(n):\n        D[i].digits = []\n        D[i].item = A[i]\n        high = A[i].key\n        for j in range(c):\n            high, low = divmod (high, n)\n            D[i].digits.append(low)\n    for i in range(c):\n        for j in range(n):\n            D[j].key = D[j].digits[i]\n        counting_sort (D)\n    for i in range(n):\n        A[i] = D[i].item"
    },
    {
        "question": "Give a summary of algorithm in terms of time, in-place, stable and comments",
        "answer": "Here is the summary:\n- Insertion Sort: Time is $n^2$, In-place is Y, Stable is Y, Comment is O(nk) for k-proximate\n- Selection Sort: Time is $n^2$, In-place is Y, Stable is N, Comment is O(n) swaps\n- Merge Sort: Time is n log n, In-place is N, Stable is Y, Comment is stable, optimal comparison\n- Counting Sort: Time is n+u, In-place is N, Stable is Y, Comment is O(n) when u = O(n)\n- Radix Sort: Time is n+n logn (u), In-place is N, Stable is Y, Comment is O(n) when u = O(n)"
    },
    {
        "question": "What is a data structure and what supports it?",
        "answer": "A data structure is a way to store data, with algorithms that support operations on the data."
    },
    {
        "question": "What is an interface?",
        "answer": "Collection of supported operations is called an interface (also API or ADT). Interface is a specification: what operations are supported (the problem!)."
    },
    {
        "question": "What is data structure?",
        "answer": "Data structure is a representation: how operations are supported (the solution!)."
    },
    {
        "question": "What are the two main interfaces covered in this class?",
        "answer": "Sequence and Set"
    },
    {
        "question": "Describe Sequence Interface (L02, L07)",
        "answer": "Maintain a sequence of items (order is extrinsic). Ex: $(x_0, x_1, x_2, ..., x_{n-1})$ (zero indexing). Use $n$ to denote the number of items stored in the data structure. Supports sequence operations such as Container build(X), Static len(), iter_seq(), get_at(i), set_at(i, x), Dynamic insert_at(i, x), delete_at(i), insert_first(x), delete_first(), insert_last(x), delete_last(). Special case interfaces: stack insert_last(x) and delete_last(), queue insert_last(x) and delete_first()"
    },
    {
        "question": "Describe Set Interface (L03-L08).",
        "answer": "Sequence about extrinsic order, set is about intrinsic order. Maintain a set of items having unique keys (e.g., item \u00d7 has key x. key). (Set or multi-set? We restrict to unique keys for now.). Often we let key of an item be the item itself, but may want to store more info than just key. Supports set operations such as Container build(X), len(), Static find(k), Dynamic insert (x), delete (k), Order iter_ord(), find_min(), find_max(), find_next(k), find_prev(k). Special case interfaces: dictionary | set without the Order operations. In recitation, you will be asked to implement a Set, given a Sequence data structure."
    },
    {
        "question": "Describe Array Sequence.",
        "answer": "Array is great for static operations! get_at(i) and set_at(i, x) in O(1) time! But not so great at dynamic operations.... For consistency, we maintain the invariant that array is full. Then inserting and removing items requires reallocating the array and shifting all items after the modified item. The table shows operation and worst case O(\u00b7) for array. For array, build(X) is n, get_at(i) and set_at(i, x) is 1 and insert_first(x), delete_first(), insert_last (x),delete_last (),insert_at(i, x),delete_at(i) is n."
    },
    {
        "question": "Describe Linked List Sequence.",
        "answer": "Pointer data structure (this is not related to a Python \"list\"). Each item stored in a node which contains a pointer to the next node in sequence. Each node has two fields: node.item and node.next. Can manipulate nodes simply by relinking pointers! Maintain pointers to the first node in sequence (called the head). Can now insert and delete from the front in O(1) time! Yay! (Inserting/deleting efficiently from back is also possible; you will do this in PS1). But now get_at(i) and set_at(i, x) each take O(n) time... :( Can we get the best of both worlds? Yes! (Kind of...). The table shows operation and worst case O(\u00b7) for linked list. For linked list, build(X), get_at(i) and set_at(i, x) is n and insert_first(x), delete_first() is 1,insert_last (x),delete_last (),insert_at(i, x),delete_at(i) is n."
    },
    {
        "question": "Describe Dynamic Array Sequence.",
        "answer": "Make an array efficient for last dynamic operations. Python \"list\" is a dynamic array. Idea! Allocate extra space so reallocation does not occur with every dynamic operation. Fill ratio: 0 \u2264 r < 1 the ratio of items to space. Whenever array is full (r = 1), allocate \u0398(n) extra space at end to fill ratio $r_1$ (e.g., 1/2). Will have to insert O(n) items before the next reallocation. A single operation can take O(n) time for reallocation. However, any sequence of O(n) operations takes O(n) time. So each operation takes O(1) time \u201con average\u201d"
    },
    {
        "question": "Describe Amortized Analysis.",
        "answer": "Data structure analysis technique to distribute cost over many operations. Operation has amortized cost T(n) if k operations cost at most \u2264 kT(n). \u201cT(n) amortized\u201d roughly means T(n) \u201con average\u201d over many operations. Inserting into a dynamic array takes O(1) amortized time. More amortization analysis techniques in 6.046!"
    },
    {
        "question": "Describe Dynamic Array Deletion.",
        "answer": "Delete from back? O(1) time without effort, yay! However, can be very wasteful in space. Want size of data structure to stay \u0398(\u03b7). Attempt: if very empty, resize to r = 1. Alternating insertion and deletion could be bad.... Idea! When r < ra, resize array to ratio $r_i$ where $r_d$ < $r_i$ (e.g., $r_d$ = 1/4, $r_i$ = 1/2). Then \u0398(n) cheap operations must be made before next expensive resize. Can limit extra space usage to (1 + \u025b)n for any \u025b > 0 (set $r_d$ = $\\frac{1}{1+\\epsilon}$, $r_i$ = $\\frac{r_d+1}{2}$). Dynamic arrays only support dynamic last operations in O(1) time. Python List append and pop are amortized O(1) time, other operations can be O(n)! (Inserting/deleting efficiently from front is also possible; you will do this in PS1). The table shows operation and worst case O(\u00b7) for dynamic array. For dynamic array, build (X) is n, get_at(i) and set_at(i, x) is 1 and insert_first(x), delete_first(), insert_last (x),delete_last () is 1(a) ,insert_at(i, x),delete_at(i) is n."
    },
    {
        "question": "What is the input and output of the All-Pairs Shortest Paths (APSP) problem?",
        "answer": "Input: directed graph G = (V, E) with weights w : E \u2192 Z. Output: \u03b4(u, v) for all u, v \u2208 V, or abort if G contains negative-weight cycle."
    },
    {
        "question": "Explain Johnson's Algorithm approach.",
        "answer": "Make all edge weights non-negative while preserving shortest paths! i.e., reweight G to G' with no negative weights, where a shortest path in G is shortest in G'. If non-negative, then just run Dijkstra |V| times to solve APSP."
    },
    {
        "question": "How to make weights non-negative?",
        "answer": "Add h to all outgoing edges and subtract h from all incoming edges"
    },
    {
        "question": "How to find a potential function such that G' has no negative edge weights?",
        "answer": "is there an h such that w(u, v) + h(u) \u2013 h(v) \u2265 0 for every (u, v) \u2208 E? Re-arrange this condition to h(v) \u2264 h(u) + w(u, v), looks like triangle inequality! Condition would be satisfied if h(v) = \u03b4(s, v) and d(s, v) is finite for some s. Add a new vertex s with a directed 0-weight edge to every v \u2208 V! :)"
    },
    {
        "question": "What are the steps of Johnson's Algorithm?",
        "answer": "Construct G\u2081 from G by adding vertex x connected to each vertex v \u2208 V with 0-weight edge. Compute \u03b4\u03b1(x, v) for every v \u2208 V (using Bellman-Ford). If \u03b4x(x, v) = \u2212\u221e for any v \u2208 V: Abort (since there is a negative-weight cycle in G). Else: Reweight each edge w'(u, v) = w(u, v) + \u03b4x(x, u) \u2013 \u03b4x(x, v) to form graph G'. For each u \u2208 V: Compute shortest-path distances \u03b4'(u, v) to all v in G' (using Dijkstra), Compute \u03b4(u, v) = \u03b4'(u, v) \u2013 \u03b4x(x, u) + \u03b4x(x, v) for all v \u2208 V"
    },
    {
        "question": "What is the running time of Johnson's Algorithm?",
        "answer": "O(|V| + |E|) time to construct Gr, O(|V||E|) time for Bellman-Ford, O(|V| + |E|) time to construct G', O(|V|\u00b7 (|V|log |V| + |E|)) time for |V| runs of Dijkstra, O(|V|\u00b2) time to compute distances in G from distances in G', O(|V|\u00b2 log |V| + |V||E|) time in total"
    },
    {
        "question": "Show a table of SSSP algorithm restrictions, weights, name and running time.",
        "answer": "Here is the table:Graph General, Weights Unweighted, Name BFS, Running Time O(|V| + |E|). Graph DAG, Weights Any, Name DAG Relaxation, Running Time O(|V| + |E|). Graph General, Weights Non-negative, Name Dijkstra, Running Time O(|V| log |V| + |E|). Graph General, Weights Any, Name Bellman-Ford, Running Time O(|V|\u00b7 |E|)."
    },
    {
        "question": "How to solve an algorithms problem?",
        "answer": "Reduce to a problem you already know, and Design your own recursive algorithm."
    },
    {
        "question": "List some Search Data Structures, Sort Algorithms and Graph Algorithms.",
        "answer": "Search Data Structures: Array, Linked List, Dynamic Array, Sorted Array, Direct-Access Array, Hash Table, AVL Tree, Binary Heap\nSort Algorithms: Insertion Sort, Selection Sort, Merge Sort, Counting Sort, Radix Sort, AVL Sort, Heap Sort\nGraph Algorithms: Breadth First Search, DAG Relaxation (DFS + Topo), Dijkstra, Bellman-Ford, Johnson"
    },
    {
        "question": "What are the steps to solve a problem recursively (SRT BOT)?",
        "answer": "1. Subproblem definition, 2. Relate subproblem solutions recursively, 3. Topological order on subproblems (\u21d2 subproblem DAG), 4. Base cases of relation, 5. Original problem solution via subproblem(s), 6. Time analysis"
    },
    {
        "question": "How to express Merge sorting an array A of n elements in SRT BOT?",
        "answer": "- Subproblems: S(i, j) = sorted array on elements of A[i : j] for 0 \u2264 i \u2264 j \u2264 n\n- Relation: S(i, j) = merge(S(i, m), S(m, j)) where m = [(i + j)/2]\n- Topo. order: Increasing j \u2013 i\n- Base cases: S(i, i + 1) = [A[i]]\n- Original: S(0, n)\n- Time: T(n) = 2T(n/2) + O(n) = O(nlgn)\n- In this case, subproblem DAG is a tree (divide & conquer)"
    },
    {
        "question": "How to compute the nth Fibonacci number \\(F_n\\)?",
        "answer": "- Subproblems: \\(F(i)\\) = the ith Fibonacci number \\(F_i\\) for \\(i \\in {0, 1, ..., n}\\)\n- Relation: \\(F(i) = F(i - 1) + F(i \u2013 2)\\) (definition of Fibonacci numbers)\n- Topo. order: Increasing i\n- Base cases: \\(F(0) = 0, F(1) = 1\\)\n- Original prob.: F(n)"
    },
    {
        "question": "What is dynamic programming?",
        "answer": "Dynamic programming is used when subproblem dependencies overlap (DAG, in-degree > 1).\n* \"Recurse but re-use\u201d (Top down: record and lookup subproblem solutions)\n* \"Careful brute force\u201d (Bottom up: do each subproblem in order)\nOften useful for counting/optimization problems: almost trivially correct recurrences"
    },
    {
        "question": "How to solve a problem recursively (SRT BOT)?",
        "answer": "1. Subproblem definition: subproblem x \u2208 X. Describe the meaning of a subproblem in words, in terms of parameters. Often subsets of input: prefixes, suffixes, contiguous substrings of a sequence. Often record partial state: add subproblems by incrementing some auxiliary variables\n2. Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i\n3. Topological order to argue relation is acyclic and subproblems form a DAG\n4. Base cases: State solutions for all (reachable) independent subproblems where relation breaks down\n5. Original problem: Show how to compute solution to original problem from solutions to subproblem(s). Possibly use parent pointers to recover actual solution, not just objective function\n6. Time analysis:\n\u03a3\u03c0\u03b5\u03c7 work(x), or if work(x) = O(W) for all x \u2208 X, then |X|\u00b7 O(W). work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time"
    },
    {
        "question": "What are the steps to find the DAG Shortest Paths",
        "answer": "- Subproblems: \\(\u03b4(s, v)\\) for all \\(v \\in V\\)\n- Relation: \\(\u03b4(s, v) = min{d(s, u) + w(u, v) | u \\in Adj^-(v)} \u222a {\u221e}\\)\n- Topo. order: Topological order of G\n- Base cases: \\(\u03b4(s, s) = 0\\)\n- Original: All subproblems\n- Time: \\(\u03a3\u03c5\u03b5\u03bd 0(1+ | Adj^-(v)|) = O(|V| + |E|)\\)"
    },
    {
        "question": "Describe the bowling problem",
        "answer": "- Given n pins labeled 0, 1, ..., \u03b7 1\n- Pin i has value vi\n- Ball of size similar to pin can hit either\n  * 1 pin i, in which case we get vi points\n  * 2 adjacent pins i and i + 1, in which case we get vi \u00b7 Vi+1 points\n- Once a pin is hit, it can't be hit again (removed)\n- Problem: Throw zero or more balls to maximize total points\n- Example: [-1, 1, 1, 1, 9,9,3, -3, -5, 2, 2"
    },
    {
        "question": "What is height balance in AVL trees?",
        "answer": "AVL trees maintain height-balance, also called the AVL Property, where a node is height-balanced if the heights of its left and right subtrees differ by at most 1. The skew of a node is defined as the height of its right subtree minus that of its left subtree, with a node being height-balanced if its skew is -1, 0, or 1."
    },
    {
        "question": "How does rotation work to maintain height?",
        "answer": "Rotations can reduce the height of the tree without changing its traversal order. A rotation relinks O(1) pointers to modify the tree structure and maintains traversal order. O(n) rotations can transform a binary tree to any other with the same traversal order. After adding or removing leaf from a height-balanced tree resulting in imbalance, fix height-balance of ancestors starting from leaf up to the root, and repeatedly rebalance lowest ancestor that is not height-balanced."
    },
    {
        "question": "Can you give formula to calculate minimum number of nodes for any given height?",
        "answer": "A binary tree with height-balanced nodes has height h = O(log n) (i.e., n = 2^{\u03a9(h)}). The fewest nodes F(h) in any height h tree is F(h) = 2^{0(h)}. Then, F(0) = 1, F(1) = 2, F(h) = 1 + F(h-1) + F(h-2) \u2265 2F(h-2) => F(h) \u2265 2^{h/2}"
    },
    {
        "question": "How to compute height of a binary tree?",
        "answer": "To determine if a node is height-balanced, compute the heights of its subtrees. Augment each node with the height of its subtree, and the height of a node can be computed in O(1) time from the heights of its children. During dynamic operations, maintain the augmentation by recomputing subtree augmentations at every node whose subtree changes."
    },
    {
        "question": "What are the steps to augment a binary tree?",
        "answer": "To augment a binary tree with a subtree property P, state the subtree property P(<X>) you want to store at each node <X>. Show how to compute P(<X>) from the augmentations of <X>'s children in O(1) time. The stored property P(<X>) can be maintained without changing dynamic operation costs."
    },
    {
        "question": "What is the time complexity to achieve by the AVL tree?",
        "answer": "Set AVL trees achieve O(log n) time for all set operations, except O(n log n) time for build and O(n) time for iter. Sequence AVL trees achieve O(log n) time for all sequence operations, except O(n) time for build and iter"
    },
    {
        "question": "Can you briefly explain the AVL sort?",
        "answer": "Any Set data structure defines a sorting algorithm: build (or repeatedly insert) then iter. AVL Sort is a new O(n log n)-time sorting algorithm"
    },
    {
        "question": "What are the goals of 6.006: Introduction to Algorithms?",
        "answer": "1. Solve hard computational problems (with non-constant-sized inputs)\n2. Argue an algorithm is correct (Induction, Recursion)\n3. Argue an algorithm is \u201cgood\u201d (Asymptotics, Model of Computation) - (effectively communicate all three above, to human or computer)"
    },
    {
        "question": "Do there always exist \u201cgood\u201d algorithms?",
        "answer": "Most problems are not solvable efficiently, but many we think of are!\nPolynomial means polynomial in size of input\nPseudopolynomial means polynomial in size of input AND size of numbers in input\nNP: Nondeterministic Polynomial time, polynomially checkable certificates\nNP-hard: set of problems that can be used to solve any problem in NP in poly-time\nNP-complete: intersection of NP-hard and NP"
    },
    {
        "question": "How to solve an algorithms problem?",
        "answer": "Reduce to a problem you know how to solve\nSearch/Sort (Q1)\n* Search: Extrinsic (Sequence) and Intrinsic (Set) Data Structures\n* Sort: Comparison Model, Stability, In-place\nGraphs (Q2)\n* Reachability, Connected Components, Cycle Detection, Topological Sort\n* Single-Source / All-Pairs Shortest Paths\nDesign a new recursive algorithm\nBrute Force\nDivide & Conquer\nDynamic Programming (Q3)\nGreedy/Incremental"
    },
    {
        "question": "What are the next steps after 6.006?",
        "answer": "(U) 6.046: Design & Analysis of Algorithms\n(G) 6.851: Advanced Data Structures\n(G) 6.854: Advanced Algorithms"
    },
    {
        "question": "What does 6.046 Extension of 6.006 cover?",
        "answer": "Data Structures: Union-Find, Amortization via potential analysis\nGraphs: Minimum Spanning Trees, Network Flows/Cuts\nAlgorithm Design (Paradigms): Divide & Conquer, Dynamic Programming, Greedy\nComplexity: Reductions"
    },
    {
        "question": "What are the concepts related to randomized algorithms?",
        "answer": "6.006 mostly deterministic (hashing)\nLas Vegas: always correct, probably fast (like hashing)\nMonte Carlo: always fast, probably correct\nCan generally get faster randomized algorithms on structured data"
    },
    {
        "question": "What are the concepts related to Numerical Algorithms/Continuous Optimization?",
        "answer": "6.006 only deals with integers\nApproximate real numbers! Pay time for precision"
    },
    {
        "question": "What are the concepts related to Approximation Algorithms?",
        "answer": "Input optimization problem (min/max over weighted outputs)\nMany optimization problems NP-hard\nHow close can we get to an optimal solution in polynomial time?"
    },
    {
        "question": "What are the concepts related to Change Model of Computation?",
        "answer": "Cache Models (memory hierarchy cost model)\nQuantum Computer (exploiting quantum properties)\nParallel Processors (use multiple CPUs instead of just one)\n* Multicore, large shared memory\n* Distributed cores, message passing"
    },
    {
        "question": "What are the future courses related to Model and Application?",
        "answer": "Model:\nComputation / Complexity (6.045, 6.840, 6.841)\nRandomness (6.842)\nQuantum (6.845)\nDistributed / message passing (6.852)\nMulticore / shared memory (6.816, 6.846)\nGraph and Matrix (6.890)\nConstant Factors / Performance (6.172)\nApplication:\nBiology (6.047)\nGame Theory (6.853)\nCryptography (6.875)\nVision (6.819)\nGraphics (6.837)\nGeometry (6.850)\nFolding (6.849)"
    },
    {
        "question": "What is a weighted graph?",
        "answer": "A weighted graph is a graph G = (V, E) together with a weight function \\(w : E \\rightarrow Z\\) where each edge \\(e = (u, v) \\in E\\) is assigned an integer weight \\(w(e) = w(u, v)\\)."
    },
    {
        "question": "What is the weight \\(w(\\pi)\\) of a path \\(\\pi\\) in a weighted graph?",
        "answer": "The weight \\(w(\\pi)\\) of a path \\(\\pi\\) in a weighted graph is the sum of the weights of the edges in the path."
    },
    {
        "question": "What is the (weighted) shortest path from \\(s \\in V\\) to \\(t \\in V\\)?",
        "answer": "The (weighted) shortest path from \\(s \\in V\\) to \\(t \\in V\\) is the path of minimum weight from s to t.  \\(\\delta(s,t) = inf\\{w(\\pi) | path \\\\pi from s to t\\}\\) is the shortest-path weight from s to t."
    },
    {
        "question": "What is a negative-weight cycle?",
        "answer": "A negative-weight cycle is a path \\(\\pi\\) starting and ending at the same vertex with \\(w(\\pi) < 0\\)."
    },
    {
        "question": "What are the algorithms to find shortest-path weights in weighted graphs, their restrictions, running time and Lecture number?",
        "answer": "The algorithms are:\n1. BFS, applies to unweighted graphs, runs in \\(O(|V|+|E|)\\) time, and discussed in Lecture L09.\n2. DAG Relaxation, applies to DAGs with any weights, runs in \\(O(|V| + |E|)\\) time, and discussed in Lecture L11.\n3. Bellman-Ford, applies to general graphs with any weights, runs in \\(O(|V| \\cdot |E|)\\) time, and discussed in Lecture L12.\n4. Dijkstra, applies to general graphs with non-negative weights, runs in \\(O(|V| log |V| + |E|)\\) time, and discussed in Lecture L13."
    },
    {
        "question": "How can a shortest-paths tree be constructed?",
        "answer": "If \\(\\delta(s, v)\\) is known for all vertices \\(v \\in V\\), a shortest-path tree can be constructed in \\(O(|V| + |E|)\\) time by keeping track of parent pointers during the search."
    },
    {
        "question": "What is the triangle inequality?",
        "answer": "The shortest-path weight from u to v cannot be greater than the shortest path from u to v through another vertex x, i.e., \\(\\delta(u, v) \\leq \\delta(u, x) + \\delta(x, v)\\) for all u, v, x \\(\\in\\) V"
    },
    {
        "question": "What is DAG Relaxation?",
        "answer": "DAG Relaxation maintains a distance estimate d(s, v) (initially \u221e) for each vertex \\(v \\in V\\), that always upper bounds true distance \\(\\delta(s, v)\\), then gradually lowers until d(s, v) = \\(\\delta(s, v)\\). This is done by processing each vertex u in a topological sort order of G and relaxing the edge (u,v) for each outgoing neighbor \\(v \\in Adj^+(u)\\)."
    },
    {
        "question": "What are the build, find, insert/delete, and find_min/find_max/find_prev/find_next operation orders for an array?",
        "answer": "For an array: build is O(n), find is O(n), insert/delete is O(n), and find_min/find_max/find_prev/find_next is O(n)."
    },
    {
        "question": "What are the build, find, insert/delete, and find_min/find_max/find_prev/find_next operation orders for a sorted array?",
        "answer": "For a sorted array: build is O(n log n), find is O(log n), insert/delete is O(n), and find_min/find_max/find_prev/find_next is O(log n)."
    },
    {
        "question": "What are some key concepts in the Comparison Model?",
        "answer": "Algorithms differentiate items via comparisons. Only comparisons between pairs of Comparable items are supported. Comparisons (<, \u2264, >, \u2265, =, \u2260) output True or False. The goal is to store n comparable items, supporting find(k). Running time is lower bounded by the number of comparisons."
    },
    {
        "question": "What are some key concepts in the Decision Tree model?",
        "answer": "Algorithms are viewed as a decision tree. An internal node is a binary comparison, branching True/False. For comparison algorithms, the tree is binary. A leaf represents algorithm termination with an output. A root-to-leaf path is an execution of the algorithm. At least one leaf is needed for each algorithm output, so search requires > n + 1 leaves."
    },
    {
        "question": "What are some key aspects in the Comparison Search Lower Bound?",
        "answer": "Worst-case running time \u2265 # comparisons \u2265 max length of any root-to-leaf path \u2265 height of tree. Minimum height of a binary tree on > n nodes is addressed. Height \u2265 [lg(n + 1)] \u2212 1 = \u03a9(log n). Sorted arrays achieve this bound. Height of tree with O(n) leaves and max branching factor b is \u03a9(logb n). To get faster, need an operation that allows super-constant \u03c9(1) branching factor."
    },
    {
        "question": "What is Direct Access Array, and how is it used?",
        "answer": "Direct Access Array exploits Word-RAM O(1) time random access indexing. Give item unique integer key k in {0, . . ., u \u2013 1}, store item in array at index k. Associate meaning with each index of array. If keys fit in machine word (u \u2264 2w), worst-case O(1) find/dynamic operations. 6.006 assumes input numbers/strings fit in word, unless length is explicitly parameterized. Anything in computer memory is a binary integer, or use (static) 64-bit address. But space O(u), so really bad if n \u226a u. Example: ten-letter names require 2610 \u2248 17.6 TB space."
    },
    {
        "question": "What is the Idea of Hashing?",
        "answer": "If n < u, map keys to a smaller range m = O(n) and use smaller direct access array. Hash function: h(k) : {0, . . ., u \u2212 1} \u2192 {0, . . ., m \u2212 1} (also hash map). Direct access array called hash table, h(k) called hash of key k. If m \u226a u, no hash function is injective by pigeonhole principle."
    },
    {
        "question": "What happens when there is Collision during hashing?",
        "answer": "Always exists keys a, b such that h(a) = h(b) \u2192 Collision!. Can't store both items at the same index, so store elsewhere in the array (open addressing - complicated analysis, but common and practical) or in another data structure supporting dynamic set interface (chaining)."
    },
    {
        "question": "What is Chaining?",
        "answer": "Store collisions in another data structure (a chain). If keys are evenly distributed, chain size is n/m = n/\u03a9(n) = O(1)!. If chain has O(1) size, all operations take O(1) time!. Need a good hash function. If not, many items may map to same location, e.g. h(k) = constant, chain size is \u0398(n)."
    },
    {
        "question": "What is the division method for hash functions?",
        "answer": "h(k) = (k mod m). It's heuristic, good when keys are uniformly distributed! m should avoid symmetries of the stored keys. Large primes far from powers of 2 and 10 can be reasonable. If u \u226b n, every hash function will have some input set that will a create O(n) size chain. Don't use a fixed hash function! Choose one randomly (but carefully)!"
    },
    {
        "question": "What is Universal hashing?",
        "answer": "hab(k) = (((ak + b) mod p) mod m). Hash Family H(p, m) = {hab | a, b \u2208 {0, . . ., p \u2212 1} and a \u2260 0}. Parameterized by a fixed prime p > u, with a and b chosen from range {0, . . ., p \u2212 1}. H is a Universal family: Prh\u2208H {h(ki) = h(kj)} < 1/m \u2200ki \u2260 kj \u2208 {0,..., u \u2013 1}. It implies short chain lengths! (in expectation)"
    },
    {
        "question": "What are the build, find, insert/delete, and find_min/find_max/find_prev/find_next operation orders for a Hash Table?",
        "answer": "For a Hash Table: build is O(n(e)), find is O(1(e)), insert/delete is O(1(a)(e)), and find_min/find_max/find_prev/find_next is O(n)."
    },
    {
        "question": "What is Single-Source Shortest Paths (SSSP) on weighted graphs?",
        "answer": "Algorithms with O(|V| + |E|) time for small positive weights or DAGS were previously discussed. Bellman-Ford, O(|V||E|) time algorithm for general graphs with negative weights was discussed last time. Today: faster for general graphs with non-negative edge weights, i.e., for e \u2208 E, w(e) \u2265 0"
    },
    {
        "question": "How does Dijkstra's algorithm generalize BFS approach to weighted graphs?",
        "answer": "It grows a sphere centered at source s, repeatedly exploring closer vertices before further ones."
    },
    {
        "question": "What is observation 1 related to weights?",
        "answer": "If weights are non-negative, monotonic distance increase along shortest paths i.e., if vertex u appears on a shortest path from s to v, then d(s, u) \u2264 \u03b4(s, v). Let Vx \u2286 V be the subset of vertices reachable within distance \u2264 x from s. If v \u2208 Vx, then any shortest path from s to v only contains vertices from Vx. Perhaps grow Vx one vertex at a time! (but growing for every x is slow if weights large)"
    },
    {
        "question": "What is observation 2 about SSSP?",
        "answer": "SSSP can be solved fast if given order of vertices in increasing distance from s. Remove edges that go against this order (since cannot participate in shortest paths). May still have cycles if zero-weight edges: repeatedly collapse into single vertices. Compute \u03b4(s, v) for each v \u2208 V using DAG relaxation in O(|V| + |E|) time."
    },
    {
        "question": "Who is Dijkstra's Algorithm named after?",
        "answer": "Edsger Dijkstra (actually D\u00ffkstra!)"
    },
    {
        "question": "What are the key ideas behind Dijkstra's Algorithm?",
        "answer": "Relax edges from each vertex in increasing order of distance from source s, and efficiently find next vertex in the order using a data structure."
    },
    {
        "question": "What is the interface for Changeable Priority Queue Q on items with keys and unique IDs, supporting operations?",
        "answer": "Q.build(X): initialize Q with items in iterator X. Q.delete_min(): remove an item with minimum key. Q.decrease_key(id, k): find stored item with ID id and change key to k."
    },
    {
        "question": "How do you Implement a Priority Queue?",
        "answer": "Implement by cross-linking a Priority Queue Q' and a Dictionary D mapping IDs into Q'. Assume vertex IDs are integers from 0 to |V| - 1 so can use a direct access array for D. For brevity, say item x is the tuple (x.id, x.key)."
    },
    {
        "question": "How to perform Dijkstra algorithm?",
        "answer": "Set d(s, v) = \u221e for all v \u2208 V, then set d(s, s) = 0. Build changeable priority queue Q with an item (v, d(s, v)) for each vertex v \u2208 V. While Q not empty, delete an item (u, d(s, u)) from Q that has minimum d(s, u). For vertex v in outgoing adjacencies Adj+(u):\n    * If d(s, v) > d(s, u) + w(u, v):\n        - Relax edge (u, v), i.e., set d(s, v) = d(s, u) + w(u, v)\n        - Decrease the key of v in Q to new estimate d(s, v)"
    },
    {
        "question": "Describe the graph included in the document.",
        "answer": "The graph contains the following nodes: s, a, b, c, d. The edge weights are labeled as follows:\n- s to a: 2\n- s to c: 3\n- c to d: 2\n- d to b: 5\n- a to d: 5\n- s to d: 7\n- c to a: 8\n- a to c: 1\n- s to a: 1"
    },
    {
        "question": "What is the claim for the correctness of Dijkstra's algorithm?",
        "answer": "At end of Dijkstra's algorithm, d(s, v) = \u03b4(s, v) for all v \u2208 V"
    },
    {
        "question": "What is the proof for the correctness of Dijkstra's algorithm?",
        "answer": "If relaxation sets d(s, y) to \u03b4(s, y), then d(s, v) = \u03b4(s, v) at the end of the algorithm. Suffices to show d(s, v) = \u03b4(s, v) when vertex v is removed from Q. Proof by induction on first k vertices removed from Q.\nBase Case (k = 1): s is first vertex removed from Q, and d(s, s) = 0 = \u03b4(s, s). Inductive Step: Assume true for k < k', consider k'th vertex v' removed from Q. Consider some shortest path \u03c0 from s to v', with w(\u03c0) = \u03b4(s, v'). Let (x, y) be the first edge in \u03c0 where y is not among first k' - 1 (perhaps y = v'). When x was removed from Q, d(s, x) = \u03b4(s, x) by induction, so:\nd(s, y) \u2264 d(s, x) + w(x, y) = \u03b4(s, x) \u2264 \u03b4(s, v') \u2264 d(s, v') \u2264 d(s, y). So d(s, v') = \u03b4(s, v'), as desired"
    },
    {
        "question": "How is running time calculated for the algorithm?",
        "answer": "Count operations on changeable priority queue Q, assuming it contains n items: Q.build(X) (n = |X|): Bn (1), Q.delete_min(): Mn (|V|), Q.decrease_key(id, k): Dn (|E|). Total running time is O(B|V| + |V| \u00b7 M|V| + |E| \u00b7 D|V|). Assume pruned graph to search only vertices reachable from the source, so |V| = O(|E|)"
    },
    {
        "question": "What is the time complexity of Dijkstra's algorithm using different priority queue implementations?",
        "answer": "Using an Array: O(|V|^2); Binary Heap: O(|E| log |V|); Fibonacci Heap: O(|E| + |V| log |V|)"
    },
    {
        "question": "What is the significance of graph density on algorithm performance?",
        "answer": "If graph is dense, i.e., |E| = \u0398(|V|^2), using an Array for Q' yields O(|V|^2) time. If graph is sparse, i.e., |E| = \u0398(|V|), using a Binary Heap for Q' yields O(|V| log |V|) time."
    },
    {
        "question": "What is the summary for the weighted single-source shortest paths algorithms based on graph type?",
        "answer": "For General Unweighted graphs, BFS takes O(|V| + |E|) time. For DAGs, DAG Relaxation takes O(|V| + |E|) time. For General Non-negative graphs, Dijkstra takes O(|V|log |V| + |E|) time. For General graphs, Bellman-Ford takes O(|V|\u00b7 |E|) time."
    },
    {
        "question": "What is the complexity of doing a SSSP algorithm |V| times?",
        "answer": "Doing a SSSP algorithm |V| times is actually pretty good, since output has size O(|V|^2). It is possible to do better than |V| \u00b7 O(|V|\u00b7 |E|) for general graphs with negative weights (next time!)."
    },
    {
        "question": "What topics were previously covered?",
        "answer": "Graph definitions (directed/undirected, simple, neighbors, degree); Graph representations (Set mapping vertices to adjacency lists); Paths and simple paths, path length, distance, shortest path; Graph Path Problems (Single_Pair_Reachability(G,s,t), Single_Source_Reachability(G,s), Single_Pair_Shortest_Path(G,s,t), Single_Source_Shortest_Paths(G,s) (SSSP)); Breadth-First Search (BFS) - algorithm that solves Single Source Shortest Paths with appropriate data structures, runs in O(|V| + |E|) time (linear in input size)."
    },
    {
        "question": "What is Depth-First Search (DFS)?",
        "answer": "Searches a graph from a vertex s, similar to BFS; Solves Single Source Reachability, not SSSP. Useful for solving other problems (later!); Return (not necessarily shortest) parent tree of parent pointers back to s; Idea! Visit outgoing adjacencies recursively, but never revisit a vertex; i.e., follow any path until you get stuck, backtrack until finding an unexplored path to explore; P(s) = None, then run visit(s), where visit(u) : for every v \u2208 Adj(u) that does not appear in P: set P(v) = u and recursively call visit(v) (DFS finishes visiting vertex u, for use later!)"
    },
    {
        "question": "What is the correctness claim and proof for DFS?",
        "answer": "Claim: DFS visits v and correctly sets P(v) for every vertex v reachable from s; Proof: induct on k, for claim on only vertices within distance k from s; Base case (k = 0): P(s) is set correctly for s and s is visited; Inductive step: Consider vertex v with \u03b4(s, v) = k' + 1; Consider vertex u, the second to last vertex on some shortest path from s to v; By induction, since d(s, u) = k', DFS visits u and sets P(u) correctly; While visiting u, DFS considers v \u2208 Adj(u); Either v is in P, so has already been visited, or v will be visited while visiting u; In either case, v will be visited by DFS and will be added correctly to P."
    },
    {
        "question": "What is the running time of DFS?",
        "answer": "Algorithm visits each vertex u at most once and spends O(1) time for each v \u2208 Adj(u); Work upper bounded by O(1) \u00d7 \u03a3v\u2208V deg(u) = O(|E|); Unlike BFS, not returning a distance for each vertex, so DFS runs in O(|E|) time."
    },
    {
        "question": "What is Full-BFS and Full-DFS?",
        "answer": "Suppose want to explore entire graph, not just vertices reachable from one vertex; Idea! Repeat a graph search algorithm A on any unvisited vertex; Repeat the following until all vertices have been visited: Choose an arbitrary unvisited vertex s, use A to explore all vertices reachable from s; We call this algorithm Full-A, specifically Full-BFS or Full-DFS if A is BFS or DFS; Visits every vertex once, so both Full-BFS and Full-DFS run in O(|V| + |E|) time."
    },
    {
        "question": "Explain Graph Connectivity.",
        "answer": "An undirected graph is connected if there is a path connecting every pair of vertices; In a directed graph, vertex u may be reachable from v, but v may not be reachable from u; Connectivity is more complicated for directed graphs (we won't discuss in this class); Connectivity(G): is undirected graph G connected?; Connected_Components(G): given undirected graph G = (V, E), return partition of V into subsets Vi \u2286 V (connected components) where each V\u00bf is connected in G and there are no edges between vertices from different connected components; Consider a graph algorithm A that solves Single Source Reachability; Claim: A can be used to solve Connected Components; Proof: Run Full-A. For each run of A, put visited vertices in a connected component."
    },
    {
        "question": "Explain Topological Sort.",
        "answer": "A Directed Acyclic Graph (DAG) is a directed graph that contains no directed cycle; A Topological Order of a graph G = (V, E) is an ordering f on the vertices such that: every edge (u, v) \u2208 E satisfies f(u) < f(v); Exercise: Prove that a directed graph admits a topological ordering if and only if it is a DAG; How to find a topological order?; A Finishing Order is the order in which a Full-DFS finishes visiting each vertex in G; Claim: If G = (V, E) is a DAG, the reverse of a finishing order is a topological order; Proof: Need to prove, for every edge (u, v) \u2208 E that u is ordered before v, i.e., the visit to v finishes before visiting u. Two cases: If u visited before v: Before visit to u finishes, will visit v (via (u, v) or otherwise); Thus the visit to v finishes before visiting u; If v visited before u: u can't be reached from v since graph is acyclic; Thus the visit to v finishes before visiting u."
    },
    {
        "question": "Explain Cycle Detection.",
        "answer": "Full-DFS will find a topological order if a graph G = (V, E) is acyclic; If reverse finishing order for Full-DFS is not a topological order, then G must contain a cycle; Check if G is acyclic: for each edge (u, v), check if v is before u in reverse finishing order; Can be done in O(|E|) time via a hash table or direct access array; To return such a cycle, maintain the set of ancestors along the path back to s in Full-DFS; Claim: If G contains a cycle, Full-DFS will traverse an edge from v to an ancestor of v.; Proof: Consider a cycle (vo, U1, . . ., Uk, Vo) in G; Without loss of generality, let vo be the first vertex visited by Full-DFS on the cycle; For each vi, before visit to vi finishes, will visit vi+1 and finish; Will consider edge (vi, Vi+1), and if vi+1 has not been visited, it will be visited now; Thus, before visit to vo finishes, will visit vk (for the first time, by vo assumption); So, before visit to vk finishes, will consider (vk, vo), where vo is an ancestor of Uk."
    },
    {
        "question": "What is a decision problem?",
        "answer": "A decision problem is an assignment of inputs to YES (1) or NO (0). Inputs are either NO inputs or YES inputs."
    },
    {
        "question": "What are some examples of decision problems and their decisions?",
        "answer": "s-t Shortest Path: Does a given G contain a path from s to t with weight at most d?\nNegative Cycle: Does a given G contain a negative weight cycle?\nLongest Simple Path: Does a given G contain a simple path with weight at least d?\nSubset Sum: Does a given set of integers A contain a subset with sum S?\nTetris: Can you survive a given sequence of pieces in given board?\nChess: Can a player force a win from a given board?\nHalting problem: Does a given computer program terminate for a given input?"
    },
    {
        "question": "What is an Algorithm/Program and what is a decidable problem?",
        "answer": "Algorithm/Program: constant-length code (working on a word-RAM with (log n)-bit words) to solve a problem, i.e., it produces correct output for every input and the length of the code is independent of the instance size.\nProblem is decidable if there exists a program to solve the problem in finite time"
    },
    {
        "question": "Explain the concept of Decidability.",
        "answer": "Program is a finite (constant) string of bits, i.e., a nonnegative integer \u2208 N. Problem is function p : N \u2192 {0, 1}, i.e., infinite string of bits.\n(# of programs |N|, countably infinite) \u226a (# of problems |R|, uncountably infinite)\nProves that most decision problems not solvable by any program (undecidable)\nE.g., the Halting problem is undecidable (many awesome proofs in 6.045)\nFortunately most problems we think of are algorithmic in structure and are decidable"
    },
    {
        "question": "Explain Decidable Decision Problems and what are R, EXP, and P. How are they related?",
        "answer": "R problems decidable in finite time ('R' comes from recursive languages)\nEXP problems decidable in exponential time 2^{n^{O(1)}} (most problems we think of are here)\nP problems decidable in polynomial time n^{O(1)} (efficient algorithms, the focus of this class)\nThese sets are distinct, i.e., P\u2286 EXP \u2286 R (via time hierarchy theorems, see 6.045)\nE.g., Chess is in EXP \\ P"
    },
    {
        "question": "Explain Nondeterministic Polynomial Time (NP).",
        "answer": "P is the set of decision problems for which there is an algorithm A such that, for every input I of size n, A on I runs in poly(n) time and solves I correctly\nNP is the set of decision problems for which there is a verification algorithm V that takes as input an input I of the problem and a certificate bit string of length polynomial in the size of I, so that:\nV always runs in time polynomial in the size of I;\nif I is a YES input, then there is some certificate c so that V outputs YES on input (I, c);\nand\nif I is a NO input, then no matter what certificate c we choose, V always output NO on input (I, c).\nYou can think of the certificate as a proof that I is a YES input.\nIf I is actually a NO input, then no proof should work."
    },
    {
        "question": "What is the certificate and Verifier of s-t Shortest Path, Negative Cycle, Longest Simple Path, Subset Sum and Tetris?",
        "answer": "s-t Shortest Path: Certificate: A path P from s to t, Verifier: Adds the weights on P and checks whether \u2264 d\nNegative Cycle: Certificate: A cycle C, Verifier: Adds the weights on C and checks whether < 0\nLongest Simple Path: Certificate: A path P, Verifier: Checks whether P is a simple path with weight \u2265 d\nSubset Sum: Certificate: A set of items A', Verifier: Checks whether A' \u2208 A has sum S\nTetris: Certificate: Sequence of moves, Verifier: Checks that the moves allow survival"
    },
    {
        "question": "What are the known facts about P, NP, and EXP relationships?",
        "answer": "P \u2286 NP: The verifier V just solves the instance ignoring any certificate\nNP \u2286 EXP: Try all possible certificates! At most 2^{n^{O(1)}} of them, run verifier V on all"
    },
    {
        "question": "What are the open problems with P, NP, and EXP relationships?",
        "answer": "Open: Does P = NP? NP = EXP?\nMost people think P \u2260 NP (\u2286 EXP), i.e., generating solutions harder than checking\nIf you prove either way, people will give you lots of money ($1M Millennium Prize)\nWhy do we care? If can show a problem is hardest problem in NP, then problem cannot be solved in polynomial time if P \u2260 NP"
    },
    {
        "question": "How do we relate difficulty of problems?",
        "answer": "Reductions!"
    },
    {
        "question": "Explain what reductions are.",
        "answer": "Suppose you want to solve problem A\nOne way to solve is to convert A into a problem B you know how to solve\nSolve using an algorithm for B and use it to compute solution to A\nThis is called a reduction from problem A to problem B (\u0410 \u2192 \u0412)\nBecause B can be used to solve A, B is at least as hard as A (A < B)\nGeneral algorithmic strategy: reduce to a problem you know how to solve"
    },
    {
        "question": "Show a conversion from an A problem to a B problem using Reductions.",
        "answer": "Unweighted Shortest Path: Give equal weights-> Weighted Shortest Path\nInteger-weighted Shortest Path: Subdivide edges-> Shortest Path\nLongest Path: Negate weights-> Unweighted Shortest Path"
    },
    {
        "question": "Define NP-hard and NP-complete.",
        "answer": "Problem A is NP-hard if every problem in NP is polynomially reducible to A\ni.e., A is at least as hard as (can be used to solve) every problem in NP (X < A for X \u2208 NP)\nNP-complete = NP \u2229 NP-hard"
    },
    {
        "question": "What is the first NP-complete problem?",
        "answer": "Every decision problem reducible to satisfying a logical circuit, a problem called \u201cCircuit SAT\u201d."
    },
    {
        "question": "What are other NP-complete problems?",
        "answer": "Longest Simple Path and Tetris are NP-complete, so if any problem is in NP \\ P, these are"
    },
    {
        "question": "What is EXP-complete?",
        "answer": "Chess is EXP-complete: in EXP and reducible from every problem in EXP (so \u2209 P)"
    },
    {
        "question": "What are some Examples of NP-complete Problems?",
        "answer": "Subset Sum from L18 (\u201cweakly NP-complete\u201d which is what allows a pseudopolynomial-time algorithm, but no polynomial algorithm unless P = NP)\n3-Partition: given n integers, can you divide them into triples of equal sum? (\u201cstrongly NP-complete\": no pseudopolynomial-time algorithm unless P = NP)\nRectangle Packing: given n rectangles and a target rectangle whose area is the sum of the n rectangle areas, pack without overlap\nReduction from 3-Partition to Rectangle Packing: transform integer a_i into 1 \u00d7 a_i rectangle; set target rectangle to n/3 \u00d7 (\u03a3a_i)/3\nJigsaw puzzles: given n pieces with possibly ambiguous tabs/pockets, fit the pieces together\nReduction from Rectangle Packing: use uniquely matching tabs/pockets to force building rectangles and rectangular boundary; use one ambiguous tab/pocket for all other boundaries\nLongest common subsequence of n strings\nLongest simple path in a graph\nTraveling Salesman Problem: shortest path that visits all vertices of a given graph (or decision version: is minimum weight \u2264 d)\nShortest path amidst obstacles in 3D\n3-coloring given graph (but 2-coloring \u2208 P)\nLargest clique in a given graph\nSAT: given a Boolean formula (made with AND, OR, NOT), is it every true? E.g., x AND NOT x is a NO input\nMinesweeper, Sudoku, and most puzzles\nSuper Mario Bros., Legend of Zelda, Pok\u00e9mon, and most video games are NP-hard (many are harder)"
    },
    {
        "question": "What are the dynamic programming steps?",
        "answer": "The dynamic programming steps are:\n1. Subproblem definition\n2. Relate subproblem solutions recursively\n3. Topological order\n4. Base cases\n5. Original problem\n6. Time analysis"
    },
    {
        "question": "Describe the subproblem definition step.",
        "answer": "The subproblem definition step involves:\n- Describing the meaning of a subproblem in words, in terms of parameters\n- Often subsets of input: prefixes, suffixes, contiguous substrings of a sequence\n- Often multiply possible subsets across multiple inputs\n- Often record partial state: add subproblems by incrementing some auxiliary variables\n- Often smaller integers than a given integer (today's focus)"
    },
    {
        "question": "Describe the relate subproblem solutions recursively step.",
        "answer": "The relate subproblem solutions recursively step is x(i) = f(x(j), . . .) for one or more j < i\n- Identify a question about a subproblem solution that, if you knew the answer to, reduces the subproblem to smaller subproblem(s)\n- Locally brute-force all possible answers to the question"
    },
    {
        "question": "Describe the topological order.",
        "answer": "The topological order is to argue relation is acyclic and subproblems form a DAG."
    },
    {
        "question": "Describe the base cases.",
        "answer": "The base cases are the state solutions for all (reachable) independent subproblems where relation breaks down."
    },
    {
        "question": "Describe the original problem.",
        "answer": "The original problem is described as:\n- Show how to compute solution to original problem from solutions to subproblem(s)\n- Possibly use parent pointers to recover actual solution, not just objective function"
    },
    {
        "question": "Describe the time analysis.",
        "answer": "The time analysis is: \u03a3xex work(x), or if work(x) = O(W) for all x \u2208 X, then |X|\u00b7 O(W). work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time"
    },
    {
        "question": "Describe the Rod Cutting problem.",
        "answer": "Given a rod of length L and value v(l) of rod of length l for all l \u2208 {1, 2, . . ., L}, the goal is to cut the rod to maximize the value of cut rod pieces."
    },
    {
        "question": "Give an example for rod cutting.",
        "answer": "Example: L = 7, \u03c5 = [0, 1, 10, 13, 18, 20, 31, 32]"
    },
    {
        "question": "Describe the subproblems of Rod Cutting.",
        "answer": "The subproblems are:\n- x(l): maximum value obtainable by cutting rod of length l\n- For l \u2208 {0, 1, . . ., L}"
    },
    {
        "question": "Describe the relate step of Rod Cutting.",
        "answer": "The relate step is:\n- First piece has some length p (Guess!)\n- x(l) = max{v(p) + x(l - p) | p \u2208 {1, ..., l}}"
    },
    {
        "question": "Describe the topological order of Rod Cutting.",
        "answer": "The topological order is: Increasing l: Subproblems x(l) depend only on strictly smaller l, so acyclic"
    },
    {
        "question": "Describe the base case of Rod Cutting.",
        "answer": "The base case is: x(0) = 0 (length-zero rod has no value!)"
    },
    {
        "question": "Describe the original problem of Rod Cutting.",
        "answer": "The original problem is:\n- Maximum value obtainable by cutting rod of length L is x(L)\n- Store choices to reconstruct cuts\n- If current rod length l and optimal choice is l', remainder is piece p = l \u2013 l'\n- (maximum-weight path in subproblem DAG!)"
    },
    {
        "question": "Describe the time analysis of Rod Cutting.",
        "answer": "The time analysis:\n- # subproblems: L + 1\n- work per subproblem: O(l) = O(L)\n- O(L2) running time"
    },
    {
        "question": "What does (Strongly) polynomial time mean?",
        "answer": "(Strongly) polynomial time means that the running time is bounded above by a constant-degree polynomial in the input size measured in words."
    },
    {
        "question": "What is the input size in Rod Cutting?",
        "answer": "In Rod Cutting, input size is L + 1 words (one integer L and L integers in v)"
    },
    {
        "question": "Is O(L2) polynomial time?",
        "answer": "O(L2) is a constant-degree polynomial in L + 1, so YES: (strongly) polynomial time"
    },
    {
        "question": "Describe the Subset Sum problem.",
        "answer": "Given a sequence of n positive integers A = {ao, a1,..., An\u22121}, is there a subset of A that sums exactly to T? (i.e., \u2203A\u2032 \u2286 A s.t. \u03a3\u03b1\u2208a' a = T?)"
    },
    {
        "question": "Give an example for Subset Sum.",
        "answer": "Example: A = (1, 3, 4, 12, 19, 21, 22), T = 47 allows A' = {3, 4, 19, 21}"
    },
    {
        "question": "Describe the subproblems of Subset Sum.",
        "answer": "The subproblems are:\n- x(i,t) = does any subset of A[i:] sum to t?\n- For i \u2208 {0, 1, . . ., n}, t \u2208 {0, 1, . . ., T}"
    },
    {
        "question": "Describe the relate step of Subset Sum.",
        "answer": "The relate step is:\n- Idea: Is first item a\u017c in a valid subset A'? (Guess!)\n- If yes, then try to sum to t ai \u2265 0 using remaining items\n- If no, then try to sum to t using remaining items\n- x(i,t) = OR {x(i + 1 + 1, t - A[i]) ift > A[i]\nx(i + 1, t) always"
    },
    {
        "question": "Describe the topological order of Subset Sum.",
        "answer": "The topological order:\n- Subproblems x(i, t) only depend on strictly larger i, so acyclic\n- Solve in order of decreasing i"
    },
    {
        "question": "Describe the base case of Subset Sum.",
        "answer": "The base cases are:\n- x(i, 0) = YES for i \u2208 {0, . . ., n} (space packed exactly!)\n- x(n,t) = NO for j \u2208 {1, . . ., T} (no more items available to pack)"
    },
    {
        "question": "Describe the original problem of Subset Sum.",
        "answer": "The original problem is given by x(0,T)\n- Example: A = (3, 4, 3, 1), T = 6 solution: A' = (3, 3)\n- Bottom up: Solve all subproblems (Example has 35)"
    },
    {
        "question": "Is O(nT) bounded above by a polynomial in n + 1?",
        "answer": "NO, not necessarily"
    },
    {
        "question": "What is pseudopolynomial time?",
        "answer": "Algorithm has pseudopolynomial time: running time is bounded above by a constant-degree polynomial in input size and input integers."
    },
    {
        "question": "Give examples of pseudopolynomial algorithms.",
        "answer": "Counting sort O(n + u), radix sort O(nlognu), direct-access array build O(n + u), and Fibonacci O(n) are all pseudopolynomial algorithms we've seen already"
    },
    {
        "question": "What is weakly polynomial?",
        "answer": "Radix sort is actually weakly polynomial (a notion in between strongly polynomial and pseudopolynomial): bounded above by a constant-degree polynomial in the input size mea-sured in bits, i.e., in the logarithm of the input integers"
    },
    {
        "question": "Is Subset Sum solvable in polynomial time when integers are not polynomially bounded?",
        "answer": "No if P \u2260 NP."
    },
    {
        "question": "What are the main features of dynamic programs?",
        "answer": "- Review of examples from lecture\n- Subproblems:\n *Prefix/suffixes: Bowling, LCS, LIS, Floyd\u2013Warshall, Rod Cutting (coincidentally, really Integer subproblems), Subset Sum\n*Substrings: Alternating Coin Game, Arithmetic Parenthesization\n*Multiple sequences: LCS\n*Integers: Fibonacci, Rod Cutting, Subset Sum\n*Pseudopolynomial: Fibonacci, Subset Sum\n*Vertices: DAG shortest paths, Bellman\u2013Ford, Floyd-Warshall\n- Subproblem constraints/expansion:\n*Nonexpansive constraint: LIS (include first item)\n*2 \u00d7 expansion: Alternating Coin Game (who goes first?), Arithmetic Parenthesization (min/max)\n*(1)\u00d7 expansion: Piano Fingering (first finger assignment)\n*(n) \u00d7 expansion: Bellman\u2013Ford (# edges)\n- Relation:\n*Branching = # dependant subproblems in each subproblem\n*(1) branching: Fibonacci, Bowling, LCS, Alternating Coin Game, Floyd-Warshall, Subset Sum\n* (degree) branching (source of |E| in running time): DAG shortest paths, Bellman-Ford\n*(n) branching: LIS, Arithmetic Parenthesization, Rod Cutting\n*Combine multiple solutions (not path in subproblem DAG): Fibonacci, Floyd-Warshall, Arithmetic Parenthesization\n- Original problem:\n*Combine multiple subproblems: DAG shortest paths, Bellman\u2013Ford, Floyd-Warshall, LIS, Piano Fingering"
    }
]