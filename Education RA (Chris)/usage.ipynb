{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Course Note Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 3/3 [00:34<00:00, 11.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 46 Q&A pairs to data/design_and_analysis_of_algorithms/course_notes/json/design_combined.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'questions': [{'question': 'What is the focus of Lecture 18?',\n",
       "   'answer': 'Fixed-Parameter Algorithms, covering Vertex Cover, Fixed-Parameter Tractability, Kernelization, and Connection to Approximation.'},\n",
       "  {'question': 'What are Fixed Parameter Algorithms and their desired features?',\n",
       "   'answer': 'Fixed Parameter Algorithms are an alternative way to deal with NP-hard problems instead of approximation algorithms, aiming to solve NP-hard problems, run in polynomial time, and get exact solutions. In general, unless P = NP, an algorithm can have two of these three features, but not all three.'},\n",
       "  {'question': 'Explain the idea behind using a specific parameter in Fixed Parameter Algorithms.',\n",
       "   'answer': 'The idea is to aim for an exact algorithm but isolate exponential terms to a specific parameter. When the value of this parameter is small, the algorithm gets fast instances. Hopefully, this parameter will be small in practice.'},\n",
       "  {'question': \"Define 'Parameter' and 'Parameterized Problem'.\",\n",
       "   'answer': 'A parameter is a nonnegative integer k(x) where x is the problem input. Typically, the parameter is a natural property of the problem (some k in input). It may not necessarily be efficiently computable (e.g., OPT). A parameterized problem is simply the problem plus the parameter or the problem as seen with respect to the parameter.'},\n",
       "  {'question': 'What is the goal of fixed-parameter algorithms?',\n",
       "   'answer': 'The goal of fixed-parameter algorithms is to have an algorithm that is polynomial in the problem size n but possibly exponential in the parameter k, and still get an exact solution.'},\n",
       "  {'question': 'Define the k-Vertex Cover problem.',\n",
       "   'answer': 'Given a graph G = (V, E) and a nonnegative integer k, is there a set \\\\(S \\\\subseteq V\\\\) of vertices of size at most k, |S| ≤ k, that covers all edges. This is a decision problem for Vertex Cover and is also NP-hard.'},\n",
       "  {'question': 'Describe the image.',\n",
       "   'answer': 'The image displays a graph with 8 nodes, one marked in red, and seven marked in black. The red node is connected to each of the seven black nodes by a single edge. The seven black nodes are not connected to each other.'},\n",
       "  {'question': \"Describe the brute-force solution for the k-Vertex Cover problem and why it is considered 'bad'.\",\n",
       "   'answer': \"The brute-force solution involves trying all \\\\(\\\\binom{n}{k}\\\\) + \\\\(\\\\binom{n}{k-1}\\\\) + ... + \\\\(\\\\binom{n}{0}\\\\) sets of ≤ k vertices. Testing coverage takes O(m) time where m is the number of edges. The total runtime is O(\\\\(V^k|E|\\\\)). It is polynomial for fixed k but not the same polynomial for all k's, making it inefficient in most cases. Hence we define \\\\(nf(k)\\\\) to be bad, where n = |V| + |E| is the input size.\"},\n",
       "  {'question': 'Describe the Bounded search-tree algorithm for the k-Vertex Cover problem.',\n",
       "   'answer': \"Pick arbitrary edge e = (u, v). Either u ∈ S or v ∈ S (or both) but don't know which. Guess which one: try both possibilities.\\n1. add u to S, delete u and incident edges from G, and recurse with k′ = k−1.\\n2. do the same but with v instead of u\\n3. return the OR of the two outcomes\"},\n",
       "  {'question': 'Describe the recursion tree and overall runtime.',\n",
       "   'answer': 'The text describes a recursion tree where each node (u,v) branches into two child nodes (u\\', v\\') and (u\", v\") where u\\' and v\\' are nodes after adding \\'u\\', deleting \\'u\\', and incident edges and u\", v\" are nodes after adding \\'v\\', deleting \\'v\\', and incident edges. At a leaf (k = 0), return YES if |E| = 0 (all edges covered). It takes O(V) time to delete u or v. Therefore this has a total runtime of \\\\((2^k|V|)\\\\).'},\n",
       "  {'question': 'When is a parameterized problem considered fixed-parameter tractable (FPT)?',\n",
       "   'answer': 'A parameterized problem is fixed-parameter tractable (FPT) if there is an algorithm with running time ≤ f(k) · \\\\(n^{o(1)}\\\\), such that f : N → N (non negative) and k is the parameter, and the O(1) degree of the polynomial is independent of k and n.'},\n",
       "  {'question': \"State the theorem regarding the existence of an \\\\(f(k) \\\\cdot n^c\\\\) algorithm in relation to the existence of an \\\\(f'(k) + n^{c'}\\\\) algorithm.\",\n",
       "   'answer': \"If ∃f(k) · \\\\(n^c\\\\) algorithm then ∃f'(k) + \\\\(n^{c'}\\\\).\"},\n",
       "  {'question': 'Define Kernelization.',\n",
       "   'answer': \"Kernelization is a simplifying self-reduction. It is a polynomial time algorithm that converts an input (x, k) into a small and equivalent input (x', k'). Here, small means |x'| ≤ f(k) and equivalent means the answer to x is the same as the answer to x'.\"},\n",
       "  {'question': 'State the theorem that relates FPT and Kernelization.',\n",
       "   'answer': 'A problem is FPT if and only if there is a kernelization.'},\n",
       "  {'question': 'Describe the steps to create a polynomial kernel for k-Vertex Cover.',\n",
       "   'answer': \"To create a kernel for k-Vertex Cover, the algorithm follows these steps:\\nMake graph simple by removing all self loops and multi-edges\\nAny vertex of degree > k must be in the cover (else would need to add > k vertices to cover incident edges)\\nRemove such vertices (and incident edges) one at a time, decreasing k accordingly\\nRemaining graph has maximum degree ≤ k\\nEach remaining vertex covers ≤ k edges\\nIf the number of remaining edges is > k, answer NO and output canonical NO instance.\\nElse, |E'| ≤ \\\\(k^2\\\\)\\nRemove all isolated vertices (degree 0 vertices)\\nNow |V'| ≤ \\\\(2k^2\\\\)\"},\n",
       "  {'question': 'What is the connection between optimization problems and approximation algorithms?',\n",
       "   'answer': 'Take an optimization problem, integral OPT and consider its associated decision problem: “OPT ≤ k ?” and parameterize by k.\\nOptimization problem has EPTAS (EPTAS: efficient PTAS, \\\\(f(\\\\frac{1}{\\\\epsilon})\\\\) · \\\\(n^{o(1)}\\\\) e.g. Approxpartition[L17]) ⇒ decision problem is FPT'},\n",
       "  {'question': 'How can a polynomial A(x) be written, and what is a coefficient vector?',\n",
       "   'answer': 'A polynomial A(x) can be written as:\\nA(x) = a0 + a1x + a2x² + ··· + an-1xn-1 = ∑(from k=0 to n-1) akxk = (a0, a1, a2,...,an-1) (coefficient vector). The degree of A is n - 1.'},\n",
       "  {'question': 'What are the three primary operations for polynomials?',\n",
       "   'answer': 'Evaluation, Addition, and Multiplication.'},\n",
       "  {'question': \"How is polynomial evaluation performed, and what is Horner's rule?\",\n",
       "   'answer': \"Evaluation: Given a polynomial A(x) and a number xo, compute A(xo). This can be done in O(n) time using O(n) arithmetic operations via Horner's rule. Horner's Rule: A(x) = ao+x(a1+x(a2+···x(an−1)･･･)). At each step, a sum is evaluated, then multiplied by x, before beginning the next step. Thus O(n) multiplications and O(n) additions are required.\"},\n",
       "  {'question': 'How is polynomial addition performed?',\n",
       "   'answer': 'Addition: Given two polynomials A(x) and B(x), compute C(x) = A(x) + B(x). This takes O(n) time using basic arithmetic, because ck = ak + bk.'},\n",
       "  {'question': 'How is polynomial multiplication performed, and what are its time complexities?',\n",
       "   'answer': 'Multiplication: Given two polynomials A(x) and B(x), compute C(x) = A(x)·B(x). Then ck = ∑(from j=0 to k) ajbk-j for 0 ≤ k ≤ 2(n-1), because the degree of the resulting polynomial is twice that of A or B. This multiplication is then equivalent to a convolution of the vectors A and reverse(B). Naive polynomial multiplication takes O(n²). O(nlog3) or even O(n1+ε) (θε > 0) is possible via Strassen-like divide-and-conquer tricks.'},\n",
       "  {'question': 'What are the 3 main representations to consider for polynomials?',\n",
       "   'answer': 'Coefficient vector with a monomial basis, Roots and a scale term, and Samples.'},\n",
       "  {'question': 'Describe the roots and a scale term polynomial representation, including its limitations.',\n",
       "   'answer': 'A(x) = (x - ro) · (x - r₁) ………… (x - rn-1) · C. It is impossible to find exact roots with only basic arithmetic operations and kth root operations. Furthermore, addition is extremely hard with this representation, or even impossible. Multiplication simply requires roots to be concatenated, and evaluation can be completed in O(n).'},\n",
       "  {'question': 'Describe the samples polynomial representation, and the conditions and operations involved.',\n",
       "   'answer': \"Samples: (xo, Yo), (X1,Y1),..., (Xn-1, Yn-1) with A(xi) = yi (∀i) and each xi is distinct. These samples uniquely determine a degree n – 1 polynomial A, according to the Lagrange and Fundamental Theorem of Algebra. Addition and multiplication can be computed by adding and multiplying the yi terms, assuming that the xi's match. However, evaluation requires interpolation.\"},\n",
       "  {'question': 'How can you combine the best of each representation of polynomial?',\n",
       "   'answer': 'We combine the best of each representation by converting between coefficients and samples in O(nlgn) time.'},\n",
       "  {'question': 'How can polynomial multiplication be formulated as a divide and conquer algorithm?',\n",
       "   'answer': 'Divide the polynomial A into its even and odd coefficients: Aeven(x) = ∑(from k=0 to [n/2]-1) a2kxk = (ao, a2, a4, ...), Aodd(x) = ∑(from k=0 to [n/2]-1) a2k+1xk = (a1, a3, a5, ...). Recursively conquer Aeven(y) for y ∈ X² and Aodd(y) for y ∈ X2, where X2 = {x² | x ∈ X}. Combine the terms: A(x) = Aeven(x2) + x · Aodd(x2) for x ∈ X.'},\n",
       "  {'question': 'What are collapsing sets?',\n",
       "   'answer': 'Collapsing sets can be constructed via square roots. Each of the following collapsing sets is computing by taking all square roots of the previous set. Some examples are: {1}, {1,-1}, {1, -1, і, -i}, {1, -1, ±(1 + i), ±(√2/2)(-1 + i)}, which lie on a unit circle.'},\n",
       "  {'question': 'What are the nth roots of unity and how are they defined?',\n",
       "   'answer': \"The nth roots of unity are n x's such that xn = 1. These points are uniformly spaced around the unit circle in the complex plane (including 1). These points are of the form (cos θ, sin θ) = cos θ + i sin θ = eiθ by Euler's Formula, for θ = 0, τ/n, 2τ/n,..., (n-1)τ/n (where τ = 2π).  The nth roots of unity where n = 2k form a collapsing set, because (eiθ)2 = ei(2θ) = ei(2θ mod τ). Therefore the even nth roots of unity are equivalent to the n/2nd roots of unity.\"},\n",
       "  {'question': 'What is the Discrete Fourier Transform (DFT)?',\n",
       "   'answer': 'The DFT allows the transformation between coefficients and samples, computing A* = V. A for xk = eitk/n where n = 2º, where A is the set of coefficients and A* is the resulting samples. The individual terms a = Σ(from j=0 to n-1) eitjk/n . Aj.'},\n",
       "  {'question': 'What is the Fast Fourier Transform (FFT)?',\n",
       "   'answer': 'The FFT algorithm is an O(nlgn) divide and conquer algorithm for DFT, used by Gauss circa 1805, and popularized by Cooley and Turkey and 1965.'},\n",
       "  {'question': 'What is the Inverse Discrete Fourier Transform?',\n",
       "   'answer': 'The Inverse Discrete Fourier Transform is an algorithm to return the coefficients of a polynomial from the multiplied samples. The transformation is of the form A* → V-1. A* = A.  Claim 1: V−1 = (1/n)V, where V is the complex conjugate of V.'},\n",
       "  {'question': 'What are the steps in Fast Polynomial Multiplication?',\n",
       "   'answer': 'In order to compute the product of two polynomials A and B, we can perform the following steps: 1. Compute A* = FFT(A) and B* = FFT(B), which converts both A and B from coefficient vectors to a sample representation. 2. Compute C* = A* · B* in sample representation in linear time by calculating Ck = A*k · B*k (∀k). 3. Compute C = IFFT(C*), which is a vector representation of our final solution.'},\n",
       "  {'question': 'What are the applications of Fourier (frequency) space?',\n",
       "   'answer': 'The polynomial A* = FFT(A) is complex, and the amplitude a represents the amplitude of the frequency-k signal, while arg(a) (the angle of the 2D vector) represents the phase shift of that signal. For example, this perspective is particularly useful for audio processing, as used by Adobe Audition, Audacity, etc.: High-pass filters zero out high frequencies, Low-pass filters zero out low frequencies, Pitch shifts shift the frequency vector, Used in MP3 compression, etc.'},\n",
       "  {'question': 'What are the key points covered in Lecture 12?',\n",
       "   'answer': \"Lecture 12 covers Greedy Algorithms and Minimum Spanning Tree, including Optimal Substructure, Greedy Choice Property, Prim's algorithm, and Kruskal's algorithm.\"},\n",
       "  {'question': 'Define greedy algorithm, tree, and spanning tree.',\n",
       "   'answer': 'A greedy algorithm makes locally best choices but ignores future effects. A tree is a connected, acyclic graph. A spanning tree of a graph G is a subset of edges of G that form a tree and include all vertices of G.'},\n",
       "  {'question': 'Describe the Minimum Spanning Tree problem.',\n",
       "   'answer': 'Given an undirected graph G = (V, E) and edge weights W : E → R, find a spanning tree T of minimum weight Σεετ ω(e).'},\n",
       "  {'question': 'What is a naive algorithm for finding the MST and what is its drawback?',\n",
       "   'answer': 'The obvious MST algorithm is to compute the weight of every tree and return the tree of minimum weight. Unfortunately, this can take exponential time in the worst case.'},\n",
       "  {'question': 'What are the properties of Greedy Algorithms?',\n",
       "   'answer': 'Problems that can be solved by greedy algorithms have two main properties: Optimal Substructure (optimal solution to a problem incorporates the optimal solution to subproblem(s)) and Greedy choice property (locally optimal choices lead to a globally optimal solution).'},\n",
       "  {'question': 'State and prove Lemma 1 regarding optimal substructure for MST.',\n",
       "   'answer': \"Lemma 1: If T' is a minimum spanning tree of G/e, then T' ∪ {e} is an MST of G. Proof: Let T* be an MST of G containing the edge e. Then T*/e is a spanning tree of G' = G/{e}. By definition, T' is an MST of G'. Thus the total weight of T' is less than or equal to that of T*/e, or w(T') ≤ w(T*/e). Then w(T) = w(T') + w(e) ≤ w(T*/e) + w(e) = w(T').\"},\n",
       "  {'question': 'State and prove Lemma 2 (Greedy-Choice Property for MST).',\n",
       "   'answer': \"Lemma 2 (Greedy-Choice Property for MST). For any cut (S, V \\\\ S) in a graph G = (V, E, w), any least-weight crossing edge e = {u, v} with u ∈ S and v ∉ S is in some MST of G. Proof: First, consider an MST T of G. Then T has a path from u to v. Because u ∈ S and v ∉ S, the path has some edge e' = {u',v'} which also crosses the cut. Then T' = T \\\\ {e'} ∪ {e} is a spanning tree of G and w(T') = w(T) – w(e') + w(e), but e is a least-weight edge crossing (S, V \\\\ S). Thus w(e) ≤ w(e'), so w(T') ≤ w(T). Therefore T' is an MST too.\"},\n",
       "  {'question': \"Provide the pseudocode for Prim's Algorithm.\",\n",
       "   'answer': '1 Maintain priority queue Q on V \\\\ S, where v.key = min{w(u, v) | u ∈ S}\\n2 Q=V\\n3 Choose arbitrary start vertex s ∈ V, s.key = Ø\\n4 for v in V \\\\{s}\\n5    v.key = ∞\\n6 while Q is not empty\\n7    u = Extract-Min(Q), add u to S\\n8    for v ∈ Adj[u]\\n9        if v ∈ Q and v ∉ S and w(u, v) < v.key:\\n10           v.key = w(u, v) (via a Decrease-Key operation)\\n11           v.parent = u\\n12 return {{v, v.parent} | v ∈ V \\\\ {s}}'},\n",
       "  {'question': \"What is the runtime complexity of Prim's algorithm?\",\n",
       "   'answer': \"Prim's algorithm runs in O(V)TExtract-Min + O(E) ·TDecrease-Key. The effective runtime of the algorithm varies with the data structures used to implement the algorithm. The table below describes the runtime with the different implementations of the priority queue.\\nPriority Queue | TExtract-Min | TDecrease-Key | Total\\nArray | O(V) | O(1) | O(V^2)\\nBinary heap | O(lg V) | O(lg V) | O(E lg V)\\nFibonacci heap | O(lg V) (amortized) | O(1) (amortized) | O(E + V lg V)\"},\n",
       "  {'question': \"What is Kruskal's Algorithm and how does it solve the MST problem?\",\n",
       "   'answer': \"Kruskal's Algorithm is another algorithm to solve the MST problem. It constructs an MST by taking the globally lowest-weight edge and contracting it.\"},\n",
       "  {'question': \"Provide the pseudocode for Kruskal's Algorithm.\",\n",
       "   'answer': '1 Maintain connected components that have been added to the\\nMST so far T, in a Union-Find structure\\n2 Initialize T = 0\\n3 for vin V\\n4   Make-Set(v)\\n5 Sort E by weight\\n6 For e = (u, v) ∈ E (in increasing-weight order):\\n7    if Find-Set(u) ≠ Find-Set(v):\\n8        Add e to T\\n9        Union(u, v)'},\n",
       "  {'question': \"What is the runtime complexity of Kruskal's algorithm?\",\n",
       "   'answer': \"Kruskal's algorithm has an overall runtime of Tsort(E) + O(V) · TMake-Set + O(E)(TFind + TUnion) = O(Elg E + Ea(V)). If all weights are integer weights, or all weights are in the range [0, EO(1)], then the runtime of the sorting step is O(E), using Counting Sort or a similar algorithm, and the runtime of Kruskal's Algorithm will be better than that of Prim's Algorithm.\"},\n",
       "  {'question': 'What is the fastest MST algorithm?',\n",
       "   'answer': 'The fastest MST algorithm is a randomized algorithm with an expected runtime of O(V + E). The algorithm was proposed by Karger, Klein, and Tarjan in 1993.'}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from course_notes_extraction import PDFToQAConverter\n",
    "\n",
    "converter = PDFToQAConverter(api_key=\"your-gemini-api-key\")\n",
    "\n",
    "converter.process_directory(\n",
    "    pdf_directory=\"data/design_and_analysis_of_algorithms/course_notes/pdfs\",  # input raw pdfs directory\n",
    "    output_json=\"data/design_and_analysis_of_algorithms/course_notes/json/design_combined.json\"          # output file name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exam QA Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  50%|█████     | 1/2 [00:26<00:26, 26.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to data/design_and_analysis_of_algorithms/exams/json\\44146f3104fa80a52514265d070ebc40_MIT6_046JS15_quiz2sols_processed.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 2/2 [00:55<00:00, 27.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to data/design_and_analysis_of_algorithms/exams/json\\5b6cd99674ea0c866c942b72aa9e7289_MIT6_046JS15_quiz1sols_processed.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files in data/design_and_analysis_of_algorithms/exams/json: 100%|██████████| 2/2 [00:00<00:00, 26.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 40 questions into data/design_and_analysis_of_algorithms/exams/design_combined.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from exam_qa_extraction import PDFQuestionExtractor, PDFProcessor\n",
    "from utils.data_util import combine_json_files\n",
    "\n",
    "extractor = PDFQuestionExtractor(api_key=\"your-gemini-api-key\")\n",
    "processor = PDFProcessor(extractor)\n",
    "processor.process_directory(\"data/design_and_analysis_of_algorithms/exams/pdfs\", \"data/design_and_analysis_of_algorithms/exams/json\")\n",
    "combine_json_files(input_dir=\"data/design_and_analysis_of_algorithms/exams/json\", output_file=\"data/design_and_analysis_of_algorithms/exams/design_combined.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create prompt datasets (n-prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch .jsonl file created: data/design_and_analysis_of_algorithms/0_shot.jsonl with 40 entries using 0-shot learning\n"
     ]
    }
   ],
   "source": [
    "from create_prompt_dataset import create_chatgpt_input_dataset\n",
    "create_chatgpt_input_dataset(input_file=\"data/design_and_analysis_of_algorithms/exams/design_combined.json\", output_file=\"data/design_and_analysis_of_algorithms/0_shot.jsonl\", shots=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully. File ID: file-6TDYY7psuRq9zeZQXJitdR\n",
      "Batch created successfully. Batch ID: batch_67e5c3fc7d148190802cd73992d74463\n",
      "Batch Status: validating\n",
      "Request Counts: BatchRequestCounts(completed=0, failed=0, total=0)\n",
      "{'file_id': 'file-6TDYY7psuRq9zeZQXJitdR', 'batch_id': 'batch_67e5c3fc7d148190802cd73992d74463', 'batch_status': 'validating'}\n",
      "pending...\n",
      "pending...\n",
      "pending...\n",
      "pending...\n",
      "pending...\n",
      "pending...\n",
      "pending...\n",
      "Batch output downloaded to data/design_and_analysis_of_algorithms/llm_responses/0_shot.jsonl\n"
     ]
    }
   ],
   "source": [
    "import openai as client\n",
    "import time\n",
    "from utils.open_ai_batch_processing import upload_and_create_batch, save_batch_response\n",
    "\n",
    "batch = upload_and_create_batch(jsonl_file_path=\"data/design_and_analysis_of_algorithms/0_shot.jsonl\")\n",
    "print(batch)\n",
    "save_batch_response(batch, output_file=\"data/design_and_analysis_of_algorithms/llm_responses/0_shot.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG with Course Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris Joel\\Desktop\\cs-research-project\\reformatted\\rag.py:72: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return self.qa_chain.run(question)\n"
     ]
    }
   ],
   "source": [
    "from rag import RAGPipeline\n",
    "rag_pipeline = RAGPipeline()\n",
    "\n",
    "# Load and process documents\n",
    "rag_pipeline.load_and_process_documents(document_json_path=\"data/design_and_analysis_of_algorithms/exams/design_combined.json\")\n",
    "\n",
    "# Process input file and generate responses\n",
    "rag_pipeline.process_jsonl_file(\n",
    "    input_file=\"data/design_and_analysis_of_algorithms/0_shot.jsonl\",\n",
    "    output_file=\"data/design_and_analysis_of_algorithms/llm_responses/rag.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation (Rouge-1, Rouge-L, BLEU, BertScore, F1, Jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chris Joel\\Desktop\\cs-research-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "| Question ID       | ROUGE-1 | ROUGE-L | BLEU | BERTScore | F1 | Jaccard |\n",
      "|-------------------|---------|---------|------|-----------|----|---------|\n",
      "| 441462sols000     |    6.90% |    6.90% |   0.34% |     84.51% | 10.71% |    5.66% |\n",
      "| 441462sols001     |   17.72% |   17.72% |   1.06% |     87.33% | 22.73% |   12.82% |\n",
      "| 441462sols002     |   14.55% |   14.55% |   0.51% |     83.16% | 12.77% |    6.82% |\n",
      "| 441462sols003     |   16.67% |   12.50% |   0.62% |     85.25% | 23.33% |   13.21% |\n",
      "| 441462sols004     |   14.29% |   10.71% |   0.33% |     84.03% | 16.00% |    8.70% |\n",
      "| 441462sols005     |   47.89% |   36.62% |  12.45% |     89.49% | 42.86% |   27.27% |\n",
      "| 441462sols006     |   16.06% |   14.60% |   1.10% |     85.09% | 15.56% |    8.43% |\n",
      "| 441462sols007     |   64.29% |   64.29% |  32.92% |     93.68% | 85.00% |   73.91% |\n",
      "| 441462sols008     |   29.55% |   25.00% |  10.59% |     89.34% | 30.00% |   17.65% |\n",
      "| 441462sols009     |   11.76% |    7.06% |   0.64% |     83.63% | 14.81% |    8.00% |\n",
      "| 441462sols010     |   69.80% |   59.06% |  40.67% |     91.36% | 68.16% |   51.69% |\n",
      "| 441462sols011     |   19.69% |   17.62% |   4.39% |     81.35% | 26.55% |   15.31% |\n",
      "| 441462sols012     |    3.51% |    3.51% |   0.00% |     82.87% | 0.00% |    0.00% |\n",
      "| 441462sols013     |   40.61% |   25.29% |   1.10% |     84.96% | 25.32% |   14.49% |\n",
      "| 441462sols014     |   79.47% |   72.53% |  65.57% |     92.82% | 76.86% |   62.42% |\n",
      "| 441462sols015     |   65.27% |   45.80% |  20.30% |     87.86% | 53.28% |   36.32% |\n",
      "| 441462sols016     |   62.83% |   61.95% |  43.05% |     90.23% | 61.42% |   44.32% |\n",
      "| 441462sols017     |   49.14% |   24.57% |  17.38% |     85.98% | 48.11% |   31.68% |\n",
      "| 5b6cd1sols000     |   16.00% |   16.00% |   0.67% |     87.10% | 18.46% |   10.17% |\n",
      "| 5b6cd1sols001     |   10.08% |   10.08% |   0.44% |     84.92% | 15.38% |    8.33% |\n",
      "| 5b6cd1sols002     |   40.00% |   27.27% |   1.11% |     89.05% | 32.91% |   19.70% |\n",
      "| 5b6cd1sols003     |   33.77% |   28.57% |   2.84% |     85.51% | 31.37% |   18.60% |\n",
      "| 5b6cd1sols004     |   34.33% |   29.85% |  11.25% |     89.17% | 44.74% |   28.81% |\n",
      "| 5b6cd1sols005     |    8.16% |    8.16% |   0.26% |     84.73% | 6.90% |    3.57% |\n",
      "| 5b6cd1sols006     |    2.04% |    2.04% |   0.19% |     86.64% | 3.28% |    1.67% |\n",
      "| 5b6cd1sols007     |   15.79% |   15.79% |   0.79% |     88.68% | 16.00% |    8.70% |\n",
      "| 5b6cd1sols008     |   17.39% |   15.22% |   0.84% |     85.73% | 20.29% |   11.29% |\n",
      "| 5b6cd1sols009     |   13.33% |   10.67% |   1.58% |     86.36% | 17.54% |    9.62% |\n",
      "| 5b6cd1sols010     |   47.27% |   29.09% |  10.33% |     88.89% | 32.65% |   19.51% |\n",
      "| 5b6cd1sols011     |   19.90% |   14.66% |   0.59% |     83.17% | 20.83% |   11.63% |\n",
      "| 5b6cd1sols012     |   72.87% |   57.36% |  44.80% |     89.69% | 69.41% |   53.15% |\n",
      "| 5b6cd1sols013     |   38.42% |   22.66% |   3.96% |     84.86% | 21.71% |   12.17% |\n",
      "| 5b6cd1sols014     |   23.16% |   21.05% |   1.59% |     87.61% | 22.58% |   12.73% |\n",
      "| 5b6cd1sols015     |   25.23% |   14.41% |   0.75% |     84.95% | 16.22% |    8.82% |\n",
      "| 5b6cd1sols016     |   19.14% |   14.35% |   0.58% |     82.86% | 17.91% |    9.84% |\n",
      "| 5b6cd1sols017     |   49.59% |   28.10% |  12.66% |     86.93% | 36.59% |   22.39% |\n",
      "| 5b6cd1sols018     |   44.75% |   29.15% |  10.99% |     85.24% | 29.11% |   17.04% |\n",
      "| 5b6cd1sols019     |   37.33% |   29.33% |   4.19% |     88.64% | 37.29% |   22.92% |\n",
      "| 5b6cd1sols020     |   39.09% |   39.09% |  18.68% |     88.27% | 46.04% |   29.91% |\n",
      "| 5b6cd1sols021     |   43.41% |   42.64% |  22.63% |     88.13% | 58.11% |   40.95% |\n",
      "| **Average**        |   32.03% |   25.65% |  10.12% |     86.75% | 31.22% |   20.51% |\n",
      "\n",
      "Evaluation report saved to: evaluation_results\\rag_evaluation.md\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'441462sols000': EvaluationMetrics(rouge1=6.896551724137931, rougeL=6.896551724137931, bleu=0.34170206403382564, bertscore=84.51013565063477, f1=10.714285714285714, jaccard=5.660377358490567),\n",
       " '441462sols001': EvaluationMetrics(rouge1=17.72151898734177, rougeL=17.72151898734177, bleu=1.0606948946692587, bertscore=87.32571601867676, f1=22.727272727272727, jaccard=12.82051282051282),\n",
       " '441462sols002': EvaluationMetrics(rouge1=14.545454545454545, rougeL=14.545454545454545, bleu=0.5145174324020216, bertscore=83.16062688827515, f1=12.765957446808512, jaccard=6.8181818181818175),\n",
       " '441462sols003': EvaluationMetrics(rouge1=16.666666666666664, rougeL=12.5, bleu=0.6235344421390427, bertscore=85.24689078330994, f1=23.33333333333333, jaccard=13.20754716981132),\n",
       " '441462sols004': EvaluationMetrics(rouge1=14.285714285714285, rougeL=10.714285714285715, bleu=0.3333421816571239, bertscore=84.02745127677917, f1=16.0, jaccard=8.695652173913043),\n",
       " '441462sols005': EvaluationMetrics(rouge1=47.88732394366197, rougeL=36.61971830985916, bleu=12.447904522984011, bertscore=89.48683142662048, f1=42.85714285714285, jaccard=27.27272727272727),\n",
       " '441462sols006': EvaluationMetrics(rouge1=16.058394160583944, rougeL=14.5985401459854, bleu=1.0966751960534586, bertscore=85.09112596511841, f1=15.555555555555555, jaccard=8.433734939759036),\n",
       " '441462sols007': EvaluationMetrics(rouge1=64.28571428571429, rougeL=64.28571428571429, bleu=32.919298780790555, bertscore=93.67534518241882, f1=85.0, jaccard=73.91304347826086),\n",
       " '441462sols008': EvaluationMetrics(rouge1=29.545454545454547, rougeL=25.000000000000007, bleu=10.585831522642648, bertscore=89.33500051498413, f1=30.0, jaccard=17.647058823529413),\n",
       " '441462sols009': EvaluationMetrics(rouge1=11.76470588235294, rougeL=7.0588235294117645, bleu=0.6435560831476584, bertscore=83.62882733345032, f1=14.814814814814813, jaccard=8.0),\n",
       " '441462sols010': EvaluationMetrics(rouge1=69.79865771812081, rougeL=59.06040268456376, bleu=40.66728801262978, bertscore=91.36043787002563, f1=68.15642458100558, jaccard=51.69491525423729),\n",
       " '441462sols011': EvaluationMetrics(rouge1=19.689119170984455, rougeL=17.61658031088083, bleu=4.3901708368881, bertscore=81.34632706642151, f1=26.548672566371685, jaccard=15.306122448979592),\n",
       " '441462sols012': EvaluationMetrics(rouge1=3.508771929824561, rougeL=3.508771929824561, bleu=0, bertscore=82.86783695220947, f1=0.0, jaccard=0.0),\n",
       " '441462sols013': EvaluationMetrics(rouge1=40.61302681992337, rougeL=25.287356321839084, bleu=1.0967016075631397, bertscore=84.96403098106384, f1=25.316455696202528, jaccard=14.492753623188406),\n",
       " '441462sols014': EvaluationMetrics(rouge1=79.46666666666667, rougeL=72.53333333333333, bleu=65.57442126147946, bertscore=92.81991124153137, f1=76.85950413223141, jaccard=62.41610738255034),\n",
       " '441462sols015': EvaluationMetrics(rouge1=65.2671755725191, rougeL=45.80152671755725, bleu=20.30162432192966, bertscore=87.86004781723022, f1=53.28185328185329, jaccard=36.31578947368421),\n",
       " '441462sols016': EvaluationMetrics(rouge1=62.83185840707964, rougeL=61.94690265486725, bleu=43.04522196435524, bertscore=90.22842049598694, f1=61.417322834645674, jaccard=44.31818181818182),\n",
       " '441462sols017': EvaluationMetrics(rouge1=49.142857142857146, rougeL=24.571428571428573, bleu=17.37685771696896, bertscore=85.97625494003296, f1=48.113207547169814, jaccard=31.67701863354037),\n",
       " '5b6cd1sols000': EvaluationMetrics(rouge1=16.0, rougeL=16.0, bleu=0.6727019953045079, bertscore=87.10095286369324, f1=18.46153846153846, jaccard=10.16949152542373),\n",
       " '5b6cd1sols001': EvaluationMetrics(rouge1=10.084033613445378, rougeL=10.084033613445378, bleu=0.4440346145616142, bertscore=84.91861820220947, f1=15.384615384615389, jaccard=8.333333333333332),\n",
       " '5b6cd1sols002': EvaluationMetrics(rouge1=40.00000000000001, rougeL=27.27272727272727, bleu=1.107258143584582, bertscore=89.05367255210876, f1=32.91139240506329, jaccard=19.696969696969695),\n",
       " '5b6cd1sols003': EvaluationMetrics(rouge1=33.76623376623377, rougeL=28.571428571428577, bleu=2.840856719034977, bertscore=85.51303148269653, f1=31.372549019607842, jaccard=18.6046511627907),\n",
       " '5b6cd1sols004': EvaluationMetrics(rouge1=34.32835820895522, rougeL=29.85074626865672, bleu=11.24536542913082, bertscore=89.16589617729187, f1=44.73684210526316, jaccard=28.8135593220339),\n",
       " '5b6cd1sols005': EvaluationMetrics(rouge1=8.16326530612245, rougeL=8.16326530612245, bleu=0.2563559704586724, bertscore=84.72610116004944, f1=6.896551724137932, jaccard=3.571428571428571),\n",
       " '5b6cd1sols006': EvaluationMetrics(rouge1=2.0408163265306123, rougeL=2.0408163265306123, bleu=0.18819088728501285, bertscore=86.63926720619202, f1=3.278688524590164, jaccard=1.6666666666666667),\n",
       " '5b6cd1sols007': EvaluationMetrics(rouge1=15.789473684210526, rougeL=15.789473684210526, bleu=0.791683523302066, bertscore=88.68107199668884, f1=16.000000000000004, jaccard=8.695652173913043),\n",
       " '5b6cd1sols008': EvaluationMetrics(rouge1=17.39130434782609, rougeL=15.217391304347824, bleu=0.8438119133455312, bertscore=85.73070168495178, f1=20.28985507246377, jaccard=11.29032258064516),\n",
       " '5b6cd1sols009': EvaluationMetrics(rouge1=13.333333333333334, rougeL=10.666666666666668, bleu=1.5750473255825854, bertscore=86.35632395744324, f1=17.543859649122805, jaccard=9.615384615384617),\n",
       " '5b6cd1sols010': EvaluationMetrics(rouge1=47.27272727272727, rougeL=29.09090909090909, bleu=10.332090908268508, bertscore=88.88506889343262, f1=32.6530612244898, jaccard=19.51219512195122),\n",
       " '5b6cd1sols011': EvaluationMetrics(rouge1=19.895287958115183, rougeL=14.659685863874344, bleu=0.5915611581058416, bertscore=83.17248225212097, f1=20.833333333333336, jaccard=11.627906976744185),\n",
       " '5b6cd1sols012': EvaluationMetrics(rouge1=72.86821705426357, rougeL=57.36434108527132, bleu=44.79857246975154, bertscore=89.69497680664062, f1=69.41176470588235, jaccard=53.153153153153156),\n",
       " '5b6cd1sols013': EvaluationMetrics(rouge1=38.42364532019704, rougeL=22.66009852216749, bleu=3.9573117536435465, bertscore=84.86135005950928, f1=21.705426356589147, jaccard=12.173913043478262),\n",
       " '5b6cd1sols014': EvaluationMetrics(rouge1=23.157894736842106, rougeL=21.052631578947366, bleu=1.5881784448751126, bertscore=87.60881423950195, f1=22.58064516129032, jaccard=12.727272727272727),\n",
       " '5b6cd1sols015': EvaluationMetrics(rouge1=25.225225225225223, rougeL=14.414414414414415, bleu=0.7510032189201753, bertscore=84.94527339935303, f1=16.216216216216214, jaccard=8.823529411764707),\n",
       " '5b6cd1sols016': EvaluationMetrics(rouge1=19.138755980861248, rougeL=14.354066985645932, bleu=0.5752092032296838, bertscore=82.85795450210571, f1=17.91044776119403, jaccard=9.836065573770492),\n",
       " '5b6cd1sols017': EvaluationMetrics(rouge1=49.58677685950413, rougeL=28.09917355371901, bleu=12.661086796290947, bertscore=86.93346977233887, f1=36.585365853658544, jaccard=22.388059701492537),\n",
       " '5b6cd1sols018': EvaluationMetrics(rouge1=44.74576271186441, rougeL=29.152542372881356, bleu=10.994123276832077, bertscore=85.23712158203125, f1=29.11392405063291, jaccard=17.037037037037038),\n",
       " '5b6cd1sols019': EvaluationMetrics(rouge1=37.333333333333336, rougeL=29.333333333333332, bleu=4.1888153808889, bertscore=88.63632082939148, f1=37.28813559322033, jaccard=22.916666666666664),\n",
       " '5b6cd1sols020': EvaluationMetrics(rouge1=39.090909090909086, rougeL=39.090909090909086, bleu=18.676073625288492, bertscore=88.26751708984375, f1=46.043165467625904, jaccard=29.906542056074763),\n",
       " '5b6cd1sols021': EvaluationMetrics(rouge1=43.41085271317829, rougeL=42.63565891472869, bleu=22.632917041952297, bertscore=88.13410997390747, f1=58.108108108108105, jaccard=40.95238095238095)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import NLPEvaluator\n",
    "\n",
    "evaluator = NLPEvaluator()\n",
    "evaluator.evaluate(\n",
    "    llm_response_file=\"data/design_and_analysis_of_algorithms/llm_responses/rag.jsonl\",\n",
    "    ground_truth_file=\"data/design_and_analysis_of_algorithms/exams/design_combined.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM Assisted Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-Assisted Evaluation Results:\n",
      "| Question ID       | Score | Comments |\n",
      "|-------------------|-------|----------|\n",
      "| 441462sols000 | 1 | The LLM correctly identifies the statement as false, which is in line with the ground truth. However, the explanation provided by the LLM contains inaccuracies. It states that O(V^3) is faster than O(Vlog2 7), which is incorrect, and it doesn't address the reason related to Strassen’s algorithm or the concept of defining negation. Therefore, the response is only partially correct. |\n",
      "| 441462sols001 | 0 | The LLM response incorrectly concludes that Floyd-Warshall is asymptotically faster. The ground truth correctly states that Johnson's algorithm is asymptotically faster since its complexity O(VE + V² log V) becomes O(V³) when E = O(V^1.5), matching with Floyd-Warshall's O(V³) but is lower bounded, allowing Johnson to potentially be faster or equal depending on implementation factors. The faulty nesting of complexity reasoning leads to a 'wrong' score. |\n",
      "| 441462sols002 | 1 | The answer correctly identifies the statement as false, in line with the ground truth. However, while it provides reasons related to the structure not matching that of a tree, it does not specifically mention that the directed graph is a Directed Acyclic Graph (DAG), which is an essential part of the ground truth explanation. |\n",
      "| 441462sols003 | 1 | The response is partially correct. While it correctly identifies that the lowest weight edge is included in a minimum spanning tree, the explanation is flawed. The reasoning about including all vertices and edges is incorrect—minimum spanning trees do not include all edges. Additionally, the concept of proving by contradiction is not properly applied here. The correct explanation, according to ground truth, involves considering Kruskal's algorithm, where the lowest weight edge is chosen first, if it doesn't form a cycle, as part of the construction. |\n",
      "| 441462sols004 | 0 | The LLM's response is incorrect because it applies the general complexities for Kruskal's or Prim's algorithms without considering the specific structure of the graph with n vertices and n edges. The ground truth explains a more efficient approach by removing the heaviest edge in the single cycle that exists, which simplifies the problem to an O(n) solution. The LLM fails to recognize this special case and incorrectly cites a higher time complexity. |\n",
      "| 441462sols005 | 2 | The response is fully correct. It accurately states that the Ford-Fulkerson algorithm runs in time O((V + E)|f|) and explains the reasoning that matches the ground truth: each iteration increases the flow by at least 1, resulting in at most |f| iterations. The response uses consistent notation and provides a clear and concise explanation that aligns with the ground truth. |\n",
      "| 441462sols006 | 0 | The response is wrong. It argues that increasing the capacity of the cut C will always result in an increase in maximum flow, but this is incorrect. The ground truth explains that the existence of another minimum cut in the network means that even if capacities of edges across C are increased, another cut can still be the limiting factor, potentially keeping the max flow unchanged. The response fails to consider this scenario, leading to an incorrect conclusion. |\n",
      "| 441462sols007 | 2 | The response is 'fully correct' because it matches the ground truth precisely. It correctly states that every linear program can have many optimal solutions if the objective function is parallel to a constraint, which is the same reasoning provided in the ground truth. |\n",
      "| 441462sols008 | 0 | The LLM response is wrong as it incorrectly interprets the implication of P = NP. According to the ground truth, if P = NP, then NP-complete problems like 3SAT would indeed have polynomial-time solutions, as P = NP implies that all NP problems can be solved in polynomial time. The LLM claims that NP-complete problems cannot be solved in polynomial time even if P = NP, which contradicts the ground truth. |\n",
      "| 441462sols009 | 0 | The LLM response is incorrect, stating the algorithm is a 2-approximation, which contradicts the ground truth that specifies it can perform as poorly as a log-log approximation. The answer erroneously justifies a 2-approximation, whereas the ground truth explicitly states 'False' and provides evidence for the poor approximation performance. Therefore, the response is fully incorrect. |\n",
      "| 441462sols010 | 2 | The response is fully correct and matches the ground truth description. It outlines the same algorithmic steps: running Johnson's algorithm on the original graph to find shortest paths between all pairs, constructing a new graph based on charging stations, and finally applying Dijkstra’s algorithm. It also correctly mentions the time complexity of the algorithm as O(VE + V² log V). |\n",
      "| 441462sols011 | 1 | The response correctly identifies the need to sort psets by increasing order of di/ci, which aligns with the ground truth solution. However, the explanation mistakenly claims this approach prioritizes 'highest penalty per day', which is incorrect. The ground truth focuses on prioritizing based on the ratio di/ci. The explanation of correctness is incorrect, as it states a contradiction based on completing higher penalty per day first, which is not the basis of the ground truth solution, leading to a partially correct answer. |\n",
      "| 441462sols012 | 0 | The provided answer is incorrect. It states that the number of nights required is equal to the path length |E|, but it fails to account for the additional nights needed for all m teens to traverse the path. The correct answer is |E| + m - 1 or |V| + m - 2, taking into account both the path length and the additional teens beyond the first one reaching the end. |\n",
      "| 441462sols013 | 1 | The proposed solution suggests a binary search strategy, which aligns with the ground truth. However, there are inaccuracies in the explanation:  1. The upper bound for the binary search is set incorrectly to 'm,' whereas the ground truth suggests it should be based on a function of 'E' and 'm' (like O(E + m), etc). 2. The time complexity analysis is flawed. The response incorrectly states the total time complexity as O((E + m)^a * log m * T(V, E, m)), missing the form O((E+m)^a log(E+m) T(V, E, m)) specified in the ground truth.  Due to these mismatches, the response is only partially correct. |\n",
      "| 441462sols014 | 1 | The response describes the transformation of the original problem into a max-flow problem correctly by constructing the graph G' = (V', E') with vertices V' = {(v, i) | v ∈ V, 0 ≤ i ≤ k} and details how to create edges. However, it is incomplete as it does not provide a full analysis of the time complexity or describe the algorithms (Edmonds-Karp or Ford-Fulkerson) that should be used to solve the max-flow problem, which are crucial parts of the ground truth answer. |\n",
      "| 441462sols015 | 2 | The LLM response correctly describes the algorithm for solving the carpool problem using a max flow approach. It details the construction of the graph, including the super source and sink, the vertices for each person and day, and the edges with appropriate capacities. It also correctly identifies the conditions for a valid driver assignment based on the max flow result. This matches the ground truth answer, both in method and reasoning, with minor variations in wording or description that do not affect the accuracy of the response. Therefore, the answer is fully correct. |\n",
      "| 441462sols016 | 2 | The LLM response is fully correct as it aligns with the ground truth. Both describe the procedure of running DFS to detect cycles and using topological sort to check for a Hamiltonian path in a DAG. The response correctly states that no cycle implies a DAG and utilizes topological sort to determine the Hamiltonian path existence. It also appropriately addresses the polynomial time complexity O(V+E) of both DFS and topological sort. |\n",
      "| 441462sols017 | 2 | The answer is fully correct as it closely aligns with the ground truth. The reduction process from the HAMILTONIAN PATH problem to construct G' is described accurately, including the construction of the cycle (U1, U2, U3) and how it relates to finding a Hamiltonian Path in G. The logic and steps provided match the ground truth solution, indicating that solving for a Hamiltonian Path in G' effectively solves the original problem for G, thus demonstrating NP-hardness. |\n",
      "| 5b6cd1sols000 | 0 | The ground truth states the answer is 'True' because using earliest start time for equal-sized intervals is equivalent to using the earliest finish time. However, the LLM's response states 'False' and provides reasoning based on potential conflicts that don't apply when all intervals are of equal size. This demonstrates a misunderstanding of the problem's constraints and an incorrect conclusion, leading to a score of 0. |\n",
      "| 5b6cd1sols001 | 0 | The response incorrectly claims that the problem cannot be solved in O(n log n) time using dynamic programming, contradicting the ground truth which states that it can. The explanation provided in the response is also incorrect and irrelevant to the problem, such as the mention of a recurrence relation that does not apply to this problem. |\n",
      "| 5b6cd1sols002 | 1 | The response correctly identifies that the statement is false because the described method does not achieve linear time complexity. However, the response incorrectly applies the recurrence relation. The given problem's solution is based on breaking into groups of 3, which does not result in $T(n) = O(n)$. The critique also inaccurately uses the Master Theorem for the fallacious recurrence $T(n) = 3T(n/3) + O(n)$ and mentions issues with finding the true median, which go beyond the scope of justifying falsehood. |\n",
      "| 5b6cd1sols003 | 0 | The LLM response is incorrect as it claims the overall time complexity remains O(n log n), whereas both the ground truth and the problem clearly reflect that the complexity would change due to the Θ(n²) merge algorithm. The ground truth correctly shows that the recurrence relation would be $T(n) = 2T(n/2) + Θ(n²)$, leading to an overall time complexity of O(n²), not O(n log n). |\n",
      "| 5b6cd1sols004 | 1 | The LLM response is close to the ground truth but contains additional and somewhat inaccurate elaboration. It correctly identifies the statement as 'False' and aligns with the ground truth by mentioning that inserting and finding successors takes n lg lg(u) time or lg(u) · lg lg(u) in terms of u. However, it incorrectly states the operations would lead to a complexity of O(lg u·lg lg(u)·lg lg lg(u)), which is not established by the ground truth. This extra and incorrect detail prevents the answer from being fully correct. |\n",
      "| 5b6cd1sols005 | 1 | The LLM correctly identifies the statement as false, aligning with the ground truth. However, the explanation contains inaccuracies. The Van Emde Boas data structure does not use O(lg u) space; it typically uses O(u) space. The mention of a binary search tree and height of O(lg u) is incorrect in the context of Van Emde Boas trees, which use a recursive method for successor search. Therefore, the response is partially correct as it contains correct and incorrect aspects. |\n",
      "| 5b6cd1sols006 | 2 | The LLM's response is fully correct. It correctly identifies the statement as 'True,' matching the ground truth. The explanation is also accurate, explaining that the potential energy represents extra energy to pay for future operations and should not go negative to ensure accurate amortized cost calculations. This aligns well with the principles of the potential method in amortized analysis. |\n",
      "| 5b6cd1sols007 | 0 | The answer provided is incorrect. The ground truth states that the auxiliary space required is O(log n), but the LLM response claims it requires O(n) due to the linear-time median finding and recursive calls, which is inaccurate. The correct implementation only requires O(log n) space for the recursion stack. |\n",
      "| 5b6cd1sols008 | 0 | The LLM response incorrectly claims that the statement is false, contradicting the ground truth which states it is true. The LLM incorrectly argues that the maximum search time would be Ω(n) instead of Ω(2n). The core issue isn't about misjudging the logarithmic nature but misrepresenting the extreme possibility of skip list configuration. Thus, the response is wrong based on the ground truth. |\n",
      "| 5b6cd1sols009 | 1 | The answer is partially correct. It correctly identifies that the statement is false, and that there is an issue with A and C colliding. However, it incorrectly states the condition for universality. The correct condition is that for any two distinct keys, the probability that they hash to the same value is at most 1/|range| rather than 1/|U|. The explanation about collision is right, but it misses the correct collision probability detail provided in the ground truth. |\n",
      "| 5b6cd1sols010 | 2 | The LLM response correctly identifies Ben's mistake as not padding the polynomials a and b with enough zeroes, which aligns with the ground truth explanation that padding is necessary for the correct calculation of the FFT and inverse FFT. Therefore, the answer is fully correct. |\n",
      "| 5b6cd1sols011 | 1 | The response correctly identifies the Van Emde Boas (VEB) tree as the fastest data structure for handling the operations associated with this problem, which matches the ground truth. The mention of O(lg lg(n)) time complexity for operations using VEB tree is also accurate.  However, it does not explicitly cover the initialization process or mention specific operations like V.insert(i), V.delete(i), and V.successor(i) as outlined in the ground truth. The answer could benefit from more detail on how each operation specifically maps to the tree operations mentioned. |\n",
      "| 5b6cd1sols012 | 2 | The LLM response accurately matches the ground truth answer. It correctly describes the algorithm for both operations, specifies the worst-case time complexities, and provides the correct potential function and amortized cost calculations. The description of both the operations (INSERT and REMOVE-BOTTOM-HALF) and the use of the potential function aligns perfectly with the ground truth. Therefore, the response is fully correct. |\n",
      "| 5b6cd1sols013 | 0 | The LLM response introduces unnecessary complexity to the randomized algorithm by involving steps like constructing $s(x)$ and probabilistic output, which are not present in the ground truth. The ground truth describes a straightforward procedure by merely checking $p(a)q(a) = r(a)$ for a random $a$. The LLM's additional steps create confusion and do not align with the ground truth. |\n",
      "| 5b6cd1sols014 | 1 | The response partially addresses the requirement, emphasizing that if \\(p(x)\\cdot q(x) = r(x)\\), the evaluations will match. However, it mistakenly states the property as being satisfied for all values of \\(x\\), which is not practical due to infinite values, reflecting a misunderstanding of using specific random points for checking. |\n",
      "| 5b6cd1sols015 | 0 | The LLM response is incorrect because it does not address the core aspect of Property 2 in the context given. Property 2 is related to the probability of randomly choosing a root that makes an erroneous equality seem true (Pr{s(a) = 0}). The ground truth specifies that s(x) is a degree-2n polynomial with at most 2n roots, and the probability Pr{s(a) = 0} is less than or equal to 2n/4n where a is picked from a set of size 4n. The LLM response talks about correctness conditions that don't apply to the probabilistic aspect or degree analysis described in the context of the problem. |\n",
      "| 5b6cd1sols016 | 1 | The LLM response captures the essential idea of using a randomized approach to verify the polynomial product and includes key steps such as evaluating the polynomials and checking the condition for zero. However, the analysis of the algorithm's correctness probability and number of iterations is incorrect. It claims the algorithm is correct with probability at least 1-1/ε, which is a conceptual error because it should be at least 1-ε. Additionally, the number of iterations should be determined based on sufficient amplification of the success probability to 1-ε, which is not clearly derived in the response. The ground truth specifies an iteration count derived from log, which is missing in the LLM response. Thus, the response is only partially correct. |\n",
      "| 5b6cd1sols017 | 1 | The response is partially correct. It correctly outlines the dynamic programming approach for computing TL[i][j] using the relation: TL[i][j] = max{T[i][j], TL[i-1][j], TL[i][j-1]}. This matches with the ground truth. However, the response incorrectly indicates returning TL[n][m], which does not make sense in the context of computing TL for all 1 < i < n and 1 < j < m. Instead, it should involve computing and acknowledging TL[i][j] for each valid i and j as per the problem constraint. The response mainly explains the algorithm correctly but has a slight error in its final statement, which led to a 'partially correct' categorization. |\n",
      "| 5b6cd1sols018 | 1 | The response is partially correct as it outlines the concept of calculating maximum tastiness for each quadrant and suggests using dynamic programming. However, it fails to fully detail the method to compute the maximum product of the four quadrants around cell (i,j) and misses the completion of the TL, BL, TR, and BR computation steps before finding the product. It also doesn't address how the calculation reaches O(nm) complexity. While the response touches on parts relevant to the solution, it lacks the completeness and clarity needed to match the ground truth. |\n",
      "| 5b6cd1sols019 | 2 | The response is fully correct as it matches the ground truth. It outlines the process of merging the two arrays, which takes O(m + n) time, and then finding the median from the merged array, which is consistent with the solution provided in the ground truth. |\n",
      "| 5b6cd1sols020 | 1 | The proposed solution in the LLM response correctly identifies the initial steps of the algorithm: picking the medians, comparing them, and discarding parts of the arrays to narrow down the search. However, it makes mistakes in the final steps and time complexity analysis:  - Step 5 introduces merging of the arrays and finding the median with a linear-time operation, which is unnecessary and incorrect for achieving O(lg n) complexity. The merge step is not part of the correct algorithm for maintaining O(log n) complexity.  - The ground truth algorithm suggests repeating the process until subsets are smaller than a constant, at which point a direct approach could find the median, but this does not involve merging.  Hence, the response is partially correct as it captures the core idea but adds incorrect steps that affect the final time complexity. |\n",
      "| 5b6cd1sols021 | 1 | The response begins correctly by discussing the initial removal of elements, which matches the ground truth. However, it incorrectly suggests a second part of the algorithm that involves merging the arrays, which is unnecessary and contradicts the constraint of O(lg(min(m, n))) complexity. The ground truth does not include merging and focuses on narrowing down the search space using medians, not merging for linear-time selection. Thus, while part of the approach is correct, the unnecessary merging step deviates from the expected efficient algorithm, making the response partially correct. |\n",
      "\n",
      "Total Score: 37/80 (46.25%)\n",
      "\n",
      "Evaluation report saved to: evaluation_results\\rag_llm_assisted.md\n"
     ]
    }
   ],
   "source": [
    "from llm_assisted_evaluation import evaluate\n",
    "\n",
    "evaluate(\n",
    "    llm_response_file=\"data/design_and_analysis_of_algorithms/llm_responses/rag.jsonl\",\n",
    "    ground_truth_file=\"data/design_and_analysis_of_algorithms/exams/design_combined.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-tuning\n",
    "https://platform.openai.com/finetune/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
